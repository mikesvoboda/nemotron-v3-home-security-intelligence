"""Event model for security event tracking."""

from datetime import UTC, datetime
from typing import TYPE_CHECKING, Any

from sqlalchemy import (
    Boolean,
    CheckConstraint,
    DateTime,
    ForeignKey,
    Index,
    Integer,
    String,
    Text,
    case,
)
from sqlalchemy.dialects.postgresql import TSVECTOR
from sqlalchemy.ext.hybrid import hybrid_property
from sqlalchemy.orm import Mapped, declared_attr, deferred, mapped_column, relationship

from backend.core.orm_utils import get_relationship_lazy_mode

from .camera import Base, Camera
from .enums import Severity

if TYPE_CHECKING:
    from .alert import Alert
    from .detection import Detection
    from .event_audit import EventAudit
    from .event_detection import EventDetection
    from .event_feedback import EventFeedback


class Event(Base):
    """Event model representing a security event.

    Events are aggregated from multiple detections within a time window,
    analyzed by the LLM to determine risk level and generate summaries.

    The search_vector column is a PostgreSQL TSVECTOR that enables full-text search
    across summary, reasoning, and object types. It is populated via a database trigger.
    """

    __tablename__ = "events"

    id: Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True)
    batch_id: Mapped[str] = mapped_column(String, nullable=False)
    camera_id: Mapped[str] = mapped_column(
        String, ForeignKey("cameras.id", ondelete="CASCADE"), nullable=False
    )
    started_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=False)
    ended_at: Mapped[datetime | None] = mapped_column(DateTime(timezone=True), nullable=True)
    risk_score: Mapped[int | None] = mapped_column(Integer, nullable=True)
    # risk_level column stores the persisted risk level from LLM analysis
    # Use the computed_risk_level hybrid_property for consistent risk level computation
    risk_level: Mapped[str | None] = mapped_column(String, nullable=True)

    # Optimistic locking version column (NEM-3408)
    # Auto-incremented on each UPDATE to prevent concurrent modification conflicts
    version: Mapped[int] = mapped_column(Integer, nullable=False, default=1, server_default="1")

    summary: Mapped[str | None] = mapped_column(Text, nullable=True)
    # Deferred columns: Large text columns that are not loaded by default
    # to reduce memory usage in list queries. Use undefer() to load when needed.
    reasoning: Mapped[str | None] = deferred(mapped_column(Text, nullable=True))
    # Full prompt sent to Nemotron LLM for analysis (for debugging/improvement)
    llm_prompt: Mapped[str | None] = deferred(mapped_column(Text, nullable=True))
    reviewed: Mapped[bool] = mapped_column(Boolean, default=False, nullable=False)
    notes: Mapped[str | None] = mapped_column(Text, nullable=True)
    is_fast_path: Mapped[bool] = mapped_column(Boolean, default=False, nullable=False)
    # Cached object types from related detections (comma-separated string)
    # Populated by the batch aggregator when events are created
    # Has a GIN trigram index (idx_events_object_types_trgm) for efficient LIKE/ILIKE queries
    object_types: Mapped[str | None] = mapped_column(Text, nullable=True)

    # Path to generated video clip for this event (optional)
    # Generated by ClipGenerator service on event close
    clip_path: Mapped[str | None] = mapped_column(String, nullable=True)

    # Full-text search vector (PostgreSQL TSVECTOR)
    # This column is auto-populated by a database trigger on INSERT/UPDATE
    # Combines: summary, reasoning, object_types, and camera_name (via join)
    search_vector: Mapped[Any] = mapped_column(TSVECTOR, nullable=True)

    # Soft delete timestamp for preserving referential integrity
    deleted_at: Mapped[datetime | None] = mapped_column(
        DateTime(timezone=True), nullable=True, default=None
    )

    # Alert snooze timestamp (NEM-2359)
    # When set, alerts for this event are snoozed until this timestamp
    snooze_until: Mapped[datetime | None] = mapped_column(
        DateTime(timezone=True), nullable=True, default=None
    )

    # ==========================================================================
    # Relationships (NEM-3405: Configurable lazy loading for N+1 prevention)
    # ==========================================================================
    # In development/test mode, relationships use 'raise_on_sql' to catch N+1 queries.
    # In production, standard 'select' loading is used.
    # Use selectinload/joinedload explicitly in queries that need eager loading.

    camera: Mapped[Camera] = relationship(
        "Camera",
        back_populates="events",
        lazy=get_relationship_lazy_mode(),
    )
    alerts: Mapped[list[Alert]] = relationship(
        "Alert",
        back_populates="event",
        cascade="all, delete-orphan",
        lazy=get_relationship_lazy_mode(),
    )
    audit: Mapped[EventAudit | None] = relationship(
        "EventAudit",
        back_populates="event",
        uselist=False,
        cascade="all, delete-orphan",
        lazy=get_relationship_lazy_mode(),
    )
    # Junction table relationship for normalized detection associations
    # This provides access to EventDetection records for this event
    detection_records: Mapped[list[EventDetection]] = relationship(
        "EventDetection",
        back_populates="event",
        cascade="all, delete-orphan",
        lazy=get_relationship_lazy_mode(),
    )
    # User feedback relationship (NEM-1794)
    feedback: Mapped[EventFeedback | None] = relationship(
        "EventFeedback",
        back_populates="event",
        uselist=False,
        cascade="all, delete-orphan",
        lazy=get_relationship_lazy_mode(),
    )
    # Many-to-many relationship to Detection via junction table
    # This is the primary way to access detections - use this instead of parsing detection_ids
    # Note: This relationship uses 'selectin' by default as it's frequently accessed together
    detections: Mapped[list[Detection]] = relationship(
        "Detection",
        secondary="event_detections",
        viewonly=True,  # Managed via detection_records
        lazy="selectin",  # Eager load to avoid N+1 queries (always eager for this relationship)
    )

    # Indexes for common queries
    # Note: idx_events_object_types_trgm (GIN trigram index on object_types) is created
    # via Alembic migration as it requires pg_trgm extension and gin_trgm_ops operator class
    __table_args__ = (
        Index("idx_events_camera_id", "camera_id"),
        Index("idx_events_started_at", "started_at"),
        Index("idx_events_risk_score", "risk_score"),
        Index("idx_events_reviewed", "reviewed"),
        Index("idx_events_batch_id", "batch_id"),
        # GIN index for full-text search
        Index("idx_events_search_vector", "search_vector", postgresql_using="gin"),
        # NEM-1529: Composite index for combined risk_level and started_at filtering
        # Enables efficient queries like "show all high-risk events from today"
        Index("idx_events_risk_level_started_at", "risk_level", "started_at"),
        # NEM-1535: Covering index for export queries to avoid table lookups
        # Includes all columns needed for export: id, started_at, ended_at, risk_level,
        # risk_score, camera_id, object_types, summary
        Index(
            "idx_events_export_covering",
            "started_at",
            "id",
            "ended_at",
            "risk_level",
            "risk_score",
            "camera_id",
            "object_types",
            "summary",
        ),
        # NEM-1536: Partial index for unreviewed events dashboard query
        # Only indexes rows WHERE reviewed = false for efficient unreviewed count
        Index(
            "idx_events_unreviewed",
            "id",
            postgresql_where="reviewed = false",
        ),
        # CHECK constraints for enum-like columns and business rules
        CheckConstraint(
            "risk_level IS NULL OR risk_level IN ('low', 'medium', 'high', 'critical')",
            name="ck_events_risk_level",
        ),
        CheckConstraint(
            "risk_score IS NULL OR (risk_score >= 0 AND risk_score <= 100)",
            name="ck_events_risk_score_range",
        ),
        CheckConstraint(
            "ended_at IS NULL OR ended_at >= started_at",
            name="ck_events_time_order",
        ),
        # BRIN index for time-series queries on started_at (append-only chronological data)
        # Much smaller than B-tree (~1000x) and ideal for range queries on ordered timestamps
        Index(
            "ix_events_started_at_brin",
            "started_at",
            postgresql_using="brin",
        ),
    )

    # ==========================================================================
    # Optimistic Locking Configuration (NEM-3408)
    # ==========================================================================
    # SQLAlchemy's version_id_col enables optimistic locking by auto-incrementing
    # the version column on each UPDATE. If a concurrent modification occurs,
    # SQLAlchemy raises StaleDataError, allowing the application to handle conflicts.
    #
    # Usage pattern:
    #   1. Load event: event = await session.get(Event, event_id)
    #   2. Modify: event.reviewed = True
    #   3. Commit: await session.commit()  # Raises StaleDataError if version mismatch

    @declared_attr  # type: ignore[arg-type]
    def __mapper_args__(cls) -> dict[str, Any]:
        """Configure mapper with version_id_col for optimistic locking."""
        return {"version_id_col": cls.__table__.c.version}

    @property
    def is_deleted(self) -> bool:
        """Check if this event is soft-deleted.

        Returns:
            True if deleted_at is set, False otherwise
        """
        return self.deleted_at is not None

    @property
    def is_snoozed(self) -> bool:
        """Check if this event's alerts are currently snoozed.

        An event is considered snoozed if snooze_until is set and is
        in the future (greater than the current time).

        Returns:
            True if event is currently snoozed, False otherwise
        """
        if self.snooze_until is None:
            return False
        return self.snooze_until > datetime.now(UTC)

    def soft_delete(self) -> None:
        """Soft delete this event by setting deleted_at timestamp.

        This marks the event as deleted without removing it from the database,
        preserving referential integrity with related records.
        """
        self.deleted_at = datetime.now(UTC)

    def restore(self) -> None:
        """Restore a soft-deleted event by clearing deleted_at timestamp."""
        self.deleted_at = None

    async def hard_delete(self, session: object) -> None:
        """Hard delete this event, permanently removing it from the database.

        Args:
            session: SQLAlchemy async session to use for deletion
        """
        await session.delete(self)  # type: ignore[attr-defined]

    def __repr__(self) -> str:
        return (
            f"<Event(id={self.id}, batch_id={self.batch_id!r}, "
            f"camera_id={self.camera_id!r}, risk_score={self.risk_score})>"
        )

    def get_severity(self) -> Severity | None:
        """Get the severity level for this event based on risk score.

        Uses the SeverityService to map the risk_score to a Severity enum.
        Returns None if no risk_score is set.

        Returns:
            Severity enum value or None if risk_score is not set
        """
        if self.risk_score is None:
            return None

        # Import here to avoid circular dependency
        from backend.services.severity import get_severity_service

        service = get_severity_service()
        return service.risk_score_to_severity(self.risk_score)

    @property
    def detection_id_list(self) -> list[int]:
        """Get list of detection IDs from the relationship.

        This replaces the need to parse the legacy detection_ids JSON column.
        Uses the normalized event_detections junction table.

        Returns:
            List of detection IDs associated with this event
        """
        return [d.id for d in self.detections]

    @property
    def detection_count(self) -> int:
        """Get count of detections associated with this event.

        Returns:
            Number of detections linked to this event
        """
        return len(self.detections)

    # ==========================================================================
    # Hybrid Properties for Computed Values (NEM-3404)
    # ==========================================================================

    @hybrid_property
    def computed_risk_level(self) -> str | None:
        """Compute risk level from risk_score using configurable thresholds.

        This hybrid_property computes the risk level dynamically from risk_score,
        allowing both Python-side computation and SQL-side CASE expressions.

        Default thresholds (configurable via settings):
        - 0-29: low
        - 30-59: medium
        - 60-84: high
        - 85-100: critical

        Returns:
            Risk level string ('low', 'medium', 'high', 'critical') or None if no risk_score
        """
        if self.risk_score is None:
            return None

        # Import settings lazily to avoid circular imports
        from backend.core.config import get_settings

        settings = get_settings()
        score = self.risk_score

        if score <= settings.severity_low_max:
            return "low"
        elif score <= settings.severity_medium_max:
            return "medium"
        elif score <= settings.severity_high_max:
            return "high"
        else:
            return "critical"

    @computed_risk_level.expression  # type: ignore[no-redef]
    @classmethod
    def computed_risk_level(cls) -> Any:
        """SQL expression for computed_risk_level.

        Generates a CASE expression for use in SQL queries:
        SELECT *, CASE WHEN risk_score IS NULL THEN NULL
                       WHEN risk_score <= 29 THEN 'low'
                       WHEN risk_score <= 59 THEN 'medium'
                       WHEN risk_score <= 84 THEN 'high'
                       ELSE 'critical' END AS computed_risk_level
        FROM events;

        Note: Uses hardcoded default thresholds for SQL compatibility.
        For dynamic thresholds, compute in Python or use a database function.
        """
        return case(
            (cls.risk_score.is_(None), None),
            (cls.risk_score <= 29, "low"),
            (cls.risk_score <= 59, "medium"),
            (cls.risk_score <= 84, "high"),
            else_="critical",
        )
