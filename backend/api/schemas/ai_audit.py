"""Pydantic schemas for AI audit API endpoints."""

from datetime import datetime

from pydantic import BaseModel, ConfigDict, Field


class ModelContributions(BaseModel):
    """Model contribution flags indicating which AI models contributed to an event's analysis.

    Each flag is True if that model provided data for the event analysis,
    False if the model was not used or had no relevant detections.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "rtdetr": True,
                "florence": True,
                "clip": False,
                "violence": False,
                "clothing": True,
                "vehicle": False,
                "pet": False,
                "weather": True,
                "image_quality": True,
                "zones": True,
                "baseline": False,
                "cross_camera": False,
            }
        }
    )

    rtdetr: bool = Field(False, description="RT-DETR object detection contributed")
    florence: bool = Field(False, description="Florence-2 vision attributes contributed")
    clip: bool = Field(False, description="CLIP embeddings contributed")
    violence: bool = Field(False, description="Violence detection model contributed")
    clothing: bool = Field(False, description="Clothing analysis model contributed")
    vehicle: bool = Field(False, description="Vehicle classification model contributed")
    pet: bool = Field(False, description="Pet classification model contributed")
    weather: bool = Field(False, description="Weather classification model contributed")
    image_quality: bool = Field(False, description="Image quality assessment contributed")
    zones: bool = Field(False, description="Zone analysis model contributed")
    baseline: bool = Field(False, description="Baseline comparison model contributed")
    cross_camera: bool = Field(False, description="Cross-camera correlation contributed")


class QualityScores(BaseModel):
    """Self-evaluation quality scores on a 1-5 rubric scale.

    Scores are generated by the LLM self-evaluation process:
    - 1: Poor - major issues with this dimension
    - 2: Below Average - notable problems
    - 3: Average - acceptable but room for improvement
    - 4: Good - solid performance with minor issues
    - 5: Excellent - exemplary quality
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "context_usage": 4.2,
                "reasoning_coherence": 4.5,
                "risk_justification": 4.0,
                "consistency": 4.3,
                "overall": 4.25,
            }
        }
    )

    context_usage: float | None = Field(
        None, ge=1, le=5, description="How well enrichment context was used (1-5)"
    )
    reasoning_coherence: float | None = Field(
        None, ge=1, le=5, description="Logical flow and clarity of reasoning (1-5)"
    )
    risk_justification: float | None = Field(
        None, ge=1, le=5, description="Quality of evidence supporting risk score (1-5)"
    )
    consistency: float | None = Field(
        None, ge=1, le=5, description="Score stability across re-evaluations (1-5)"
    )
    overall: float | None = Field(
        None, ge=1, le=5, description="Weighted average of all quality dimensions (1-5)"
    )


class PromptImprovements(BaseModel):
    """Prompt improvement suggestions generated during self-evaluation.

    The LLM identifies areas where the prompt template could be improved
    to produce better risk analysis results.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "missing_context": [
                    "Time since last detected motion",
                    "Historical activity patterns for this camera",
                ],
                "confusing_sections": [],
                "unused_data": ["Weather data provided but not referenced"],
                "format_suggestions": ["Consider grouping detection context by confidence"],
                "model_gaps": ["Vehicle classification could help with driveway events"],
            }
        }
    )

    missing_context: list[str] = Field(
        default_factory=list,
        description="Context the LLM wished it had for better analysis",
    )
    confusing_sections: list[str] = Field(
        default_factory=list,
        description="Parts of the prompt that were unclear or ambiguous",
    )
    unused_data: list[str] = Field(
        default_factory=list,
        description="Enrichment data provided but not utilized in reasoning",
    )
    format_suggestions: list[str] = Field(
        default_factory=list,
        description="Suggestions for better prompt structure or formatting",
    )
    model_gaps: list[str] = Field(
        default_factory=list,
        description="AI models that could help but were not available",
    )


class EventAuditResponse(BaseModel):
    """Full audit response containing all evaluation data for a single event.

    Includes model contributions, quality scores, consistency metrics,
    and prompt improvement suggestions.
    """

    model_config = ConfigDict(
        from_attributes=True,
        json_schema_extra={
            "example": {
                "id": 1,
                "event_id": 42,
                "audited_at": "2026-01-03T10:30:00Z",
                "is_fully_evaluated": True,
                "contributions": {
                    "rtdetr": True,
                    "florence": True,
                    "clip": False,
                    "violence": False,
                    "clothing": True,
                    "vehicle": False,
                    "pet": False,
                    "weather": True,
                    "image_quality": True,
                    "zones": True,
                    "baseline": False,
                    "cross_camera": False,
                },
                "prompt_length": 2500,
                "prompt_token_estimate": 625,
                "enrichment_utilization": 0.85,
                "scores": {
                    "context_usage": 4.2,
                    "reasoning_coherence": 4.5,
                    "risk_justification": 4.0,
                    "consistency": 4.3,
                    "overall": 4.25,
                },
                "consistency_risk_score": 72,
                "consistency_diff": 3,
                "self_eval_critique": "Good context usage but could include more temporal patterns.",
                "improvements": {
                    "missing_context": ["time since last event"],
                    "confusing_sections": [],
                    "unused_data": ["weather data not referenced"],
                    "format_suggestions": [],
                    "model_gaps": [],
                },
            }
        },
    )

    id: int = Field(..., ge=1, description="Unique audit record ID")
    event_id: int = Field(..., ge=1, description="ID of the audited event")
    audited_at: datetime = Field(..., description="When the audit was created (UTC)")
    is_fully_evaluated: bool = Field(..., description="Whether full self-evaluation has been run")

    # Model contributions
    contributions: ModelContributions = Field(
        ..., description="Which AI models contributed to the event analysis"
    )

    # Prompt metrics
    prompt_length: int = Field(..., ge=0, description="Character count of the LLM prompt")
    prompt_token_estimate: int = Field(..., ge=0, description="Estimated token count (chars/4)")
    enrichment_utilization: float = Field(
        ..., ge=0, le=1, description="Fraction of enrichments used in reasoning (0-1)"
    )

    # Quality scores (None if not evaluated)
    scores: QualityScores = Field(..., description="Self-evaluation rubric scores (1-5 scale)")

    # Consistency check
    consistency_risk_score: int | None = Field(
        None, ge=0, le=100, description="Risk score from consistency re-evaluation"
    )
    consistency_diff: int | None = Field(
        None, description="Difference between original and consistency risk scores"
    )

    # Self-evaluation text
    self_eval_critique: str | None = Field(
        None, description="LLM's self-critique text identifying blind spots"
    )

    # Prompt improvements
    improvements: PromptImprovements = Field(
        ..., description="Suggestions for improving the prompt template"
    )


class AuditStatsResponse(BaseModel):
    """Aggregate audit statistics over a time period.

    Provides summary metrics for monitoring AI pipeline health and
    identifying optimization opportunities.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "total_events": 1250,
                "audited_events": 1200,
                "fully_evaluated_events": 950,
                "avg_quality_score": 4.2,
                "avg_consistency_rate": 0.92,
                "avg_enrichment_utilization": 0.78,
                "model_contribution_rates": {
                    "rtdetr": 0.98,
                    "florence": 0.85,
                    "clip": 0.45,
                    "clothing": 0.62,
                    "weather": 0.73,
                    "zones": 0.88,
                },
                "audits_by_day": [
                    {"date": "2026-01-01", "count": 180},
                    {"date": "2026-01-02", "count": 195},
                ],
            }
        }
    )

    total_events: int = Field(..., ge=0, description="Total events in the period")
    audited_events: int = Field(..., ge=0, description="Events with audit records")
    fully_evaluated_events: int = Field(
        ..., ge=0, description="Events with complete self-evaluation"
    )

    avg_quality_score: float | None = Field(
        None, ge=1, le=5, description="Average overall quality score (1-5)"
    )
    avg_consistency_rate: float | None = Field(None, description="Average consistency rate")
    avg_enrichment_utilization: float | None = Field(
        None, ge=0, le=1, description="Average enrichment utilization (0-1)"
    )

    # Model contribution rates (0-1)
    model_contribution_rates: dict[str, float] = Field(
        ..., description="Contribution rate for each AI model (0-1)"
    )

    # Audits by day for trending
    audits_by_day: list[dict] = Field(..., description="Daily audit counts for trend visualization")


class ModelLeaderboardEntry(BaseModel):
    """Single entry in the AI model leaderboard.

    Represents one model's performance metrics for ranking.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "model_name": "rtdetr",
                "contribution_rate": 0.98,
                "quality_correlation": 0.85,
                "event_count": 1180,
            }
        }
    )

    model_name: str = Field(..., description="Name of the AI model")
    contribution_rate: float = Field(
        ..., ge=0, le=1, description="Rate of contribution to events (0-1)"
    )
    quality_correlation: float | None = Field(
        None, ge=-1, le=1, description="Correlation between model use and quality score"
    )
    event_count: int = Field(..., ge=0, description="Number of events where model contributed")


class LeaderboardResponse(BaseModel):
    """Model leaderboard ranked by contribution rate.

    Lists AI models ordered by their contribution frequency, with
    quality correlation data to identify which models most improve results.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "entries": [
                    {
                        "model_name": "rtdetr",
                        "contribution_rate": 0.98,
                        "quality_correlation": 0.85,
                        "event_count": 1180,
                    },
                    {
                        "model_name": "zones",
                        "contribution_rate": 0.88,
                        "quality_correlation": 0.72,
                        "event_count": 1056,
                    },
                ],
                "period_days": 7,
            }
        }
    )

    entries: list[ModelLeaderboardEntry] = Field(..., description="Ranked list of model entries")
    period_days: int = Field(..., ge=1, le=90, description="Number of days in the analysis period")


class RecommendationItem(BaseModel):
    """Single prompt improvement recommendation.

    Aggregated from multiple event audits to identify common improvement opportunities.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "category": "missing_context",
                "suggestion": "Time since last detected motion or event",
                "frequency": 45,
                "priority": "high",
            }
        }
    )

    category: str = Field(
        ...,
        description="Category: missing_context, unused_data, model_gaps, format_suggestions",
    )
    suggestion: str = Field(..., description="The improvement suggestion text")
    frequency: int = Field(..., ge=0, description="Number of events mentioning this suggestion")
    priority: str = Field(
        ..., pattern="^(high|medium|low)$", description="Priority: high, medium, low"
    )


class ExampleImprovement(BaseModel):
    """Example of how a suggestion could improve a specific event's analysis.

    Shows the potential impact of applying a suggestion by comparing
    before/after risk scores for a real event.
    """

    model_config = ConfigDict(
        populate_by_name=True,
        json_schema_extra={
            "example": {
                "eventId": 142,
                "beforeScore": 65,
                "estimatedAfterScore": 40,
            }
        },
    )

    event_id: int = Field(
        ...,
        alias="eventId",
        description="The event ID used as an example",
    )
    before_score: int = Field(
        ...,
        ge=0,
        le=100,
        alias="beforeScore",
        description="Risk score with original prompt",
    )
    estimated_after_score: int = Field(
        ...,
        ge=0,
        le=100,
        alias="estimatedAfterScore",
        description="Estimated risk score if suggestion is applied",
    )


class EnrichedSuggestion(BaseModel):
    """Enhanced suggestion schema for smart prompt modification.

    Extends the basic recommendation with fields for:
    - Smart application: Identifies where and how to insert the suggestion
    - Learning mode: Explains impact with evidence from actual events

    Used by the Prompt Playground to transform AI audit recommendations
    into actionable prompt improvements through a progressive disclosure UX.
    """

    model_config = ConfigDict(
        populate_by_name=True,
        json_schema_extra={
            "example": {
                "category": "missing_context",
                "suggestion": "Time since last detected motion or event",
                "priority": "high",
                "frequency": 3,
                "targetSection": "Camera & Time Context",
                "insertionPoint": "append",
                "proposedVariable": "{time_since_last_event}",
                "proposedLabel": "Time Since Last Event:",
                "impactExplanation": "Adding time-since-last-event helps the AI distinguish between routine activity and unusual timing patterns. Events occurring shortly after previous motion are often less suspicious than isolated incidents.",
                "sourceEventIds": [142, 156, 189],
                "exampleImprovement": {
                    "eventId": 142,
                    "beforeScore": 65,
                    "estimatedAfterScore": 40,
                },
            }
        },
    )

    # Existing fields from RecommendationItem (no alias needed - single-word or standard)
    category: str = Field(
        ...,
        description="Suggestion category: missing_context, unused_data, model_gaps, format_suggestions",
    )
    suggestion: str = Field(
        ...,
        description="The improvement suggestion text",
    )
    priority: str = Field(
        ...,
        description="Priority level: high, medium, low",
    )
    frequency: int = Field(
        ...,
        ge=0,
        description="How many events mentioned this suggestion",
    )

    # Smart application fields
    target_section: str = Field(
        ...,
        alias="targetSection",
        description="Target section header in the prompt (e.g., 'Camera & Time Context')",
    )
    insertion_point: str = Field(
        ...,
        alias="insertionPoint",
        description="Where to insert: append, prepend, or replace",
    )
    proposed_variable: str = Field(
        ...,
        alias="proposedVariable",
        description="The variable to add (e.g., '{time_since_last_event}')",
    )
    proposed_label: str = Field(
        ...,
        alias="proposedLabel",
        description="Human-readable label for the variable (e.g., 'Time Since Last Event:')",
    )

    # Learning mode fields
    impact_explanation: str = Field(
        ...,
        alias="impactExplanation",
        description="Explanation of why this suggestion matters and its expected impact",
    )
    source_event_ids: list[int] = Field(
        default_factory=list,
        alias="sourceEventIds",
        description="IDs of events that triggered this suggestion",
    )

    # Optional improvement estimate
    example_improvement: ExampleImprovement | None = Field(
        None,
        alias="exampleImprovement",
        description="Optional example showing before/after scores for a specific event",
    )


class RecommendationsResponse(BaseModel):
    """Aggregated prompt improvement recommendations.

    Compiles suggestions from multiple event audits, prioritized by frequency and impact.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "recommendations": [
                    {
                        "category": "missing_context",
                        "suggestion": "Time since last detected motion or event",
                        "frequency": 45,
                        "priority": "high",
                    },
                    {
                        "category": "unused_data",
                        "suggestion": "Weather data rarely referenced in analysis",
                        "frequency": 28,
                        "priority": "medium",
                    },
                ],
                "total_events_analyzed": 950,
            }
        }
    )

    recommendations: list[RecommendationItem] = Field(
        ..., description="Prioritized list of improvement suggestions"
    )
    total_events_analyzed: int = Field(
        ..., ge=0, description="Number of fully-evaluated events analyzed"
    )


class BatchAuditRequest(BaseModel):
    """Request for batch audit processing.

    Specify criteria for selecting events to audit in bulk.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "limit": 100,
                "min_risk_score": 50,
                "force_reevaluate": False,
            }
        }
    )

    limit: int = Field(100, ge=1, le=1000, description="Maximum number of events to process")
    min_risk_score: int | None = Field(
        None, ge=0, le=100, description="Only process events with risk >= this score"
    )
    force_reevaluate: bool = Field(False, description="Re-evaluate even if already fully evaluated")


class BatchAuditResponse(BaseModel):
    """Response from batch audit processing.

    Reports how many events were queued for processing.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "queued_count": 25,
                "message": "Successfully processed 25 events for audit evaluation",
            }
        }
    )

    queued_count: int = Field(..., ge=0, description="Number of events processed")
    message: str = Field(..., description="Status message with details")


# =============================================================================
# Prompt Playground Schemas
# =============================================================================


class ModelName(str):
    """Supported AI model names for prompt management."""

    NEMOTRON = "nemotron"
    FLORENCE2 = "florence2"
    YOLO_WORLD = "yolo_world"
    XCLIP = "xclip"
    FASHION_CLIP = "fashion_clip"

    @classmethod
    def all_models(cls) -> list[str]:
        """Return all supported model names."""
        return [cls.NEMOTRON, cls.FLORENCE2, cls.YOLO_WORLD, cls.XCLIP, cls.FASHION_CLIP]


class NemotronConfig(BaseModel):
    """Configuration for Nemotron LLM risk analyzer."""

    system_prompt: str = Field(..., description="Full system prompt text for risk analysis")
    temperature: float = Field(0.7, ge=0.0, le=2.0, description="LLM temperature setting")
    max_tokens: int = Field(2048, ge=100, le=8192, description="Maximum tokens in response")


class Florence2Config(BaseModel):
    """Configuration for Florence-2 VQA model."""

    vqa_queries: list[str] = Field(
        ...,
        description="List of visual question-answering queries",
        min_length=1,
    )


class YoloWorldConfig(BaseModel):
    """Configuration for YOLO-World object detection."""

    object_classes: list[str] = Field(
        ...,
        description="List of object classes to detect",
        min_length=1,
    )
    confidence_threshold: float = Field(
        0.5,
        ge=0.0,
        le=1.0,
        description="Minimum confidence for detections",
    )


class XClipConfig(BaseModel):
    """Configuration for X-CLIP action recognition."""

    action_classes: list[str] = Field(
        ...,
        description="List of action classes to recognize",
        min_length=1,
    )


class FashionClipConfig(BaseModel):
    """Configuration for FashionCLIP clothing analysis."""

    clothing_categories: list[str] = Field(
        ...,
        description="List of clothing categories to classify",
        min_length=1,
    )
    suspicious_indicators: list[str] = Field(
        default_factory=lambda: ["all black", "face mask", "hoodie up", "gloves at night"],
        description="Clothing patterns that indicate suspicious activity",
    )


class PromptConfigUnion(BaseModel):
    """Union type for model configurations."""

    nemotron: NemotronConfig | None = None
    florence2: Florence2Config | None = None
    yolo_world: YoloWorldConfig | None = None
    xclip: XClipConfig | None = None
    fashion_clip: FashionClipConfig | None = None


class PromptVersionInfo(BaseModel):
    """Version information for a prompt configuration."""

    model_config = ConfigDict(from_attributes=True)

    version: int = Field(..., ge=1, description="Version number (1-indexed)")
    created_at: datetime = Field(..., description="When this version was created")
    created_by: str = Field("system", description="Who created this version")
    description: str | None = Field(None, description="Optional description of changes")


class ModelPromptResponse(BaseModel):
    """Response for a single model's prompt configuration.

    Contains the current active configuration for an AI model
    along with version tracking metadata.
    """

    model_config = ConfigDict(
        from_attributes=True,
        json_schema_extra={
            "example": {
                "model_name": "nemotron",
                "config": {
                    "system_prompt": "You are a security analyst reviewing camera footage...",
                    "temperature": 0.7,
                    "max_tokens": 2048,
                },
                "version": 5,
                "updated_at": "2026-01-03T10:30:00Z",
            }
        },
    )

    model_name: str = Field(..., description="Name of the AI model")
    config: dict = Field(..., description="Current configuration for this model")
    version: int = Field(..., ge=1, description="Current version number")
    updated_at: datetime = Field(..., description="When last updated")


class AllPromptsResponse(BaseModel):
    """Response containing prompts for all supported AI models.

    Returns configurations for all models in the AI pipeline:
    nemotron, florence2, yolo_world, xclip, and fashion_clip.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "prompts": {
                    "nemotron": {
                        "model_name": "nemotron",
                        "config": {
                            "system_prompt": "You are a security analyst...",
                            "temperature": 0.7,
                            "max_tokens": 2048,
                        },
                        "version": 5,
                        "updated_at": "2026-01-03T10:30:00Z",
                    },
                    "florence2": {
                        "model_name": "florence2",
                        "config": {"vqa_queries": ["What is happening?", "Who is present?"]},
                        "version": 3,
                        "updated_at": "2026-01-02T14:00:00Z",
                    },
                }
            }
        }
    )

    prompts: dict[str, ModelPromptResponse] = Field(
        ...,
        description="Dictionary mapping model names to their configurations",
    )


class PromptUpdateRequest(BaseModel):
    """Request to update a model's prompt configuration.

    The config dict must contain all required fields for the target model.
    Optional description helps track changes in version history.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "config": {
                    "system_prompt": "You are a security analyst reviewing camera footage. Analyze detections and assess risk level...",
                    "temperature": 0.7,
                    "max_tokens": 2048,
                },
                "description": "Added temporal context guidance for better risk assessment",
            }
        }
    )

    config: dict = Field(..., description="New configuration for the model")
    description: str | None = Field(None, description="Description of the changes")


class PromptUpdateResponse(BaseModel):
    """Response after updating a model's prompt configuration.

    Confirms the update was successful and returns the new version number.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "model_name": "nemotron",
                "version": 6,
                "message": "Configuration updated to version 6",
                "config": {
                    "system_prompt": "You are a security analyst...",
                    "temperature": 0.7,
                    "max_tokens": 2048,
                },
            }
        }
    )

    model_name: str = Field(..., description="Name of the updated model")
    version: int = Field(..., ge=1, description="New version number after update")
    message: str = Field(..., description="Success message with version info")
    config: dict = Field(..., description="The updated configuration")


class PromptTestRequest(BaseModel):
    """Request to test a modified prompt against an event.

    Used for A/B testing prompt changes by comparing results from
    the current and modified configurations on the same event.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "model": "nemotron",
                "config": {
                    "system_prompt": "You are a security analyst with expertise in behavioral analysis...",
                    "temperature": 0.6,
                    "max_tokens": 2048,
                },
                "event_id": 42,
            }
        }
    )

    model: str = Field(..., description="Model name to test (nemotron, florence2, etc.)")
    config: dict = Field(..., description="Modified configuration to test")
    event_id: int = Field(..., ge=1, description="Event ID to test against")


class PromptTestResultBefore(BaseModel):
    """Result from the original (current) prompt.

    Shows the baseline results that will be compared against the modified version.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "score": 65,
                "risk_level": "high",
                "summary": "Person detected at front door",
            }
        }
    )

    score: int = Field(..., ge=0, le=100, description="Risk score from original prompt")
    risk_level: str = Field(..., description="Risk level (low, medium, high, critical)")
    summary: str = Field(..., description="Summary from original analysis")


class PromptTestResultAfter(BaseModel):
    """Result from the modified prompt.

    Shows results after applying the proposed configuration changes.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "score": 45,
                "risk_level": "medium",
                "summary": "Delivery person at front door during business hours",
            }
        }
    )

    score: int = Field(..., ge=0, le=100, description="Risk score from modified prompt")
    risk_level: str = Field(..., description="Risk level (low, medium, high, critical)")
    summary: str = Field(..., description="Summary from modified analysis")


class PromptTestResponse(BaseModel):
    """Response from testing a modified prompt configuration.

    Provides a side-by-side comparison of before/after results
    to help evaluate whether the change improves risk analysis.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "before": {
                    "score": 65,
                    "risk_level": "high",
                    "summary": "Person detected at front door",
                },
                "after": {
                    "score": 45,
                    "risk_level": "medium",
                    "summary": "Delivery person at front door during business hours",
                },
                "improved": True,
                "inference_time_ms": 2150,
            }
        }
    )

    before: PromptTestResultBefore = Field(..., description="Results from original prompt")
    after: PromptTestResultAfter = Field(..., description="Results from modified prompt")
    improved: bool = Field(..., description="Whether the modification improved results")
    inference_time_ms: int = Field(..., ge=0, description="Time taken for inference in ms")


class PromptHistoryEntry(BaseModel):
    """A single entry in prompt version history.

    Represents one version of a model's configuration with metadata
    about who created it and why.
    """

    model_config = ConfigDict(
        from_attributes=True,
        json_schema_extra={
            "example": {
                "version": 5,
                "config": {
                    "system_prompt": "You are a security analyst...",
                    "temperature": 0.7,
                    "max_tokens": 2048,
                },
                "created_at": "2026-01-03T10:30:00Z",
                "created_by": "user",
                "description": "Added temporal context guidance",
            }
        },
    )

    version: int = Field(..., ge=1, description="Version number")
    config: dict = Field(..., description="Configuration at this version")
    created_at: datetime = Field(..., description="When this version was created")
    created_by: str = Field("system", description="Who created this version")
    description: str | None = Field(None, description="Description of changes")


class PromptHistoryResponse(BaseModel):
    """Response containing version history for a model's prompts.

    Returns all versions ordered by version number descending (newest first),
    with pagination support through limit/offset query parameters.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "model_name": "nemotron",
                "versions": [
                    {
                        "version": 5,
                        "config": {"system_prompt": "...", "temperature": 0.7},
                        "created_at": "2026-01-03T10:30:00Z",
                        "created_by": "user",
                        "description": "Added temporal context guidance",
                    },
                    {
                        "version": 4,
                        "config": {"system_prompt": "...", "temperature": 0.8},
                        "created_at": "2026-01-02T14:00:00Z",
                        "created_by": "user",
                        "description": None,
                    },
                ],
                "total_versions": 5,
            }
        }
    )

    model_name: str = Field(..., description="Name of the AI model")
    versions: list[PromptHistoryEntry] = Field(
        ..., description="List of version entries, newest first"
    )
    total_versions: int = Field(..., ge=0, description="Total number of versions available")


class PromptRestoreRequest(BaseModel):
    """Request to restore a specific version of a prompt.

    The restore operation creates a new version with the configuration
    from the specified version. Original versions are preserved.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "description": "Reverting to version 3 due to quality regression",
            }
        }
    )

    description: str | None = Field(
        None,
        description="Optional description for the restore action",
    )


class PromptRestoreResponse(BaseModel):
    """Response after restoring a prompt version.

    Confirms which version was restored and the new version number created.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "model_name": "nemotron",
                "restored_version": 3,
                "new_version": 6,
                "message": "Restored version 3 as new version 6",
            }
        }
    )

    model_name: str = Field(..., description="Name of the AI model")
    restored_version: int = Field(..., ge=1, description="Version that was restored")
    new_version: int = Field(..., ge=1, description="New version number created")
    message: str = Field(..., description="Success message with details")


class PromptExportResponse(BaseModel):
    """Response containing all prompt configurations for export.

    Provides a complete snapshot of all AI model configurations
    that can be imported to another instance or used for backup.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "exported_at": "2026-01-03T10:30:00Z",
                "version": "1.0",
                "prompts": {
                    "nemotron": {
                        "system_prompt": "You are a security analyst...",
                        "temperature": 0.7,
                        "max_tokens": 2048,
                    },
                    "florence2": {
                        "vqa_queries": ["What is happening?", "Who is present?"],
                    },
                },
            }
        }
    )

    exported_at: datetime = Field(..., description="When the export was created")
    version: str = Field("1.0", description="Export format version")
    prompts: dict[str, dict] = Field(
        ...,
        description="All model configurations keyed by model name",
    )


class PromptImportRequest(BaseModel):
    """Request to import prompt configurations.

    Import configurations for multiple models at once.
    By default, existing configurations are skipped unless overwrite=true.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "prompts": {
                    "nemotron": {
                        "system_prompt": "You are a security analyst...",
                        "temperature": 0.7,
                        "max_tokens": 2048,
                    },
                    "florence2": {
                        "vqa_queries": ["What is happening?", "Who is present?"],
                    },
                },
                "overwrite": False,
            }
        }
    )

    prompts: dict[str, dict] = Field(
        ...,
        description="Model configurations to import, keyed by model name",
    )
    overwrite: bool = Field(
        False,
        description="Whether to overwrite existing configurations",
    )


class PromptImportResponse(BaseModel):
    """Response after importing prompt configurations.

    Reports the results of the import operation including how many
    models were imported, skipped, or encountered errors.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "imported_count": 3,
                "skipped_count": 2,
                "errors": [],
                "message": "Imported 3 model(s), skipped 2 (already exist)",
            }
        }
    )

    imported_count: int = Field(..., ge=0, description="Number of models imported")
    skipped_count: int = Field(..., ge=0, description="Number of models skipped")
    errors: list[str] = Field(default_factory=list, description="Any errors encountered")
    message: str = Field(..., description="Summary message of import results")


# =============================================================================
# Database-backed Prompt Config Schemas (for Playground Save functionality)
# =============================================================================


class PromptConfigRequest(BaseModel):
    """Request to update a model's prompt configuration (database-backed).

    Used by the Prompt Playground "Save" functionality to persist
    prompt configurations to the database.
    """

    model_config = ConfigDict(
        from_attributes=True,
        populate_by_name=True,
        json_schema_extra={
            "example": {
                "systemPrompt": "You are a security analyst reviewing camera footage. Analyze the detected objects and assess the risk level...",
                "temperature": 0.7,
                "maxTokens": 2048,
            }
        },
    )

    system_prompt: str = Field(
        ...,
        min_length=1,
        alias="systemPrompt",
        description="Full system prompt text for the model",
    )
    temperature: float = Field(
        default=0.7,
        ge=0.0,
        le=2.0,
        description="LLM temperature setting (0-2)",
    )
    max_tokens: int = Field(
        default=2048,
        ge=100,
        le=8192,
        alias="maxTokens",
        description="Maximum tokens in response (100-8192)",
    )


class PromptConfigResponse(BaseModel):
    """Response containing a model's prompt configuration (database-backed).

    Returned when retrieving or updating prompt configurations.
    """

    model_config = ConfigDict(
        from_attributes=True,
        populate_by_name=True,
        json_schema_extra={
            "example": {
                "model": "nemotron",
                "systemPrompt": "You are a security analyst reviewing camera footage...",
                "temperature": 0.7,
                "maxTokens": 2048,
                "version": 3,
                "updatedAt": "2026-01-03T10:30:00Z",
            }
        },
    )

    model: str = Field(..., description="Model name")
    system_prompt: str = Field(
        ...,
        alias="systemPrompt",
        description="Full system prompt text for the model",
    )
    temperature: float = Field(..., description="LLM temperature setting (0-2)")
    max_tokens: int = Field(
        ...,
        alias="maxTokens",
        description="Maximum tokens in response (100-8192)",
    )
    version: int = Field(..., ge=1, description="Configuration version number")
    updated_at: datetime = Field(
        ...,
        alias="updatedAt",
        description="When the configuration was last updated",
    )


# =============================================================================
# A/B Testing Schemas (Prompt Playground)
# =============================================================================


class CustomTestPromptRequest(BaseModel):
    """Request to test a custom prompt against an existing event.

    This is used for A/B testing in the Prompt Playground - testing a
    modified prompt without persisting results to the database.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "event_id": 42,
                "custom_prompt": "You are an expert security analyst. Analyze the following detections and assess the risk level considering time of day, location, and behavioral patterns...",
                "temperature": 0.7,
                "max_tokens": 2048,
                "model": "nemotron",
            }
        }
    )

    event_id: int = Field(..., ge=1, description="Event ID to test the prompt against")
    custom_prompt: str = Field(..., min_length=1, description="Custom prompt text to test")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="LLM temperature setting")
    max_tokens: int = Field(default=2048, ge=100, le=8192, description="Maximum tokens in response")
    model: str = Field(default="nemotron", description="Model name to use for testing")


class CustomTestPromptResponse(BaseModel):
    """Response from testing a custom prompt against an event.

    Results are NOT persisted - this is for A/B testing only.
    """

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "risk_score": 45,
                "risk_level": "medium",
                "reasoning": "Person detected approaching front door during normal business hours. No suspicious behavior patterns observed - appears to be routine delivery activity based on movement patterns and timing.",
                "summary": "Routine visitor activity at front entrance.",
                "entities": [
                    {"type": "person", "confidence": 0.92, "bbox": [120, 80, 340, 520]},
                    {"type": "package", "confidence": 0.87, "bbox": [180, 400, 260, 480]},
                ],
                "flags": [],
                "recommended_action": "Review - Check event details when convenient",
                "processing_time_ms": 1250,
                "tokens_used": 825,
            }
        }
    )

    risk_score: int = Field(..., ge=0, le=100, description="Computed risk score (0-100)")
    risk_level: str = Field(..., description="Risk level: low, medium, high, or critical")
    reasoning: str = Field(..., description="LLM reasoning for the risk assessment")
    summary: str = Field(..., description="Brief summary of the event analysis")
    entities: list[dict] = Field(
        default_factory=list, description="Detected entities in the analysis"
    )
    flags: list[dict] = Field(
        default_factory=list, description="Risk flags identified in the analysis"
    )
    recommended_action: str = Field(
        default="", description="Recommended action based on risk analysis"
    )
    processing_time_ms: int = Field(
        ..., ge=0, description="Time taken for inference in milliseconds"
    )
    tokens_used: int = Field(..., ge=0, description="Number of tokens used in inference")
