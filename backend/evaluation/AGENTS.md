# Evaluation Module

## Purpose

This module provides tools for evaluating and comparing Nemotron prompt templates against ground truth scenarios. It is designed to work with synthetic scenarios generated by NeMo Data Designer.

**Note:** This module requires the `nemo` optional dependency group: `uv sync --extra nemo`

## Directory Structure

```
backend/evaluation/
├── AGENTS.md              # This file
├── __init__.py            # Module exports with lazy imports
├── ab_experiment_runner.py # A/B experiment statistical analysis (NEM-3731)
├── harness.py             # PromptEvaluator class and CLI
├── metrics.py             # Score calculation functions
├── prompt_eval_dataset.py # Synthetic dataset loader
├── prompt_evaluator.py    # Prediction evaluation and metrics
└── reports.py             # Report generation (JSON, HTML)
```

## Key Components

### harness.py

The main evaluation harness for running scenarios through prompt templates.

| Class/Function              | Description                                                  |
| --------------------------- | ------------------------------------------------------------ |
| `PromptEvaluator`           | Main evaluator class for running scenarios through templates |
| `generate_mock_scenarios()` | Generates mock scenarios for testing without NeMo            |
| `PromptTemplate`            | Dataclass defining a prompt template                         |
| `EvaluationResult`          | Dataclass holding evaluation results                         |
| `PROMPT_TEMPLATES`          | List of 5 available prompt templates                         |

**Prompt Templates:**

1. `basic` - Basic prompt without enrichment
2. `enriched` - With zone/baseline context
3. `full_enriched` - With vision enrichment (plates, faces, OCR)
4. `vision_enhanced` - With Florence-2 attributes and re-ID
5. `model_zoo_enhanced` - With all enrichment models

### metrics.py

Functions for calculating evaluation metrics.

| Function                                              | Description                                  |
| ----------------------------------------------------- | -------------------------------------------- |
| `calculate_risk_deviation(actual, expected_range)`    | Deviation from ground truth risk range       |
| `calculate_reasoning_similarity(actual, expected)`    | Jaccard similarity of reasoning texts        |
| `calculate_key_point_coverage(reasoning, key_points)` | Fraction of key points mentioned             |
| `aggregate_metrics(results_df)`                       | Aggregate metrics by template, scenario type |
| `rank_templates(metrics)`                             | Rank templates by composite score            |

### reports.py

Functions for generating evaluation reports.

| Function                                 | Description                        |
| ---------------------------------------- | ---------------------------------- |
| `generate_json_report(results, metrics)` | JSON report with full metrics      |
| `generate_html_report(results, metrics)` | HTML report with tables and charts |
| `save_report(report, path, format)`      | Save report to file                |
| `generate_summary_table(metrics)`        | Text summary for terminal output   |

### prompt_eval_dataset.py

Loader for synthetic evaluation scenarios from `data/synthetic/`.

| Class/Function                          | Description                                         |
| --------------------------------------- | --------------------------------------------------- |
| `PromptEvalSample`                      | Dataclass for a single evaluation sample            |
| `load_synthetic_eval_dataset(data_dir)` | Load all scenarios from synthetic data directory    |
| `get_samples_by_category(samples)`      | Group samples by category (normal, suspicious, etc) |
| `get_samples_by_risk_level(samples)`    | Group samples by expected risk level                |
| `filter_samples_with_media(samples)`    | Filter to samples with images/videos                |
| `get_scenario_summary(samples)`         | Get summary statistics of loaded samples            |

### prompt_evaluator.py

Functions for evaluating predictions against expected values.

| Class/Function                                   | Description                                |
| ------------------------------------------------ | ------------------------------------------ |
| `EvaluationResult`                               | Result from evaluating a single prediction |
| `evaluate_prediction(sample, score, level)`      | Evaluate one prediction against expected   |
| `calculate_metrics(results)`                     | Calculate aggregate metrics from results   |
| `evaluate_batch(samples, predictions)`           | Evaluate multiple predictions at once      |
| `summarize_results(results)`                     | Generate human-readable summary            |
| `get_misclassified(results, by_score, by_level)` | Get results where predictions didn't match |

### ab_experiment_runner.py

A/B experiment statistical analysis with significance testing (NEM-3731).

| Class/Function                                | Description                                     |
| --------------------------------------------- | ----------------------------------------------- |
| `ExperimentResults`                           | Dataclass holding statistical analysis results  |
| `select_variant(experiment)`                  | Random variant selection based on traffic split |
| `analyze_experiment(control, variant, alpha)` | Two-sample t-test with Cohen's d effect size    |
| `summarize_results(results)`                  | Human-readable summary with interpretation      |

**Usage:**

```python
from backend.evaluation import analyze_experiment, summarize_ab_results
from backend.config import get_experiment

# Collect performance scores from A/B test
control_scores = [0.80, 0.82, 0.79, 0.81, 0.83]
variant_scores = [0.88, 0.90, 0.87, 0.89, 0.91]

# Analyze for statistical significance
results = analyze_experiment(control_scores, variant_scores)
print(summarize_ab_results(results))
# Shows: t-statistic, p-value, effect size, significance determination
```

## Synthetic Data Structure

The `data/synthetic/` directory contains pre-generated evaluation scenarios:

```
data/synthetic/
├── normal/              # Low-risk scenarios
│   ├── delivery_driver_*/
│   ├── pet_activity_*/
│   └── ...
├── suspicious/          # Medium-risk scenarios
│   ├── casing_*/
│   ├── loitering_*/
│   └── ...
└── threats/             # High-risk scenarios
    ├── break_in_attempt_*/
    ├── package_theft_*/
    └── ...
```

Each scenario folder contains:

- `expected_labels.json` - Ground truth with risk score ranges
- `scenario_spec.json` - Full scenario specification
- `metadata.json` - Generation metadata
- `media/` - Generated images/videos (optional)

## Usage

### Programmatic Usage

```python
import asyncio
from backend.evaluation import PromptEvaluator, generate_json_report

# Create evaluator (mock mode for testing)
evaluator = PromptEvaluator(mock_mode=True)

# Generate mock scenarios (or load from parquet)
from backend.evaluation.harness import generate_mock_scenarios
scenarios = generate_mock_scenarios(50)

# Run evaluation
results = asyncio.run(evaluator.evaluate_all(scenarios))

# Generate report
metrics = evaluator.get_metrics()
report = generate_json_report(results, metrics)
```

### CLI Usage

```bash
# Run with mock data (no Nemotron required)
uv run python -m backend.evaluation.harness --mock --output reports/test.json

# Run with real scenarios
uv run python -m backend.evaluation.harness \
    --scenarios backend/tests/fixtures/synthetic/scenarios.parquet \
    --output reports/evaluation.json

# Generate HTML report
uv run python -m backend.evaluation.harness --mock --output reports/report.html --format html
```

### Loading Synthetic Evaluation Data

```python
from backend.evaluation import (
    load_synthetic_eval_dataset,
    get_samples_by_category,
    evaluate_prediction,
    calculate_metrics,
)

# Load all synthetic samples
samples = load_synthetic_eval_dataset()
print(f"Loaded {len(samples)} samples")

# Group by category
by_category = get_samples_by_category(samples)
for cat, cat_samples in by_category.items():
    print(f"  {cat}: {len(cat_samples)} samples")

# Evaluate predictions (example with mock scores)
results = []
for sample in samples:
    # In practice, get these from your model
    mock_score = 50
    mock_level = "medium"
    result = evaluate_prediction(sample, mock_score, mock_level)
    results.append(result)

# Calculate aggregate metrics
metrics = calculate_metrics(results)
print(f"Accuracy: {metrics['accuracy']:.2%}")
print(f"Level accuracy: {metrics['level_accuracy']:.2%}")
```

## Design Decisions

1. **Lazy pandas imports**: The module can be imported without pandas installed. Pandas is only required when actually running evaluations.

2. **Mock mode**: Allows testing the harness without a running Nemotron service.

3. **Template compatibility**: Uses the same prompt templates as the production NemotronAnalyzer from `backend/services/prompts.py`.

4. **Jaccard similarity for MVP**: Uses word overlap (Jaccard) for reasoning similarity instead of embeddings to avoid additional model dependencies.

## Related Documentation

- [NeMo Data Designer Integration Design](../../docs/plans/2026-01-21-nemo-data-designer-integration-design.md)
- [Scenario Config Models](../../tools/nemo_data_designer/config.py)
- [Nemotron Prompt Templates](../services/prompts.py)

## Testing

Tests are in `backend/tests/unit/evaluation/` and are skipped when pandas is not available:

```bash
# Run tests (skipped if pandas not installed)
uv run pytest backend/tests/unit/evaluation/ -v

# Run with nemo extra for full tests
uv sync --extra nemo
uv run pytest backend/tests/unit/evaluation/ -v
```
