# Evaluation Module

## Purpose

This module provides tools for evaluating and comparing Nemotron prompt templates against ground truth scenarios. It is designed to work with synthetic scenarios generated by NeMo Data Designer.

**Note:** This module requires the `nemo` optional dependency group: `uv sync --extra nemo`

## Directory Structure

```
backend/evaluation/
├── AGENTS.md           # This file
├── __init__.py         # Module exports with lazy imports
├── harness.py          # PromptEvaluator class and CLI
├── metrics.py          # Score calculation functions
└── reports.py          # Report generation (JSON, HTML)
```

## Key Components

### harness.py

The main evaluation harness for running scenarios through prompt templates.

| Class/Function              | Description                                                  |
| --------------------------- | ------------------------------------------------------------ |
| `PromptEvaluator`           | Main evaluator class for running scenarios through templates |
| `generate_mock_scenarios()` | Generates mock scenarios for testing without NeMo            |
| `PromptTemplate`            | Dataclass defining a prompt template                         |
| `EvaluationResult`          | Dataclass holding evaluation results                         |
| `PROMPT_TEMPLATES`          | List of 5 available prompt templates                         |

**Prompt Templates:**

1. `basic` - Basic prompt without enrichment
2. `enriched` - With zone/baseline context
3. `full_enriched` - With vision enrichment (plates, faces, OCR)
4. `vision_enhanced` - With Florence-2 attributes and re-ID
5. `model_zoo_enhanced` - With all enrichment models

### metrics.py

Functions for calculating evaluation metrics.

| Function                                              | Description                                  |
| ----------------------------------------------------- | -------------------------------------------- |
| `calculate_risk_deviation(actual, expected_range)`    | Deviation from ground truth risk range       |
| `calculate_reasoning_similarity(actual, expected)`    | Jaccard similarity of reasoning texts        |
| `calculate_key_point_coverage(reasoning, key_points)` | Fraction of key points mentioned             |
| `aggregate_metrics(results_df)`                       | Aggregate metrics by template, scenario type |
| `rank_templates(metrics)`                             | Rank templates by composite score            |

### reports.py

Functions for generating evaluation reports.

| Function                                 | Description                        |
| ---------------------------------------- | ---------------------------------- |
| `generate_json_report(results, metrics)` | JSON report with full metrics      |
| `generate_html_report(results, metrics)` | HTML report with tables and charts |
| `save_report(report, path, format)`      | Save report to file                |
| `generate_summary_table(metrics)`        | Text summary for terminal output   |

## Usage

### Programmatic Usage

```python
import asyncio
from backend.evaluation import PromptEvaluator, generate_json_report

# Create evaluator (mock mode for testing)
evaluator = PromptEvaluator(mock_mode=True)

# Generate mock scenarios (or load from parquet)
from backend.evaluation.harness import generate_mock_scenarios
scenarios = generate_mock_scenarios(50)

# Run evaluation
results = asyncio.run(evaluator.evaluate_all(scenarios))

# Generate report
metrics = evaluator.get_metrics()
report = generate_json_report(results, metrics)
```

### CLI Usage

```bash
# Run with mock data (no Nemotron required)
uv run python -m backend.evaluation.harness --mock --output reports/test.json

# Run with real scenarios
uv run python -m backend.evaluation.harness \
    --scenarios backend/tests/fixtures/synthetic/scenarios.parquet \
    --output reports/evaluation.json

# Generate HTML report
uv run python -m backend.evaluation.harness --mock --output reports/report.html --format html
```

## Design Decisions

1. **Lazy pandas imports**: The module can be imported without pandas installed. Pandas is only required when actually running evaluations.

2. **Mock mode**: Allows testing the harness without a running Nemotron service.

3. **Template compatibility**: Uses the same prompt templates as the production NemotronAnalyzer from `backend/services/prompts.py`.

4. **Jaccard similarity for MVP**: Uses word overlap (Jaccard) for reasoning similarity instead of embeddings to avoid additional model dependencies.

## Related Documentation

- [NeMo Data Designer Integration Design](/docs/plans/2026-01-21-nemo-data-designer-integration-design.md)
- [Scenario Config Models](/tools/nemo_data_designer/config.py)
- [Nemotron Prompt Templates](/backend/services/prompts.py)

## Testing

Tests are in `backend/tests/unit/evaluation/` and are skipped when pandas is not available:

```bash
# Run tests (skipped if pandas not installed)
uv run pytest backend/tests/unit/evaluation/ -v

# Run with nemo extra for full tests
uv sync --extra nemo
uv run pytest backend/tests/unit/evaluation/ -v
```
