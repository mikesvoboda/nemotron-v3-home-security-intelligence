"""Evaluation harness for Nemotron prompt template comparison.

This module provides tools for evaluating and comparing prompt templates
against ground truth scenarios generated by NeMo Data Designer.

**Note:** This module requires the `nemo` optional dependency group.
Install with: `uv sync --extra nemo`

Key Components:
    - PromptEvaluator: Runs scenarios through prompt templates and captures results
    - calculate_risk_deviation: Measures deviation from expected risk ranges
    - calculate_key_point_coverage: Measures reasoning quality
    - generate_json_report: Creates structured evaluation reports
    - generate_html_report: Creates visual evaluation reports

Example Usage:
    >>> from backend.evaluation import PromptEvaluator, generate_json_report
    >>>
    >>> evaluator = PromptEvaluator()
    >>> results = await evaluator.evaluate_all(scenarios_df)
    >>> report = generate_json_report(results, evaluator.aggregate_metrics())
"""

from __future__ import annotations

from typing import Any

# Lazy imports to handle optional pandas dependency
# These functions don't require pandas at import time
from backend.evaluation.metrics import (
    calculate_key_point_coverage,
    calculate_reasoning_similarity,
    calculate_risk_deviation,
)

__all__ = [
    "PromptEvaluator",
    "aggregate_metrics",
    "calculate_key_point_coverage",
    "calculate_reasoning_similarity",
    "calculate_risk_deviation",
    "generate_html_report",
    "generate_json_report",
    "save_report",
]


def __getattr__(name: str) -> Any:
    """Lazy import for pandas-dependent components.

    This allows importing the basic metrics functions without pandas,
    while still providing access to the full harness and report generation
    when pandas is available.
    """
    if name == "PromptEvaluator":
        from backend.evaluation.harness import PromptEvaluator

        return PromptEvaluator
    if name == "aggregate_metrics":
        from backend.evaluation.metrics import aggregate_metrics

        return aggregate_metrics
    if name == "generate_json_report":
        from backend.evaluation.reports import generate_json_report

        return generate_json_report
    if name == "generate_html_report":
        from backend.evaluation.reports import generate_html_report

        return generate_html_report
    if name == "save_report":
        from backend.evaluation.reports import save_report

        return save_report
    raise AttributeError(f"module {__name__!r} has no attribute {name!r}")
