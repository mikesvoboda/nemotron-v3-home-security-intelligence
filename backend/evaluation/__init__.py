"""Evaluation harness for Nemotron prompt template comparison.

This module provides tools for evaluating and comparing prompt templates
against ground truth scenarios generated by NeMo Data Designer.

**Note:** Some components require the `nemo` optional dependency group.
Install with: `uv sync --extra nemo`

Key Components:
    - PromptEvaluator: Runs scenarios through prompt templates and captures results
    - PromptEvalSample: Data class for synthetic evaluation samples
    - EvaluationResult: Result from evaluating a single prediction
    - load_synthetic_eval_dataset: Load synthetic scenarios for evaluation
    - evaluate_prediction: Evaluate predictions against expected values
    - calculate_risk_deviation: Measures deviation from expected risk ranges
    - calculate_key_point_coverage: Measures reasoning quality
    - generate_json_report: Creates structured evaluation reports
    - generate_html_report: Creates visual evaluation reports

Example Usage:
    >>> from backend.evaluation import PromptEvaluator, generate_json_report
    >>>
    >>> evaluator = PromptEvaluator()
    >>> results = await evaluator.evaluate_all(scenarios_df)
    >>> report = generate_json_report(results, evaluator.aggregate_metrics())

    # Or for synthetic data evaluation:
    >>> from backend.evaluation import load_synthetic_eval_dataset, evaluate_prediction
    >>>
    >>> samples = load_synthetic_eval_dataset()
    >>> result = evaluate_prediction(samples[0], actual_score=50, actual_level="medium")
"""

from __future__ import annotations

from typing import Any

# A/B experiment runner (NEM-3731)
from backend.evaluation.ab_experiment_runner import (
    ExperimentResults,
    analyze_experiment,
    select_variant,
)
from backend.evaluation.ab_experiment_runner import (
    summarize_results as summarize_ab_results,
)

# Lazy imports to handle optional pandas dependency
# These functions don't require pandas at import time
from backend.evaluation.metrics import (
    calculate_key_point_coverage,
    calculate_reasoning_similarity,
    calculate_risk_deviation,
)

# Direct imports for synthetic dataset evaluation (no pandas required)
from backend.evaluation.prompt_eval_dataset import (
    PromptEvalSample,
    filter_samples_with_media,
    get_samples_by_category,
    get_samples_by_risk_level,
    get_scenario_summary,
    load_synthetic_eval_dataset,
)
from backend.evaluation.prompt_evaluator import (
    EvaluationResult,
    calculate_metrics,
    evaluate_batch,
    evaluate_prediction,
    get_misclassified,
    summarize_results,
)

__all__ = [
    "EvaluationResult",
    "ExperimentResults",
    "PromptEvalSample",
    "PromptEvaluator",
    "aggregate_metrics",
    "analyze_experiment",
    "calculate_key_point_coverage",
    "calculate_metrics",
    "calculate_reasoning_similarity",
    "calculate_risk_deviation",
    "evaluate_batch",
    "evaluate_prediction",
    "filter_samples_with_media",
    "generate_html_report",
    "generate_json_report",
    "get_misclassified",
    "get_samples_by_category",
    "get_samples_by_risk_level",
    "get_scenario_summary",
    "load_synthetic_eval_dataset",
    "save_report",
    "select_variant",
    "summarize_ab_results",
    "summarize_results",
]


def __getattr__(name: str) -> Any:
    """Lazy import for pandas-dependent components.

    This allows importing the basic metrics functions without pandas,
    while still providing access to the full harness and report generation
    when pandas is available.
    """
    if name == "PromptEvaluator":
        from backend.evaluation.harness import PromptEvaluator

        return PromptEvaluator
    if name == "aggregate_metrics":
        from backend.evaluation.metrics import aggregate_metrics

        return aggregate_metrics
    if name == "generate_json_report":
        from backend.evaluation.reports import generate_json_report

        return generate_json_report
    if name == "generate_html_report":
        from backend.evaluation.reports import generate_html_report

        return generate_html_report
    if name == "save_report":
        from backend.evaluation.reports import save_report

        return save_report
    raise AttributeError(f"module {__name__!r} has no attribute {name!r}")
