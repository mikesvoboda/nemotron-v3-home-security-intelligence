"""Integration tests for database transaction isolation levels.

Tests verify PostgreSQL isolation level behavior:
- READ COMMITTED (default): prevents dirty reads
- REPEATABLE READ: prevents dirty and non-repeatable reads
- SERIALIZABLE: prevents dirty, non-repeatable, and phantom reads
- Concurrent read/write behavior
- Deadlock detection and recovery

Reference: NEM-2057
"""

from __future__ import annotations

import asyncio
from datetime import UTC, datetime

import pytest
from sqlalchemy import select, text
from sqlalchemy.exc import DBAPIError, OperationalError

from backend.core.database import get_session, get_session_factory
from backend.models.camera import Camera
from backend.models.detection import Detection
from backend.tests.conftest import unique_id

pytestmark = pytest.mark.integration


class TestReadCommittedIsolation:
    """Tests for READ COMMITTED isolation level (PostgreSQL default).

    READ COMMITTED guarantees:
    - No dirty reads (can't see uncommitted changes from other transactions)
    - Non-repeatable reads ARE possible (data can change between reads)
    - Phantom reads ARE possible (new rows can appear between reads)
    """

    @pytest.mark.asyncio
    async def test_read_committed_prevents_dirty_reads(self, integration_db: str) -> None:
        """Verify that uncommitted changes are not visible to other transactions."""
        camera_id = unique_id("dirty_read_cam")

        # Session 1: Create camera but don't commit
        factory = get_session_factory()
        session1 = factory()

        try:
            camera = Camera(
                id=camera_id,
                name="Dirty Read Test Camera",
                folder_path=f"/export/foscam/{camera_id}",
                status="online",
            )
            session1.add(camera)
            await session1.flush()

            # Session 2: Should NOT see the uncommitted camera
            async with get_session() as session2:
                result = await session2.execute(select(Camera).where(Camera.id == camera_id))
                visible_camera = result.scalar_one_or_none()

                # Dirty read prevented: session2 can't see uncommitted data
                assert visible_camera is None

            # Session 1: Commit the camera
            await session1.commit()

        finally:
            await session1.close()

        # Session 3: Now the camera should be visible
        async with get_session() as session3:
            result = await session3.execute(select(Camera).where(Camera.id == camera_id))
            visible_camera = result.scalar_one_or_none()
            assert visible_camera is not None
            assert visible_camera.id == camera_id

        # Cleanup (safe: camera_id is generated by unique_id, not user input)
        async with get_session() as cleanup_session:
            await cleanup_session.execute(
                text(f"DELETE FROM cameras WHERE id = '{camera_id}'")  # noqa: S608  # nosemgrep
            )
            await cleanup_session.commit()

    @pytest.mark.asyncio
    async def test_read_committed_allows_non_repeatable_reads(self, integration_db: str) -> None:
        """Verify that data can change between reads in same transaction."""
        camera_id = unique_id("non_repeatable_cam")

        # Session 1: Create and commit a camera
        async with get_session() as session1:
            camera = Camera(
                id=camera_id,
                name="Original Name",
                folder_path=f"/export/foscam/{camera_id}",
                status="online",
            )
            session1.add(camera)
            await session1.commit()

        # Session 2: Start transaction and read camera
        factory = get_session_factory()
        session2 = factory()

        try:
            # First read
            result = await session2.execute(select(Camera).where(Camera.id == camera_id))
            first_read = result.scalar_one()
            first_name = first_read.name  # Store value before object is expired
            assert first_name == "Original Name"

            # Session 3: Update the camera and commit
            async with get_session() as session3:
                result = await session3.execute(select(Camera).where(Camera.id == camera_id))
                camera_to_update = result.scalar_one()
                camera_to_update.name = "Updated Name"
                await session3.commit()

            # Session 2: Second read in same transaction (force new query with expire_all)
            session2.expire_all()  # Expire cached objects to force fresh query
            result = await session2.execute(select(Camera).where(Camera.id == camera_id))
            second_read = result.scalar_one()

            # Non-repeatable read: data changed between reads in same transaction
            assert second_read.name == "Updated Name"
            assert first_name != second_read.name

        finally:
            await session2.close()

        # Cleanup (safe: camera_id is generated by unique_id, not user input)
        async with get_session() as cleanup_session:
            await cleanup_session.execute(
                text(f"DELETE FROM cameras WHERE id = '{camera_id}'")  # noqa: S608  # nosemgrep
            )
            await cleanup_session.commit()

    @pytest.mark.asyncio
    async def test_read_committed_allows_phantom_reads(self, integration_db: str) -> None:
        """Verify that new rows can appear between reads in same transaction."""
        camera_prefix = unique_id("phantom")

        # Session 1: Start transaction and count cameras with prefix
        factory = get_session_factory()
        session1 = factory()

        try:
            # First count
            result = await session1.execute(
                select(Camera).where(Camera.id.like(f"{camera_prefix}%"))
            )
            first_count = len(result.scalars().all())
            assert first_count == 0  # No cameras yet

            # Session 2: Insert a new camera and commit
            camera_id = unique_id(f"{camera_prefix}_new")
            async with get_session() as session2:
                camera = Camera(
                    id=camera_id,
                    name="Phantom Camera",
                    folder_path=f"/export/foscam/{camera_id}",
                )
                session2.add(camera)
                await session2.commit()

            # Session 1: Second count in same transaction
            result = await session1.execute(
                select(Camera).where(Camera.id.like(f"{camera_prefix}%"))
            )
            second_count = len(result.scalars().all())

            # Phantom read: new row appeared between reads
            assert second_count == 1
            assert first_count != second_count

        finally:
            await session1.close()

        # Cleanup (safe: camera_prefix is generated by unique_id, not user input)
        async with get_session() as cleanup_session:
            await cleanup_session.execute(
                text(f"DELETE FROM cameras WHERE id LIKE '{camera_prefix}%'")  # noqa: S608  # nosemgrep
            )
            await cleanup_session.commit()


class TestConcurrentReadBehavior:
    """Tests for concurrent read operations.

    Verifies that:
    - Multiple readers don't block each other
    - Reads don't block writes (in READ COMMITTED)
    - Consistent snapshot behavior within transactions
    """

    @pytest.mark.asyncio
    async def test_concurrent_reads_dont_block(self, integration_db: str) -> None:
        """Verify multiple concurrent reads can execute simultaneously."""
        camera_id = unique_id("concurrent_read_cam")

        # Create test camera
        async with get_session() as setup_session:
            camera = Camera(
                id=camera_id,
                name="Concurrent Read Camera",
                folder_path=f"/export/foscam/{camera_id}",
            )
            setup_session.add(camera)
            await setup_session.commit()

        read_times: list[float] = []

        async def concurrent_read(reader_id: int) -> None:
            """Perform a read operation and record execution time."""
            import time

            start = time.perf_counter()
            async with get_session() as session:
                result = await session.execute(select(Camera).where(Camera.id == camera_id))
                camera = result.scalar_one()
                assert camera is not None
                # Simulate some processing
                await asyncio.sleep(0.01)
            duration = time.perf_counter() - start
            read_times.append(duration)

        # Run 10 concurrent reads
        tasks = [concurrent_read(i) for i in range(10)]
        start_time = asyncio.get_event_loop().time()
        await asyncio.gather(*tasks)
        total_time = asyncio.get_event_loop().time() - start_time

        # All reads should complete
        assert len(read_times) == 10

        # Total time should be similar to individual read time (not 10x)
        # This proves reads didn't block each other
        avg_read_time = sum(read_times) / len(read_times)
        # Total time should be less than 2x average (allowing for overhead)
        assert total_time < avg_read_time * 2, (
            f"Reads may have blocked: total={total_time:.3f}s, "
            f"avg={avg_read_time:.3f}s, expected<{avg_read_time * 2:.3f}s"
        )

        # Cleanup (safe: camera_id is generated by unique_id, not user input)
        async with get_session() as cleanup_session:
            await cleanup_session.execute(
                text(f"DELETE FROM cameras WHERE id = '{camera_id}'")  # noqa: S608  # nosemgrep
            )
            await cleanup_session.commit()

    @pytest.mark.asyncio
    async def test_reads_dont_block_writes_in_read_committed(self, integration_db: str) -> None:
        """Verify that reads don't prevent concurrent writes in READ COMMITTED."""
        camera_id = unique_id("read_write_cam")

        # Create test camera
        async with get_session() as setup_session:
            camera = Camera(
                id=camera_id,
                name="Read/Write Test Camera",
                folder_path=f"/export/foscam/{camera_id}",
            )
            setup_session.add(camera)
            await setup_session.commit()

        write_completed = asyncio.Event()
        read_started = asyncio.Event()

        async def long_read() -> None:
            """Hold a read transaction open."""
            async with get_session() as session:
                result = await session.execute(select(Camera).where(Camera.id == camera_id))
                camera = result.scalar_one()
                assert camera is not None
                read_started.set()
                # Wait for write to complete while holding read transaction
                await write_completed.wait()

        async def concurrent_write() -> None:
            """Perform a write while read is active."""
            # Wait for read to start
            await read_started.wait()
            # This write should not block on the read
            async with get_session() as session:
                result = await session.execute(select(Camera).where(Camera.id == camera_id))
                camera = result.scalar_one()
                camera.status = "offline"
                await session.commit()
                write_completed.set()

        # Run read and write concurrently
        await asyncio.gather(long_read(), concurrent_write())

        # Verify write succeeded
        async with get_session() as verify_session:
            result = await verify_session.execute(select(Camera).where(Camera.id == camera_id))
            camera = result.scalar_one()
            assert camera.status == "offline"

        # Cleanup (safe: camera_id is generated by unique_id, not user input)
        async with get_session() as cleanup_session:
            await cleanup_session.execute(
                text(f"DELETE FROM cameras WHERE id = '{camera_id}'")  # noqa: S608  # nosemgrep
            )
            await cleanup_session.commit()


class TestConcurrentWriteSerialization:
    """Tests for concurrent write operations.

    Verifies that:
    - Concurrent writes to same row are serialized
    - Updates don't get lost due to race conditions
    - Last writer wins in concurrent update scenarios
    """

    @pytest.mark.asyncio
    async def test_concurrent_writes_to_same_row_serialized(self, integration_db: str) -> None:
        """Verify concurrent writes to same row are properly serialized."""
        camera_id = unique_id("write_serialization_cam")

        # Create test camera
        async with get_session() as setup_session:
            camera = Camera(
                id=camera_id,
                name="Write Serialization Camera",
                folder_path=f"/export/foscam/{camera_id}",
                status="online",
            )
            setup_session.add(camera)
            await setup_session.commit()

        update_order: list[int] = []
        lock = asyncio.Lock()

        async def update_camera_status(worker_id: int, new_status: str) -> None:
            """Update camera status and record order."""
            async with get_session() as session:
                result = await session.execute(select(Camera).where(Camera.id == camera_id))
                camera = result.scalar_one()
                camera.status = new_status
                async with lock:
                    update_order.append(worker_id)
                await session.commit()

        # Run concurrent updates
        tasks = [
            update_camera_status(0, "offline"),
            update_camera_status(1, "error"),
            update_camera_status(2, "online"),
            update_camera_status(3, "unknown"),
            update_camera_status(4, "offline"),
        ]
        await asyncio.gather(*tasks)

        # All updates should complete without error
        assert len(update_order) == 5

        # Final status should be one of the values (last writer wins)
        async with get_session() as verify_session:
            result = await verify_session.execute(select(Camera).where(Camera.id == camera_id))
            camera = result.scalar_one()
            assert camera.status in ["offline", "error", "online", "unknown"]

        # Cleanup (safe: camera_id is generated by unique_id, not user input)
        async with get_session() as cleanup_session:
            await cleanup_session.execute(
                text(f"DELETE FROM cameras WHERE id = '{camera_id}'")  # noqa: S608  # nosemgrep
            )
            await cleanup_session.commit()

    @pytest.mark.asyncio
    async def test_lost_update_prevention(self, integration_db: str) -> None:
        """Verify that updates aren't lost in concurrent modification scenarios."""
        camera_id = unique_id("lost_update_cam")

        # Create camera with detections
        async with get_session() as setup_session:
            camera = Camera(
                id=camera_id,
                name="Lost Update Camera",
                folder_path=f"/export/foscam/{camera_id}",
            )
            setup_session.add(camera)
            await setup_session.flush()

            # Create initial detections
            for i in range(5):
                detection = Detection(
                    camera_id=camera_id,
                    file_path=f"/export/foscam/{camera_id}/initial_{i}.jpg",
                    file_type="jpg",
                    detected_at=datetime.now(UTC),
                    object_type="person",
                    confidence=0.9,
                )
                setup_session.add(detection)

            await setup_session.commit()

        # Concurrent workers adding detections
        async def add_detections(worker_id: int, count: int) -> int:
            """Add detections and return count."""
            added = 0
            async with get_session() as session:
                for i in range(count):
                    detection = Detection(
                        camera_id=camera_id,
                        file_path=f"/export/foscam/{camera_id}/worker{worker_id}_{i}.jpg",
                        file_type="jpg",
                        detected_at=datetime.now(UTC),
                        object_type="vehicle",
                        confidence=0.85,
                    )
                    session.add(detection)
                    added += 1
                await session.commit()
            return added

        # Run concurrent additions
        tasks = [add_detections(i, 3) for i in range(5)]
        results = await asyncio.gather(*tasks)

        # All additions should succeed
        assert sum(results) == 15  # 5 workers * 3 detections each

        # Verify total count
        async with get_session() as verify_session:
            result = await verify_session.execute(
                select(Detection).where(Detection.camera_id == camera_id)
            )
            all_detections = result.scalars().all()
            # 5 initial + 15 concurrent = 20 total
            assert len(all_detections) == 20

        # Cleanup (safe: camera_id is generated by unique_id, not user input)
        async with get_session() as cleanup_session:
            await cleanup_session.execute(
                text(f"DELETE FROM detections WHERE camera_id = '{camera_id}'")  # noqa: S608  # nosemgrep
            )
            await cleanup_session.execute(
                text(f"DELETE FROM cameras WHERE id = '{camera_id}'")  # noqa: S608  # nosemgrep
            )
            await cleanup_session.commit()


class TestDeadlockDetectionAndRecovery:
    """Tests for deadlock detection and recovery mechanisms.

    PostgreSQL automatically detects deadlocks and aborts one transaction.
    Tests verify proper error handling and retry logic.
    """

    @pytest.mark.asyncio
    @pytest.mark.xdist_group(name="deadlock_tests")  # Serialize deadlock tests
    async def test_deadlock_exception_handling(self, integration_db: str) -> None:
        """Verify that deadlock exceptions can be properly caught and handled.

        Note: This test verifies exception handling patterns rather than
        attempting to force a deadlock, which is inherently unreliable.
        """
        camera_id = unique_id("deadlock_cam")

        # Create test camera
        async with get_session() as setup_session:
            camera = Camera(
                id=camera_id,
                name="Deadlock Test Camera",
                folder_path=f"/export/foscam/{camera_id}",
            )
            setup_session.add(camera)
            await setup_session.commit()

        # Verify that deadlock exception handling pattern works
        deadlock_handled = False

        async def operation_with_deadlock_handling() -> bool:
            """Example of how to handle deadlocks in production code."""
            nonlocal deadlock_handled
            try:
                async with get_session() as session:
                    result = await session.execute(select(Camera).where(Camera.id == camera_id))
                    camera = result.scalar_one()
                    camera.status = "offline"
                    await session.commit()
                    return True
            except (OperationalError, DBAPIError) as e:
                if "deadlock detected" in str(e).lower():
                    deadlock_handled = True
                    return False
                raise

        # Execute operation - should succeed since no actual deadlock
        result = await operation_with_deadlock_handling()
        assert result is True, "Operation should succeed"

        # Verify exception handling pattern is correct
        # (deadlock_handled would be True if a deadlock occurred)
        assert deadlock_handled is False, "No deadlock should occur in simple update"

        # Cleanup (safe: camera_id is generated by unique_id, not user input)
        async with get_session() as cleanup_session:
            await cleanup_session.execute(
                text(f"DELETE FROM cameras WHERE id = '{camera_id}'")  # noqa: S608  # nosemgrep
            )
            await cleanup_session.commit()

    @pytest.mark.asyncio
    async def test_deadlock_retry_logic(self, integration_db: str) -> None:
        """Verify that operations can be retried after deadlock detection."""
        camera_id = unique_id("deadlock_retry_cam")

        # Create test camera
        async with get_session() as setup_session:
            camera = Camera(
                id=camera_id,
                name="Deadlock Retry Camera",
                folder_path=f"/export/foscam/{camera_id}",
                status="online",
            )
            setup_session.add(camera)
            await setup_session.commit()

        async def update_with_retry(new_status: str, max_retries: int = 3) -> bool:
            """Update camera with retry logic on deadlock."""
            for attempt in range(max_retries):
                try:
                    async with get_session() as session:
                        # Simple update without FOR UPDATE lock
                        result = await session.execute(select(Camera).where(Camera.id == camera_id))
                        camera = result.scalar_one()
                        camera.status = new_status
                        await session.commit()
                        return True
                except (OperationalError, DBAPIError) as e:
                    if "deadlock detected" in str(e).lower() and attempt < max_retries - 1:
                        # Retry after brief delay with exponential backoff
                        await asyncio.sleep(0.01 * (2**attempt))
                        continue
                    if "deadlock detected" in str(e).lower():
                        # Last retry failed, but this is acceptable
                        return False
                    raise
            return False

        # Test that retry logic works (even if deadlock doesn't occur)
        result = await update_with_retry("offline")
        assert result is True, "Update with retry should succeed"

        # Verify the update
        async with get_session() as verify_session:
            result = await verify_session.execute(select(Camera).where(Camera.id == camera_id))
            camera = result.scalar_one()
            assert camera.status == "offline"

        # Cleanup (safe: camera_id is generated by unique_id, not user input)
        async with get_session() as cleanup_session:
            await cleanup_session.execute(
                text(f"DELETE FROM cameras WHERE id = '{camera_id}'")  # noqa: S608  # nosemgrep
            )
            await cleanup_session.commit()


class TestIsolationLevelConfiguration:
    """Tests for explicit isolation level configuration.

    PostgreSQL supports explicit isolation level settings:
    - READ COMMITTED (default)
    - REPEATABLE READ
    - SERIALIZABLE
    """

    @pytest.mark.asyncio
    async def test_read_committed_is_default(self, integration_db: str) -> None:
        """Verify that READ COMMITTED is the default isolation level."""
        async with get_session() as session:
            result = await session.execute(text("SHOW transaction_isolation"))
            isolation_level = result.scalar()
            # PostgreSQL returns 'read committed'
            assert isolation_level.lower() == "read committed"

    @pytest.mark.asyncio
    async def test_can_set_repeatable_read(self, integration_db: str) -> None:
        """Verify that isolation level can be changed to REPEATABLE READ."""
        async with get_session() as session:
            # Set isolation level for this transaction
            await session.execute(text("SET TRANSACTION ISOLATION LEVEL REPEATABLE READ"))

            result = await session.execute(text("SHOW transaction_isolation"))
            isolation_level = result.scalar()
            assert isolation_level.lower() == "repeatable read"

    @pytest.mark.asyncio
    async def test_can_set_serializable(self, integration_db: str) -> None:
        """Verify that isolation level can be changed to SERIALIZABLE."""
        async with get_session() as session:
            # Set isolation level for this transaction
            await session.execute(text("SET TRANSACTION ISOLATION LEVEL SERIALIZABLE"))

            result = await session.execute(text("SHOW transaction_isolation"))
            isolation_level = result.scalar()
            assert isolation_level.lower() == "serializable"

    @pytest.mark.asyncio
    async def test_repeatable_read_prevents_non_repeatable_reads(self, integration_db: str) -> None:
        """Verify REPEATABLE READ prevents non-repeatable reads."""
        camera_id = unique_id("repeatable_read_cam")

        # Create test camera
        async with get_session() as setup_session:
            camera = Camera(
                id=camera_id,
                name="Repeatable Read Camera",
                folder_path=f"/export/foscam/{camera_id}",
                status="online",
            )
            setup_session.add(camera)
            await setup_session.commit()

        # Session 1: REPEATABLE READ transaction
        factory = get_session_factory()
        session1 = factory()

        try:
            # Set REPEATABLE READ isolation
            await session1.execute(text("SET TRANSACTION ISOLATION LEVEL REPEATABLE READ"))

            # First read
            result = await session1.execute(select(Camera).where(Camera.id == camera_id))
            first_read = result.scalar_one()
            assert first_read.status == "online"

            # Session 2: Update camera
            async with get_session() as session2:
                result = await session2.execute(select(Camera).where(Camera.id == camera_id))
                camera = result.scalar_one()
                camera.status = "offline"
                await session2.commit()

            # Session 1: Second read (should still see original value)
            result = await session1.execute(select(Camera).where(Camera.id == camera_id))
            second_read = result.scalar_one()

            # In REPEATABLE READ, we should see the same value
            assert second_read.status == "online"
            assert first_read.status == second_read.status

        finally:
            await session1.close()

        # Cleanup (safe: camera_id is generated by unique_id, not user input)
        async with get_session() as cleanup_session:
            await cleanup_session.execute(
                text(f"DELETE FROM cameras WHERE id = '{camera_id}'")  # noqa: S608  # nosemgrep
            )
            await cleanup_session.commit()
