"""Integration tests for Nemotron prompt template evaluation.

This module tests the 5 Nemotron prompt templates against synthetic scenarios
generated by NeMo Data Designer. It validates:

1. Risk scores fall within ground truth ranges
2. Reasoning quality and context utilization
3. Template consistency and reliability
4. Edge case handling

Tests are marked with @pytest.mark.prompt_evaluation and can be run separately
from regular integration tests.

Requirements:
    - Synthetic scenarios generated (via tools/nemo_data_designer/)
    - Nemotron service running (or mock mode enabled)
    - pandas installed (uv sync --extra nemo)

Usage:
    # Run with mock Nemotron (no service required)
    pytest backend/tests/integration/test_nemotron_prompts.py -m prompt_evaluation --mock

    # Run against real Nemotron service
    pytest backend/tests/integration/test_nemotron_prompts.py -m prompt_evaluation
"""

from __future__ import annotations

import pytest

# Prompt templates from the evaluation harness
PROMPT_TEMPLATES = ["basic", "enriched", "full_enriched", "vision_enhanced", "model_zoo_enhanced"]


# Skip all tests if pandas is not available
pytestmark = [
    pytest.mark.integration,
    pytest.mark.prompt_evaluation,
]


def skip_if_no_scenarios(synthetic_scenarios):
    """Helper to skip tests if synthetic scenarios are not available."""
    if synthetic_scenarios is None:
        pytest.skip("Synthetic scenarios not available")


def skip_if_no_nemotron(request):
    """Helper to skip tests if Nemotron service is not available.

    Uses --mock flag or MOCK_NEMOTRON env var to enable mock mode.
    """
    import os

    # Check for mock mode flag
    mock_mode = (
        request.config.getoption("--mock", False)
        or os.getenv("MOCK_NEMOTRON", "false").lower() == "true"
    )
    if not mock_mode:
        # Try to connect to Nemotron service
        try:
            import httpx

            # Intentionally checking if real service is available (not mocked)
            response = httpx.get("http://localhost:8091/health", timeout=2.0)  # intentional
            if response.status_code != 200:
                pytest.skip("Nemotron service not available (use --mock for mock mode)")
        except Exception:
            pytest.skip("Nemotron service not available (use --mock for mock mode)")


@pytest.mark.parametrize("template", PROMPT_TEMPLATES)
def test_risk_score_within_ground_truth_range(
    synthetic_scenarios, scenario_by_type, template, request
):
    """Each template should produce scores within expected ranges for threat scenarios.

    Validates that threat scenarios (ground truth 70-100) produce high risk scores
    and normal scenarios (ground truth 0-25) produce low risk scores.
    """
    skip_if_no_scenarios(synthetic_scenarios)
    skip_if_no_nemotron(request)

    # Get mock mode from request
    import os

    from backend.evaluation.harness import PromptEvaluator

    mock_mode = (
        request.config.getoption("--mock", False)
        or os.getenv("MOCK_NEMOTRON", "false").lower() == "true"
    )

    evaluator = PromptEvaluator(mock_mode=mock_mode)

    # Test threat scenarios - should score high (70-100)
    threat_scenarios = scenario_by_type["threat"]
    if len(threat_scenarios) == 0:
        pytest.skip("No threat scenarios available")

    # Sample up to 5 threat scenarios
    sample_count = min(5, len(threat_scenarios))
    sampled_threats = threat_scenarios.head(sample_count)

    results_df = evaluator.evaluate_all_sync(
        sampled_threats,
        templates=[t for t in evaluator._templates if t.name == template],
    )

    # Check that most results are within ground truth range
    within_range = results_df[results_df["risk_deviation"] <= 10]  # Within 10 points
    within_range_pct = len(within_range) / len(results_df) * 100

    assert within_range_pct >= 60, (
        f"Template '{template}' only scored {within_range_pct:.1f}% of threat scenarios "
        f"within ground truth range (expected >= 60%)"
    )


def test_enrichment_context_reflected_in_reasoning(synthetic_scenarios, request):
    """Full enrichment scenarios should reference context in reasoning.

    Validates that when enrichment context is provided, the Nemotron reasoning
    mentions key contextual points like zone information and baseline deviations.
    """
    skip_if_no_scenarios(synthetic_scenarios)
    skip_if_no_nemotron(request)

    # Get mock mode from request
    import os

    from backend.evaluation.harness import PromptEvaluator

    mock_mode = (
        request.config.getoption("--mock", False)
        or os.getenv("MOCK_NEMOTRON", "false").lower() == "true"
    )

    evaluator = PromptEvaluator(mock_mode=mock_mode)

    # Filter for full enrichment level
    full_enrichment = synthetic_scenarios[synthetic_scenarios["enrichment_level"] == "full"]
    if len(full_enrichment) == 0:
        pytest.skip("No full enrichment scenarios available")

    # Sample up to 3 scenarios
    sample_count = min(3, len(full_enrichment))
    sampled = full_enrichment.head(sample_count)

    results_df = evaluator.evaluate_all_sync(sampled)

    # Check key point coverage (should be > 0.3 on average)
    mean_coverage = results_df["key_point_coverage"].mean()
    assert mean_coverage > 0.3, (
        f"Mean key point coverage {mean_coverage:.2f} is too low (expected > 0.3). "
        "Enrichment context may not be reflected in reasoning."
    )


def test_threat_scenarios_score_high(scenario_by_type, request):
    """Threat scenarios should consistently score >= 70.

    Validates that clear security threats (forced entry, weapons, etc.)
    receive appropriately high risk scores from all templates.
    """
    if scenario_by_type is None:
        pytest.skip("Scenarios not available")

    skip_if_no_nemotron(request)

    # Get mock mode from request
    import os

    from backend.evaluation.harness import PromptEvaluator

    mock_mode = (
        request.config.getoption("--mock", False)
        or os.getenv("MOCK_NEMOTRON", "false").lower() == "true"
    )

    evaluator = PromptEvaluator(mock_mode=mock_mode)

    threat_scenarios = scenario_by_type["threat"]
    if len(threat_scenarios) == 0:
        pytest.skip("No threat scenarios available")

    # Sample up to 3 scenarios
    sample_count = min(3, len(threat_scenarios))
    sampled = threat_scenarios.head(sample_count)

    results_df = evaluator.evaluate_all_sync(sampled)

    # Check that most results score >= 70
    high_scores = results_df[results_df["risk_score"] >= 70]
    high_score_pct = len(high_scores) / len(results_df) * 100

    assert high_score_pct >= 60, (
        f"Only {high_score_pct:.1f}% of threat scenarios scored >= 70 (expected >= 60%). "
        "Model may be under-estimating threat severity."
    )


def test_normal_scenarios_score_low(scenario_by_type, request):
    """Normal scenarios should consistently score <= 30.

    Validates that routine household activity (family, pets, deliveries)
    receives appropriately low risk scores from all templates.
    """
    if scenario_by_type is None:
        pytest.skip("Scenarios not available")

    skip_if_no_nemotron(request)

    # Get mock mode from request
    import os

    from backend.evaluation.harness import PromptEvaluator

    mock_mode = (
        request.config.getoption("--mock", False)
        or os.getenv("MOCK_NEMOTRON", "false").lower() == "true"
    )

    evaluator = PromptEvaluator(mock_mode=mock_mode)

    normal_scenarios = scenario_by_type["normal"]
    if len(normal_scenarios) == 0:
        pytest.skip("No normal scenarios available")

    # Sample up to 5 scenarios
    sample_count = min(5, len(normal_scenarios))
    sampled = normal_scenarios.head(sample_count)

    results_df = evaluator.evaluate_all_sync(sampled)

    # Check that most results score <= 30
    low_scores = results_df[results_df["risk_score"] <= 30]
    low_score_pct = len(low_scores) / len(results_df) * 100

    assert low_score_pct >= 60, (
        f"Only {low_score_pct:.1f}% of normal scenarios scored <= 30 (expected >= 60%). "
        "Model may be over-estimating risk for routine activity."
    )


def test_template_consistency(synthetic_scenarios, request):
    """Same scenario should produce similar scores across runs.

    Tests that the evaluation is deterministic enough for comparison purposes.
    With temperature=0.1, we expect < 15 point deviation between runs.
    """
    skip_if_no_scenarios(synthetic_scenarios)
    skip_if_no_nemotron(request)

    # Get mock mode from request
    import os

    from backend.evaluation.harness import PromptEvaluator

    mock_mode = (
        request.config.getoption("--mock", False)
        or os.getenv("MOCK_NEMOTRON", "false").lower() == "true"
    )

    evaluator = PromptEvaluator(mock_mode=mock_mode)

    # Sample a single scenario
    if len(synthetic_scenarios) == 0:
        pytest.skip("No scenarios available")

    single_scenario = synthetic_scenarios.head(1)

    # Run twice
    results_1 = evaluator.evaluate_all_sync(single_scenario)
    results_2 = evaluator.evaluate_all_sync(single_scenario)

    # Check consistency for each template
    for template in PROMPT_TEMPLATES:
        template_results_1 = results_1[results_1["template_name"] == template]
        template_results_2 = results_2[results_2["template_name"] == template]

        if len(template_results_1) == 0 or len(template_results_2) == 0:
            continue

        score_1 = template_results_1.iloc[0]["risk_score"]
        score_2 = template_results_2.iloc[0]["risk_score"]

        deviation = abs(score_1 - score_2)
        assert deviation <= 15, (
            f"Template '{template}' produced inconsistent scores: {score_1} vs {score_2} "
            f"(deviation: {deviation}, expected <= 15)"
        )


# Pytest command line option for mock mode
def pytest_addoption(parser):
    """Add --mock option to pytest."""
    parser.addoption(
        "--mock",
        action="store_true",
        default=False,
        help="Use mock Nemotron responses instead of real service",
    )
