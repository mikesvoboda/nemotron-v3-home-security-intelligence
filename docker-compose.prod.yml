# Production Docker Compose
#
# This uses optimized builds with multi-stage Dockerfiles.
# Compatible with Docker and Podman on Linux and macOS.
#
# Usage:
#   Docker:  docker compose -f docker-compose.prod.yml up -d
#   Podman:  podman-compose -f docker-compose.prod.yml up -d
#
# GPU Requirements:
#   - NVIDIA GPU with CUDA support required for AI services
#   - nvidia-container-toolkit must be installed
#
# Credential Management (Choose One Approach):
#
#   OPTION 1: Environment Variables (Default)
#   - POSTGRES_PASSWORD: Database password (REQUIRED - no default for security)
#     Generate with: openssl rand -base64 32
#
#   OPTION 2: Docker Secrets (Recommended for Production)
#   - Enhanced security: secrets stored separately from environment variables
#   - Credentials not visible in 'docker inspect' output
#   - Easier credential rotation without image rebuild
#   - See DOCKER_SECRETS_SETUP section below for setup instructions
#
# Environment Variables (Optional):
#   - AI_MODELS_PATH: Base path for AI models (default: /export/ai_models)
#   - FRONTEND_PORT: Host port for frontend HTTP (default: 5173)
#   - FRONTEND_HTTPS_PORT: Host port for frontend HTTPS (default: 8443)
#   - FRONTEND_INTERNAL_PORT: Container port for nginx (default: 8080, used for health checks)
#   - YOLO26_CONFIDENCE: YOLO26 detection confidence threshold (default: 0.5)
#   - GPU_LAYERS: Number of GPU layers for LLM (default: 30)
#   - CTX_SIZE: LLM context window size (default: 131072)
#   - HF_CACHE: HuggingFace cache directory (default: uses hf_cache volume)
#   - HF_HUB_OFFLINE: Enable offline mode for HuggingFace Hub (default: 0, set to 1 for production)
#   - REDIS_PASSWORD: Redis authentication password (optional, for production use)
#   - GF_ADMIN_PASSWORD: Grafana admin password (default: admin, anonymous access enabled)

version: '3.8'

services:
  postgres:
    # Security hardening: Prevent privilege escalation
    security_opt:
      - no-new-privileges:true
    # NOTE: Postgres needs CHOWN/DAC_OVERRIDE for data directory initialization
    # Other capabilities are dropped for security
    cap_drop:
      - NET_RAW
      - SYS_ADMIN
      - SYS_PTRACE
    image: docker.io/library/postgres:16-alpine
    tmpfs:
      - /tmp
      - /var/run/postgresql
    ports:
      - '5432:5432'
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-security}
      # SECURITY: No default password - must be explicitly set via environment or secrets
      # Option 1: Set POSTGRES_PASSWORD environment variable (e.g., in .env file)
      # Option 2: Use Docker secrets (uncomment secrets section below)
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:?POSTGRES_PASSWORD must be set - run ./setup.sh or set manually}
      - POSTGRES_DB=${POSTGRES_DB:-security}
    # Uncomment to use Docker secrets instead of environment variables:
    # secrets:
    #   - postgres_password
    # environment:
    #   - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_password
    # PostgreSQL server-side query logging (NEM-3054)
    # Logs slow queries (>1s) to container stdout, collected by Alloy -> Loki
    # - log_min_duration_statement: Log queries exceeding 1000ms (1 second)
    # - log_duration: Include query duration in all logged statements
    # - log_line_prefix: Timestamp, PID, user@database for correlation
    # - log_statement: 'ddl' logs only schema changes (not all queries)
    # - log_checkpoints: Log checkpoint activity for performance analysis
    # - log_connections/disconnections: Track connection lifecycle
    # - log_lock_waits: Log queries waiting >1s for locks (deadlock detection)
    command:
      - postgres
      - -c
      - log_min_duration_statement=1000
      - -c
      - log_duration=on
      - -c
      - log_line_prefix=%t [%p] %u@%d
      - -c
      - log_statement=ddl
      - -c
      - log_checkpoints=on
      - -c
      - log_connections=on
      - -c
      - log_disconnections=on
      - -c
      - log_lock_waits=on
      - -c
      - deadlock_timeout=1s
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U ${POSTGRES_USER:-security} -d ${POSTGRES_DB:-security}']
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1G

  ai-detector:
    tmpfs:
      - /tmp
      - /home/rtdetr/.cache # PyTorch compile cache
    labels:
      pyroscope.profile: 'true'
      pyroscope.service: 'ai-detector'
    build:
      context: .
      dockerfile: ai/rtdetr/Dockerfile
    ports:
      - '8090:8090'
    volumes:
      # Persistent HuggingFace cache volume (NEM-3812)
      # :U tells Podman to recursively chown the volume to match container user (rtdetr)
      # Docker ignores the :U flag, making this backward compatible
      - hf_cache:/cache/huggingface:U
    environment:
      - SERVICE_NAME=ai-detector
      - PYROSCOPE_ENABLED=${PYROSCOPE_ENABLED:-true}
      - PYROSCOPE_URL=http://pyroscope:4040
      - RTDETR_CONFIDENCE=${RTDETR_CONFIDENCE:-0.5}
      - RTDETR_MODEL_PATH=${RTDETR_MODEL_PATH:-PekingU/rtdetr_r50vd_coco_o365}
      # HuggingFace cache configuration (NEM-3812)
      - HF_HOME=/cache/huggingface
      - HF_HUB_OFFLINE=${HF_HUB_OFFLINE:-0}
      - HF_HUB_DISABLE_TELEMETRY=1
    healthcheck:
      test:
        [
          'CMD',
          'python',
          '-c',
          "import httpx; r = httpx.get('http://localhost:8090/health'); exit(0 if r.status_code == 200 else 1)",
        ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # YOLO26 TensorRT-optimized object detection (alternative to RT-DETRv2)
  # Enable by setting DETECTOR_TYPE=yolo26 in backend environment
  ai-yolo26:
    labels:
      pyroscope.profile: 'true'
      pyroscope.service: 'ai-yolo26'
    build:
      context: .
      dockerfile: ai/yolo26/Dockerfile
    ports:
      - '8095:8095'
    volumes:
      - ${AI_MODELS_PATH:-/export/ai_models}/model-zoo/yolo26:/models/yolo26:ro
    environment:
      - SERVICE_NAME=ai-yolo26
      - PYROSCOPE_ENABLED=${PYROSCOPE_ENABLED:-true}
      - PYROSCOPE_URL=http://pyroscope:4040
      - YOLO26_CONFIDENCE=${YOLO26_CONFIDENCE:-0.5}
      - YOLO26_MODEL_PATH=/models/yolo26/exports/yolo26m_fp16.engine
    healthcheck:
      test:
        [
          'CMD',
          'python',
          '-c',
          "import httpx; r = httpx.get('http://localhost:8095/health'); exit(0 if r.status_code == 200 else 1)",
        ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        reservations:
          devices:
            # GPU 1 (A400 4GB) - YOLO26 TensorRT uses ~1GB, shares with CLIP and Florence
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]

  ai-llm:
    tmpfs:
      - /tmp
      - /home/llama/.cache # llama.cpp temp cache
    labels:
      pyroscope.profile: 'true'
      pyroscope.service: 'ai-llm'
    build:
      context: ./ai/nemotron
      dockerfile: Dockerfile
    ports:
      - '8091:8091'
    volumes:
      - ${AI_MODELS_PATH:-/export/ai_models}/nemotron/nemotron-3-nano-30b-a3b-q4km:/models:ro
    environment:
      - GPU_LAYERS=${GPU_LAYERS:-40}
      # Reduced context for home security analysis (was 131072, saves ~8GB VRAM)
      - CTX_SIZE=${CTX_SIZE:-65536}
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8091/health']
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 120s
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Florence-2 vision-language model for dense captioning and visual understanding
  # Assigned to GPU 1 (A400 4GB) - shares with CLIP (~722MB) and YOLO26 (~1GB)
  ai-florence:
    tmpfs:
      - /tmp
      - /home/florence/.cache # PyTorch compile cache
    labels:
      pyroscope.profile: 'true'
      pyroscope.service: 'ai-florence'
    build:
      context: .
      dockerfile: ai/florence/Dockerfile
    ports:
      - '8092:8092'
    volumes:
      - ${AI_MODELS_PATH:-/export/ai_models}/model-zoo/florence-2-large:/models/florence-2-large:ro
      # Persistent HuggingFace cache volume (NEM-3812)
      - hf_cache:/cache/huggingface:U
    environment:
      - SERVICE_NAME=ai-florence
      - PYROSCOPE_ENABLED=${PYROSCOPE_ENABLED:-true}
      - PYROSCOPE_URL=http://pyroscope:4040
      - MODEL_PATH=/models/florence-2-large
      # HuggingFace cache configuration (NEM-3812)
      - HF_HOME=/cache/huggingface
      - HF_HUB_OFFLINE=${HF_HUB_OFFLINE:-0}
      - HF_HUB_DISABLE_TELEMETRY=1
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8092/health']
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        reservations:
          devices:
            # GPU 1 (A400 4GB) - Florence uses ~530MB, shares with CLIP (~722MB) and YOLO26 (~1GB)
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]

  # CLIP ViT-L model for entity re-identification embeddings
  # Assigned to GPU 1 (A400 4GB) - shares with Florence (~530MB) and YOLO26 (~1GB)
  ai-clip:
    tmpfs:
      - /tmp
      - /home/clip/.cache # PyTorch compile cache
    labels:
      pyroscope.profile: 'true'
      pyroscope.service: 'ai-clip'
    build:
      context: .
      dockerfile: ai/clip/Dockerfile
    ports:
      - '8093:8093'
    volumes:
      # Read-only model mount (HuggingFace CLIP weights)
      - ${AI_MODELS_PATH:-/export/ai_models}/model-zoo/clip-vit-l:/models/clip-vit-l:ro
      # Writable mount for TensorRT engine cache (persists between restarts)
      - clip-tensorrt-cache:/cache/tensorrt
    environment:
      - SERVICE_NAME=ai-clip
      - PYROSCOPE_ENABLED=${PYROSCOPE_ENABLED:-true}
      - PYROSCOPE_URL=http://pyroscope:4040
      - CLIP_MODEL_PATH=/models/clip-vit-l
      # TensorRT acceleration (1.5-2x faster inference, auto-exports engine on first run)
      - CLIP_USE_TENSORRT=${CLIP_USE_TENSORRT:-true}
      - CLIP_ENGINE_PATH=/cache/tensorrt/vision_encoder_fp16.engine
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8093/health']
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        reservations:
          devices:
            # GPU 1 (A400 4GB) - CLIP TensorRT uses ~722MB, shares with Florence (~530MB) and YOLO26 (~1GB)
            # Total GPU 1 VRAM: ~2.3GB / 4GB (~57% utilization)
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Lightweight enrichment service for small, efficient models (GPU 1)
  # Hosts: pose, threat, reid, pet, depth models (~1.2GB total)
  ai-enrichment-light:
    tmpfs:
      - /tmp
      - /home/enrichment/.cache
    labels:
      pyroscope.profile: 'true'
      pyroscope.service: 'ai-enrichment-light'
    build:
      context: .
      dockerfile: ai/enrichment-light/Dockerfile
    ports:
      - '8096:8096'
    volumes:
      # Light models for GPU 1
      - ${AI_MODELS_PATH:-/export/ai_models}/model-zoo/yolov8n-pose:/models/yolov8n-pose:ro
      - ${AI_MODELS_PATH:-/export/ai_models}/model-zoo/threat-detection-yolov8n:/models/threat-detection-yolov8n:ro
      - ${AI_MODELS_PATH:-/export/ai_models}/model-zoo/osnet-x0-25:/models/osnet-x0-25:ro
      - ${AI_MODELS_PATH:-/export/ai_models}/model-zoo/pet-classifier:/models/pet-classifier:ro
      - ${AI_MODELS_PATH:-/export/ai_models}/model-zoo/depth-anything-v2-small:/models/depth-anything-v2-small:ro
    environment:
      - SERVICE_NAME=ai-enrichment-light
      - PYROSCOPE_ENABLED=${PYROSCOPE_ENABLED:-true}
      - PYROSCOPE_URL=http://pyroscope:4040
      # Model assignment configuration (which models this service loads)
      # Set to 'light' to load on this service, 'heavy' to skip
      - ENRICHMENT_POSE_SERVICE=${ENRICHMENT_POSE_SERVICE:-light}
      - ENRICHMENT_THREAT_SERVICE=${ENRICHMENT_THREAT_SERVICE:-light}
      - ENRICHMENT_REID_SERVICE=${ENRICHMENT_REID_SERVICE:-light}
      - ENRICHMENT_PET_SERVICE=${ENRICHMENT_PET_SERVICE:-light}
      - ENRICHMENT_DEPTH_SERVICE=${ENRICHMENT_DEPTH_SERVICE:-light}
      # Light model paths
      - POSE_MODEL_PATH=/models/yolov8n-pose/yolov8n-pose.pt
      - THREAT_MODEL_PATH=/models/threat-detection-yolov8n/weights/best.pt
      - REID_MODEL_PATH=/models/osnet-x0-25/osnet_x0_25.pth
      - PET_MODEL_PATH=/models/pet-classifier
      - DEPTH_MODEL_PATH=/models/depth-anything-v2-small
      # TensorRT acceleration for YOLO models
      - POSE_USE_TENSORRT=${POSE_USE_TENSORRT:-true}
      - THREAT_USE_TENSORRT=${THREAT_USE_TENSORRT:-true}
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8096/health']
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 90s
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        reservations:
          devices:
            # GPU 1 (A400 4GB) - Light models use ~1.2GB
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]

  # Heavy enrichment service for large transformer models (GPU 0)
  # Hosts: vehicle, fashion, demographics, action models (~4.3GB total)
  ai-enrichment:
    tmpfs:
      - /tmp
      - /home/enrichment/.cache # PyTorch compile cache
    labels:
      pyroscope.profile: 'true'
      pyroscope.service: 'ai-enrichment'
    build:
      context: .
      dockerfile: ai/enrichment/Dockerfile
    ports:
      - '8094:8094'
    volumes:
      # Heavy models only (light models moved to ai-enrichment-light)
      - ${AI_MODELS_PATH:-/export/ai_models}/model-zoo/vehicle-segment-classification:/models/vehicle-segment-classification:ro
      - ${AI_MODELS_PATH:-/export/ai_models}/model-zoo/fashion-clip:/models/fashion-clip:ro
      - ${AI_MODELS_PATH:-/export/ai_models}/model-zoo/vit-age-classifier:/models/vit-age-classifier:ro
      - ${AI_MODELS_PATH:-/export/ai_models}/model-zoo/vit-gender-classifier:/models/vit-gender-classifier:ro
      - ${AI_MODELS_PATH:-/export/ai_models}/model-zoo/xclip-base-patch32:/models/xclip-base-patch32:ro
      # Persistent HuggingFace cache volume (NEM-3812)
      - hf_cache:/cache/huggingface:U
    environment:
      - SERVICE_NAME=ai-enrichment
      - PYROSCOPE_ENABLED=${PYROSCOPE_ENABLED:-true}
      - PYROSCOPE_URL=http://pyroscope:4040
      # Heavy models only (light models on ai-enrichment-light)
      - VEHICLE_MODEL_PATH=/models/vehicle-segment-classification
      - CLOTHING_MODEL_PATH=/models/fashion-clip
      - AGE_MODEL_PATH=/models/vit-age-classifier
      - GENDER_MODEL_PATH=/models/vit-gender-classifier
      - ACTION_MODEL_PATH=/models/xclip-base-patch32
      # VRAM management for heavy models
      - VRAM_BUDGET_GB=6.0
      - MODEL_LOAD_TIMEOUT=30
      # HuggingFace cache configuration (NEM-3812)
      - HF_HOME=/cache/huggingface
      - HF_HUB_OFFLINE=${HF_HUB_OFFLINE:-0}
      - HF_HUB_DISABLE_TELEMETRY=1
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8094/health']
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 120s
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        reservations:
          devices:
            # GPU 0 (A5500 24GB) - Heavy transformer models
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  backend:
    # NOTE: Security hardening temporarily disabled for nvidia-cdi-hook compatibility
    tmpfs:
      - /tmp
      - /home/appuser/.cache
    build:
      context: .
      dockerfile: backend/Dockerfile
      target: prod
    ports:
      - '8000:8000'
    volumes:
      # :U tells Podman to recursively chown the volume to match container user (appuser)
      # Docker ignores the :U flag, making this backward compatible
      - ./backend/data:/app/data:z,U
      - ${FOSCAM_BASE_PATH:-/export/foscam}:/cameras:ro
      - ${AI_MODELS_PATH:-/export/ai_models}/model-zoo:/models/model-zoo:ro,z
    environment:
      # Free-threaded Python: Disabled - standard Python 3.14 image doesn't support no-GIL
      # To enable, use python:3.14t-slim-bookworm image and uncomment:
      # - PYTHON_GIL=0
      # Environment setting: defaults to 'development' for local deploys
      # Override with ENVIRONMENT=production for actual production deployments
      - ENVIRONMENT=${ENVIRONMENT:-development}
      # SECURITY: Uses same POSTGRES_PASSWORD as postgres service (must be set)
      - DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER:-security}:${POSTGRES_PASSWORD:?POSTGRES_PASSWORD must be set}@postgres:5432/${POSTGRES_DB:-security}
      - REDIS_URL=redis://redis:6379
      # SECURITY: Redis password authentication (optional, for production use)
      # Must match REDIS_PASSWORD set in the redis service
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
      # Pyroscope continuous profiling (enabled by default, set PYROSCOPE_ENABLED=false to disable)
      - PYROSCOPE_ENABLED=${PYROSCOPE_ENABLED:-true}
      - PYROSCOPE_URL=http://pyroscope:4040
      - NEMOTRON_URL=http://ai-llm:8091
      - FLORENCE_URL=http://ai-florence:8092
      - CLIP_URL=http://ai-clip:8093
      - ENRICHMENT_URL=http://ai-enrichment:8094
      - ENRICHMENT_LIGHT_URL=http://ai-enrichment-light:8096
      # Enrichment model assignment (determines request routing)
      # Set to 'light' for GPU 1 (A400) or 'heavy' for GPU 0 (A5500)
      - ENRICHMENT_POSE_SERVICE=${ENRICHMENT_POSE_SERVICE:-light}
      - ENRICHMENT_THREAT_SERVICE=${ENRICHMENT_THREAT_SERVICE:-light}
      - ENRICHMENT_REID_SERVICE=${ENRICHMENT_REID_SERVICE:-light}
      - ENRICHMENT_PET_SERVICE=${ENRICHMENT_PET_SERVICE:-light}
      - ENRICHMENT_DEPTH_SERVICE=${ENRICHMENT_DEPTH_SERVICE:-light}
      - ENRICHMENT_VEHICLE_SERVICE=${ENRICHMENT_VEHICLE_SERVICE:-heavy}
      - ENRICHMENT_CLOTHING_SERVICE=${ENRICHMENT_CLOTHING_SERVICE:-heavy}
      - ENRICHMENT_ACTION_SERVICE=${ENRICHMENT_ACTION_SERVICE:-heavy}
      - ENRICHMENT_DEMOGRAPHICS_SERVICE=${ENRICHMENT_DEMOGRAPHICS_SERVICE:-heavy}
      - YOLO26_URL=http://ai-yolo26:8095
      - DETECTOR_TYPE=${DETECTOR_TYPE:-yolo26}
      - FRONTEND_URL=http://frontend:${FRONTEND_INTERNAL_PORT:-8080}
      # Always use /cameras (container mount point) regardless of host env var
      - FOSCAM_BASE_PATH=/cameras
      - DEBUG=${DEBUG:-false}
      - ADMIN_ENABLED=${ADMIN_ENABLED:-false}
      - FAST_PATH_ENABLED=${FAST_PATH_ENABLED:-false}
      - FAST_PATH_CONFIDENCE_THRESHOLD=${FAST_PATH_CONFIDENCE_THRESHOLD:-0.90}
      # OpenTelemetry distributed tracing (NEM-1629)
      # Enabled by default since monitoring stack is included
      - OTEL_ENABLED=${OTEL_ENABLED:-true}
      - OTEL_SERVICE_NAME=${OTEL_SERVICE_NAME:-nemotron-backend}
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT:-http://alloy:4317}
      - OTEL_TRACE_SAMPLE_RATE=${OTEL_TRACE_SAMPLE_RATE:-1.0}
      # Container orchestrator: connect to Podman API via TCP for self-healing
      # Requires: podman system service --time=0 tcp:0.0.0.0:2375 &
      - ORCHESTRATOR_DOCKER_HOST=${ORCHESTRATOR_DOCKER_HOST:-tcp://host.containers.internal:2375}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      # AI services have health-based startup ordering (NEM-1582)
      # Backend will start after critical AI services are healthy
      # Expected startup times:
      #   - ai-yolo26 (YOLO26): ~60s (TensorRT model loading)
      #   - ai-llm (Nemotron): ~120s (LLM loading, largest model)
      #   - ai-florence: ~60s (Florence-2 model loading)
      #   - ai-clip: ~60s (CLIP model loading)
      #   - ai-enrichment: ~180s (multiple models: vehicle, pet, clothing, depth)
      ai-yolo26:
        condition: service_healthy
      ai-llm:
        condition: service_healthy
      # Non-critical AI services - backend can start without these (graceful degradation)
      # ai-florence:
      #   condition: service_healthy
      # ai-clip:
      #   condition: service_healthy
      # ai-enrichment:
      #   condition: service_healthy
    healthcheck:
      test:
        [
          'CMD',
          'python',
          '-c',
          "import httpx; r = httpx.get('http://localhost:8000/api/system/health/ready'); exit(0 if r.status_code == 200 else 1)",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  redis:
    tmpfs:
      - /tmp
    # Security hardening: Prevent privilege escalation
    # NOTE: Redis needs CHOWN/DAC_OVERRIDE for data persistence
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - NET_RAW
      - SYS_ADMIN
      - SYS_PTRACE
    image: docker.io/library/redis:7.4-alpine3.21
    ports:
      - '6379:6379'
    volumes:
      - redis_data:/data
    # SECURITY: Password authentication via REDIS_PASSWORD environment variable
    # If REDIS_PASSWORD is set, Redis requires authentication for all connections.
    # If REDIS_PASSWORD is empty or not set, Redis runs without authentication (local dev only).
    command: >-
      sh -c '
      if [ -n "$$REDIS_PASSWORD" ]; then
        echo "Starting Redis with password authentication"
        redis-server --appendonly yes --appendfsync everysec --maxmemory 450mb --maxmemory-policy allkeys-lru --slowlog-log-slower-than 10000 --slowlog-max-len 128 --requirepass "$$REDIS_PASSWORD"
      else
        echo "Starting Redis without authentication (development mode)"
        redis-server --appendonly yes --appendfsync everysec --maxmemory 450mb --maxmemory-policy allkeys-lru --slowlog-log-slower-than 10000 --slowlog-max-len 128
      fi
      '
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
    healthcheck:
      # Use AUTH if password is set, otherwise just ping
      test: >-
        sh -c '
        if [ -n "$$REDIS_PASSWORD" ]; then
          redis-cli -a "$$REDIS_PASSWORD" ping 2>/dev/null | grep -q PONG
        else
          redis-cli ping | grep -q PONG
        fi
        '
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M

  # =============================================================================
  # Redis Sentinel for High Availability (NEM-3413) - Optional
  # =============================================================================
  # Redis Sentinel provides automatic failover and master discovery.
  # Enable by setting REDIS_USE_SENTINEL=true in .env and uncommenting below.
  #
  # Requires at least 3 Sentinel instances for proper quorum (2 must agree for failover).
  # Configure backend with:
  #   - REDIS_USE_SENTINEL=true
  #   - REDIS_SENTINEL_MASTER_NAME=mymaster
  #   - REDIS_SENTINEL_HOSTS=sentinel1:26379,sentinel2:26379,sentinel3:26379
  #
  # To enable, uncomment the sentinel services below and add redis replicas.

  # sentinel1:
  #   image: docker.io/library/redis:7.4-alpine3.21
  #   ports:
  #     - '26379:26379'
  #   volumes:
  #     - ./config/sentinel.conf:/etc/redis/sentinel.conf:ro,z
  #     - sentinel1_data:/data
  #   command: >-
  #     sh -c '
  #     cp /etc/redis/sentinel.conf /data/sentinel.conf &&
  #     if [ -n "$$REDIS_PASSWORD" ]; then
  #       echo "sentinel auth-pass mymaster $$REDIS_PASSWORD" >> /data/sentinel.conf
  #     fi &&
  #     redis-sentinel /data/sentinel.conf
  #     '
  #   environment:
  #     - REDIS_PASSWORD=${REDIS_PASSWORD:-}
  #   depends_on:
  #     redis:
  #       condition: service_healthy
  #   healthcheck:
  #     test: ['CMD', 'redis-cli', '-p', '26379', 'ping']
  #     interval: 10s
  #     timeout: 5s
  #     retries: 3
  #   restart: unless-stopped
  #   networks:
  #     - security-net
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '0.25'
  #         memory: 64M

  # sentinel2:
  #   image: docker.io/library/redis:7.4-alpine3.21
  #   ports:
  #     - '26380:26379'
  #   volumes:
  #     - ./config/sentinel.conf:/etc/redis/sentinel.conf:ro,z
  #     - sentinel2_data:/data
  #   command: >-
  #     sh -c '
  #     cp /etc/redis/sentinel.conf /data/sentinel.conf &&
  #     if [ -n "$$REDIS_PASSWORD" ]; then
  #       echo "sentinel auth-pass mymaster $$REDIS_PASSWORD" >> /data/sentinel.conf
  #     fi &&
  #     redis-sentinel /data/sentinel.conf
  #     '
  #   environment:
  #     - REDIS_PASSWORD=${REDIS_PASSWORD:-}
  #   depends_on:
  #     redis:
  #       condition: service_healthy
  #   healthcheck:
  #     test: ['CMD', 'redis-cli', '-p', '26379', 'ping']
  #     interval: 10s
  #     timeout: 5s
  #     retries: 3
  #   restart: unless-stopped
  #   networks:
  #     - security-net
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '0.25'
  #         memory: 64M

  # sentinel3:
  #   image: docker.io/library/redis:7.4-alpine3.21
  #   ports:
  #     - '26381:26379'
  #   volumes:
  #     - ./config/sentinel.conf:/etc/redis/sentinel.conf:ro,z
  #     - sentinel3_data:/data
  #   command: >-
  #     sh -c '
  #     cp /etc/redis/sentinel.conf /data/sentinel.conf &&
  #     if [ -n "$$REDIS_PASSWORD" ]; then
  #       echo "sentinel auth-pass mymaster $$REDIS_PASSWORD" >> /data/sentinel.conf
  #     fi &&
  #     redis-sentinel /data/sentinel.conf
  #     '
  #   environment:
  #     - REDIS_PASSWORD=${REDIS_PASSWORD:-}
  #   depends_on:
  #     redis:
  #       condition: service_healthy
  #   healthcheck:
  #     test: ['CMD', 'redis-cli', '-p', '26379', 'ping']
  #     interval: 10s
  #     timeout: 5s
  #     retries: 3
  #   restart: unless-stopped
  #   networks:
  #     - security-net
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '0.25'
  #         memory: 64M

  frontend:
    # NOTE: Security hardening temporarily disabled for nvidia-cdi-hook compatibility
    tmpfs:
      - /tmp
      - /var/run # nginx pid file
      - /var/cache/nginx # nginx cache
    build:
      context: ./frontend
      dockerfile: Dockerfile
      target: prod
    ports:
      # Map external port to internal port (nginx-unprivileged runs on 8080)
      # FRONTEND_INTERNAL_PORT defines the container-side port for health checks
      - '${FRONTEND_PORT:-5173}:${FRONTEND_INTERNAL_PORT:-8080}'
      # HTTPS port mapping (only used when SSL_ENABLED=true)
      # Default 8443 to avoid privileged port issues with rootless containers
      - '${FRONTEND_HTTPS_PORT:-8443}:8443'
    volumes:
      # SSL certificates - writable for auto-generation, or mount pre-existing certs
      - frontend_certs:/etc/nginx/certs:U
    environment:
      # Extra SANs for the auto-generated certificate (e.g., your server's IP)
      # Format: IP:192.168.1.100,DNS:myhost.local
      - SSL_SAN_EXTRA=${SSL_SAN_EXTRA:-}
      # SSL/TLS Configuration
      # SSL enabled by default - auto-generates self-signed certs if none mounted
      - SSL_ENABLED=${SSL_ENABLED:-true}
      # Custom certificate paths (relative to /etc/nginx/certs mount)
      - SSL_CERT_PATH=${SSL_CERT_PATH:-/etc/nginx/certs/cert.pem}
      - SSL_KEY_PATH=${SSL_KEY_PATH:-/etc/nginx/certs/key.pem}
      # HTTPS port for redirect (must match external port mapping)
      - FRONTEND_HTTPS_PORT=${FRONTEND_HTTPS_PORT:-8443}
    # NOTE: VITE_* environment variables are NOT used at runtime.
    # Vite bakes these values at build time. The frontend uses relative URLs
    # (empty BASE_URL), and nginx proxies /api and /ws to the backend.
    # See frontend/nginx.conf for proxy configuration.
    depends_on:
      backend:
        condition: service_healthy
    healthcheck:
      # Use /health endpoint which returns 200 directly (avoids 301 redirect issues)
      test: ['CMD', 'wget', '--no-verbose', '--tries=1', '--spider', 'http://localhost:8080/health']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M

  # =============================================================================
  # Monitoring Stack (Included by Default)
  # =============================================================================
  # Monitoring services are now part of the core deployment for full observability.
  # Access points:
  #   - Grafana: http://localhost:3002 (anonymous access enabled by default)
  #   - Prometheus: http://localhost:9090
  #   - Jaeger: http://localhost:16686 (distributed tracing)
  #   - Alertmanager: http://localhost:9093

  # Elasticsearch for Jaeger trace storage (NEM-3053)
  # Single-node deployment with 30-day retention via ILM
  elasticsearch:
    # Security hardening: Drop all capabilities except IPC_LOCK for memory locking
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - IPC_LOCK # Required for bootstrap.memory_lock=true (mlockall)
    image: docker.io/elasticsearch:8.12.0
    environment:
      # Single-node discovery (no cluster)
      - discovery.type=single-node
      # Disable security for internal network (no external exposure)
      - xpack.security.enabled=false
      # Memory settings - ES needs locked memory for performance
      - bootstrap.memory_lock=true
      - 'ES_JAVA_OPTS=-Xms${ES_HEAP_SIZE:-2g} -Xmx${ES_HEAP_SIZE:-2g}'
      # Reduce disk watermarks for single-node
      - cluster.routing.allocation.disk.threshold_enabled=true
      - cluster.routing.allocation.disk.watermark.low=85%
      - cluster.routing.allocation.disk.watermark.high=90%
      - cluster.routing.allocation.disk.watermark.flood_stage=95%
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
      - ./monitoring/elasticsearch:/usr/share/elasticsearch/config/ilm:ro,z
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    healthcheck:
      test:
        [
          'CMD-SHELL',
          'curl -s http://localhost:9200/_cluster/health | grep -q ''"status":"green"\|"status":"yellow"''',
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: ${ES_MEMORY_LIMIT:-4G}

  # Jaeger all-in-one for distributed tracing (NEM-1629, NEM-3053)
  # Provides trace collection, storage, and visualization for cross-service request correlation
  # Uses Elasticsearch backend for persistent storage with 30-day retention
  jaeger:
    # Security hardening: Drop all capabilities, prevent privilege escalation
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    image: docker.io/jaegertracing/all-in-one:1.54
    ports:
      - '16686:16686' # Jaeger UI
      - '4317:4317' # OTLP gRPC receiver
      - '4318:4318' # OTLP HTTP receiver
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      # Elasticsearch storage backend (NEM-3053)
      - SPAN_STORAGE_TYPE=elasticsearch
      - ES_SERVER_URLS=http://elasticsearch:9200
      # Index configuration
      - ES_INDEX_PREFIX=jaeger
      - ES_TAGS_AS_FIELDS_ALL=true
      # Performance tuning
      - ES_NUM_SHARDS=1
      - ES_NUM_REPLICAS=0
      - ES_BULK_SIZE=5000000
      - ES_BULK_WORKERS=1
      - ES_BULK_FLUSH_INTERVAL=200ms
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ['CMD', 'wget', '--no-verbose', '--tries=1', '--spider', 'http://localhost:16686']
      interval: 15s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M

  prometheus:
    # Security hardening: Drop all capabilities, prevent privilege escalation
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    image: docker.io/prom/prometheus:v2.48.0
    ports:
      - '9090:9090'
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro,z
      - ./monitoring/prometheus_rules.yml:/etc/prometheus/prometheus_rules.yml:ro,z
      - ./monitoring/prometheus-rules.yml:/etc/prometheus/prometheus-rules.yml:ro,z
      - ./monitoring/alerting-rules.yml:/etc/prometheus/alerting-rules.yml:ro,z
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=${PROMETHEUS_RETENTION_TIME:-15d}'
      - '--web.enable-lifecycle'
    healthcheck:
      test:
        ['CMD', 'wget', '--no-verbose', '--tries=1', '--spider', 'http://localhost:9090/-/healthy']
      interval: 15s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M

  grafana:
    # Security hardening: Drop all capabilities, prevent privilege escalation
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    image: docker.io/grafana/grafana:10.2.3
    ports:
      - '3002:3000'
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro,z
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro,z
    environment:
      # Anonymous access enabled for local development (no login required)
      # Set GF_AUTH_ANONYMOUS_ENABLED=false in .env to require login
      # When disabling anonymous, also set GF_AUTH_DISABLE_LOGIN_FORM=false to enable login form
      - GF_AUTH_ANONYMOUS_ENABLED=${GF_AUTH_ANONYMOUS_ENABLED:-true}
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=${GF_AUTH_DISABLE_LOGIN_FORM:-true}
      # Admin credentials (anonymous access is enabled by default, so password is optional)
      # For production with login enabled: generate with openssl rand -base64 32
      - GF_SECURITY_ADMIN_USER=${GF_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_ADMIN_PASSWORD:-admin}
      # Allow embedding Grafana dashboards in iframes (required for AI Performance page)
      - GF_SECURITY_ALLOW_EMBEDDING=true
      # Enable Grafana Explore for distributed tracing (required for Tracing page)
      - GF_EXPLORE_ENABLED=true
      - GF_USERS_ALLOW_SIGN_UP=false
      # Subpath configuration for nginx proxy at /grafana/
      # This enables Grafana to be accessed via https://<host>:8444/grafana/
      # Use relative root URL for reverse proxy compatibility (allows any hostname/port)
      - GF_SERVER_ROOT_URL=/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      # Pin JSON API plugin to v1.3.0 for Grafana 10.x compatibility (v1.3.15+ requires Grafana 11.6.0+)
      # Note: grafana-pyroscope-datasource is a core plugin in Grafana 10.x and should not be installed
      - GF_INSTALL_PLUGINS=marcusolsson-json-datasource 1.3.0
      # SMTP Configuration for Email Alerts (NEM-3511)
      # Configure via .env file for email alerting capability
      # See docs/operator/smtp-configuration.md for setup instructions
      - GF_SMTP_ENABLED=${GF_SMTP_ENABLED:-false}
      - GF_SMTP_HOST=${GF_SMTP_HOST:-smtp.example.com:587}
      - GF_SMTP_USER=${GF_SMTP_USER:-}
      - GF_SMTP_PASSWORD=${GF_SMTP_PASSWORD:-}
      - GF_SMTP_FROM_ADDRESS=${GF_SMTP_FROM_ADDRESS:-grafana@example.com}
      - GF_SMTP_FROM_NAME=${GF_SMTP_FROM_NAME:-Grafana Alerts}
      - GF_SMTP_STARTTLS_POLICY=${GF_SMTP_STARTTLS_POLICY:-MandatoryStartTLS}
      - GF_SMTP_SKIP_VERIFY=${GF_SMTP_SKIP_VERIFY:-false}
    depends_on:
      prometheus:
        condition: service_healthy
    healthcheck:
      test:
        ['CMD', 'wget', '--no-verbose', '--tries=1', '--spider', 'http://localhost:3000/api/health']
      interval: 15s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 256M

  redis-exporter:
    # Security hardening: Drop all capabilities, prevent privilege escalation
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    image: docker.io/oliver006/redis_exporter:v1.55.0
    ports:
      - '9121:9121'
    environment:
      - REDIS_ADDR=redis://redis:6379
      # SECURITY: Redis password for exporter authentication (must match redis service)
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
    depends_on:
      redis:
        condition: service_healthy
    # NOTE: redis_exporter is a minimal Go binary without shell tools (wget/curl).
    # Disable container health check; rely on process running and Prometheus scraping.
    healthcheck:
      disable: true
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 64M

  json-exporter:
    # Security hardening: Drop all capabilities, prevent privilege escalation
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    image: docker.io/prometheuscommunity/json-exporter:v0.6.0
    ports:
      - '7979:7979'
    volumes:
      - ./monitoring/json-exporter-config.yml:/etc/json-exporter/config.yml:ro,z
    command:
      - '--config.file=/etc/json-exporter/config.yml'
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 64M

  # Alertmanager for alert routing, grouping, and delivery (NEM-1631)
  alertmanager:
    # Security hardening: Drop all capabilities, prevent privilege escalation
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    image: docker.io/prom/alertmanager:v0.27.0
    ports:
      - '9093:9093'
    volumes:
      - ./monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro,z
      - alertmanager_data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.listen-address=:9093'
      - '--cluster.listen-address='
    healthcheck:
      test:
        ['CMD', 'wget', '--no-verbose', '--tries=1', '--spider', 'http://localhost:9093/-/healthy']
      interval: 15s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 128M

  # Blackbox Exporter for synthetic monitoring (NEM-1637)
  # Probes endpoints for availability, latency, and health status
  blackbox-exporter:
    # Security hardening: Drop all capabilities, prevent privilege escalation
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    image: docker.io/prom/blackbox-exporter:v0.24.0
    ports:
      - '9115:9115'
    volumes:
      - ./monitoring/blackbox-exporter.yml:/etc/blackbox_exporter/config.yml:ro,z
    command:
      - '--config.file=/etc/blackbox_exporter/config.yml'
      - '--log.level=info'
    healthcheck:
      test:
        ['CMD', 'wget', '--no-verbose', '--tries=1', '--spider', 'http://localhost:9115/metrics']
      interval: 15s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 64M

  # Loki for log aggregation (NEM-3093)
  # Receives logs from Alloy and stores them for querying via Grafana
  loki:
    # Security hardening: Drop all capabilities, prevent privilege escalation
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    image: docker.io/grafana/loki:2.9.4
    container_name: loki
    volumes:
      - ./monitoring/loki/loki-config.yml:/etc/loki/config.yml:ro,z
      - loki_data:/loki
    command: -config.file=/etc/loki/config.yml
    ports:
      - '3100:3100'
    healthcheck:
      test: ['CMD', 'wget', '-q', '--spider', 'http://localhost:3100/ready']
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 512M

  # Pyroscope for continuous profiling (NEM-3097)
  # Custom image with busybox/wget for health checks (the official image is scratch-based)
  # Services push profiles via py-spy when PYROSCOPE_ENABLED=true
  # NEM-3514: Upgraded from 1.4.0 to 1.18.0 to fix "unknown profile type: seconds" error.
  # Fix: https://github.com/grafana/pyroscope/pull/4568 (released in v1.9.2+)
  pyroscope:
    # Security hardening: Drop all capabilities, prevent privilege escalation
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    build:
      context: ./monitoring/pyroscope
      dockerfile: Dockerfile
    container_name: pyroscope
    volumes:
      - ./monitoring/pyroscope/pyroscope-config.yml:/etc/pyroscope/config.yml:ro,z
      - pyroscope_data:/data
    command: -config.file=/etc/pyroscope/config.yml
    ports:
      - '4040:4040'
    networks:
      - security-net
    healthcheck:
      test: ['CMD', 'wget', '-q', '--spider', 'http://localhost:4040/ready']
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 512M

  # Alloy for log collection via eBPF (NEM-3093)
  # Collects container logs from Podman and forwards to Loki
  alloy:
    image: docker.io/grafana/alloy:v1.0.0
    container_name: alloy
    # Use specific capabilities instead of privileged mode for eBPF
    # CAP_SYS_ADMIN: Required for BPF operations
    # CAP_SYS_PTRACE: Required for process tracing
    # CAP_PERFMON: Linux 5.8+ perf event access
    # CAP_BPF: Linux 5.8+ BPF program loading
    # CAP_DAC_READ_SEARCH: Read files without permission checks
    # CAP_SYS_RESOURCE: Resource limits for cgroup access
    cap_drop:
      - ALL
    cap_add:
      - SYS_ADMIN
      - SYS_PTRACE
      - PERFMON
      - BPF
      - DAC_READ_SEARCH
      - SYS_RESOURCE
    pid: host
    user: root
    security_opt:
      - label:disable
      - no-new-privileges:true
    volumes:
      - ./monitoring/alloy/config.alloy:/etc/alloy/config.alloy:ro,z
      - /run/user/1000/podman/podman.sock:/run/user/1000/podman/podman.sock:ro
      - /sys/kernel/debug:/sys/kernel/debug:ro
      - /sys/fs/cgroup:/sys/fs/cgroup:ro
      - /proc:/host/proc:ro
    environment:
      - HOST_PROC=/host/proc
    command:
      - run
      - --stability.level=public-preview
      - /etc/alloy/config.alloy
    ports:
      - '12345:12345' # Alloy UI
      - '4317' # OTLP gRPC (internal only)
      - '4318' # OTLP HTTP (internal only)
    depends_on:
      - loki
      - pyroscope
    healthcheck:
      # Alloy doesn't have wget/curl - use process check instead
      test: ['CMD', 'pgrep', '-f', 'alloy']
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - security-net
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 768M

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  elasticsearch_data:
    driver: local
  # Persistent HuggingFace cache volume (NEM-3812)
  # Shared across all AI services to avoid re-downloading models
  hf_cache:
    driver: local
  # Redis Sentinel volumes (uncomment when enabling Sentinel)
  # sentinel1_data:
  #   driver: local
  # sentinel2_data:
  #   driver: local
  # sentinel3_data:
  #   driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  alertmanager_data:
    driver: local
  clip-tensorrt-cache:
    driver: local
  loki_data:
    driver: local
  pyroscope_data:
    driver: local
  # Frontend SSL certificates volume for auto-generation with read-only filesystem
  frontend_certs:
    driver: local

networks:
  security-net:
    driver: bridge
# ============================================================================
# DOCKER SECRETS SETUP (Optional - Recommended for Production)
# ============================================================================
#
# Docker Secrets provide enhanced security for credential management:
#   - Credentials stored separately from environment variables
#   - Not visible in 'docker inspect' output
#   - Easier credential rotation without image rebuild
#   - Secrets mounted read-only in /run/secrets/ inside containers
#
# SETUP INSTRUCTIONS:
# ==================
#
# 1. Create secrets directory and files:
#    mkdir -p secrets
#    chmod 700 secrets
#
# 2. Generate strong passwords:
#    openssl rand -base64 32 > secrets/postgres_password.txt
#    openssl rand -base64 32 > secrets/redis_password.txt
#    openssl rand -base64 32 > secrets/grafana_admin_password.txt
#    chmod 600 secrets/*.txt
#
# 3. Add secret values to the files (or use commands above to generate):
#    echo "your_postgres_password" > secrets/postgres_password.txt
#    echo "your_redis_password" > secrets/redis_password.txt
#    echo "your_grafana_password" > secrets/grafana_admin_password.txt
#
# 4. Enable Docker secrets by uncommenting the sections below
#
# 5. Validate configuration:
#    docker compose -f docker-compose.prod.yml config
#
# 6. Start services with secrets:
#    docker compose -f docker-compose.prod.yml up -d
#
# 7. Verify secrets are mounted in containers:
#    docker compose -f docker-compose.prod.yml exec postgres ls -la /run/secrets/
#
# ROTATION PROCEDURE (Zero-Downtime):
# ===================================
#
# 1. Update secret file:
#    echo "new_password" > secrets/postgres_password.txt
#
# 2. Restart affected service:
#    docker compose -f docker-compose.prod.yml restart postgres
#
# 3. Update dependent services if needed:
#    docker compose -f docker-compose.prod.yml restart backend redis-exporter
#
# IMPORTANT NOTES:
# ================
#
# - Docker Secrets work with Docker Swarm and standalone Docker/Podman
# - Secrets are NOT encrypted at rest in standalone Docker (use full-disk encryption)
# - Never commit secrets/ directory to version control
# - Add secrets/ to .gitignore (already configured)
# - When using secrets, REMOVE corresponding environment variables from .env
# - Ensure file paths are relative to docker-compose.prod.yml location
#
# ============================================================================

# Uncomment the entire sections below to enable Docker Secrets
#
# secrets:
#   postgres_password:
#     file: ./secrets/postgres_password.txt
#   redis_password:
#     file: ./secrets/redis_password.txt
#   grafana_admin_password:
#     file: ./secrets/grafana_admin_password.txt
