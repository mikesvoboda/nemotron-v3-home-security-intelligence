# Benchmark Documentation

> Performance benchmarks and measurements for the Home Security Intelligence AI pipeline.

This directory contains benchmark results documenting AI model performance, VRAM usage, load times, and inference speeds on the target hardware (NVIDIA RTX A5500).

---

## Current Benchmarks

| Benchmark                                     | Description                                     | Last Updated |
| --------------------------------------------- | ----------------------------------------------- | ------------ |
| [Model Zoo Benchmark](model-zoo-benchmark.md) | Performance metrics for all Model Zoo AI models | 2026-01-01   |

---

## Model Zoo Benchmark Summary

The Model Zoo benchmark tests AI models for:

- **Load time** - Time to load model into GPU memory
- **VRAM usage** - GPU memory consumed
- **Inference time** - Time to process a single image
- **VRAM recovery** - Whether memory is properly released after unload

### Success Criteria

| Criteria           | Target  |
| ------------------ | ------- |
| Max VRAM per model | <1500MB |
| Max load time      | <5s     |
| VRAM recovered     | Yes     |

### Current Results

| Model                  | Status | VRAM  | Inference |
| ---------------------- | ------ | ----- | --------- |
| fashion-clip           | ERROR  | N/A   | N/A       |
| weather-classification | OK     | 444MB | 274ms     |
| pet-classifier         | OK     | 2MB   | N/A       |

---

## Running Benchmarks

Benchmarks are generated by Python scripts:

```bash
# Run Model Zoo benchmark (requires GPU)
uv run python scripts/benchmark_model_zoo.py

# Benchmark specific models
uv run python scripts/benchmark_model_zoo.py --models yolo11-license-plate,clip-vit-l

# Results are written to docs/benchmarks/model-zoo-benchmark.md
```

---

## Refresh Schedule

Refresh benchmarks when:

- **New models added** to the Model Zoo
- **Model updates** change versions or configurations
- **Hardware changes** when deploying to new GPU
- **Quarterly** at minimum for ongoing validation

### Refresh Procedure

```bash
# 1. Verify GPU access
nvidia-smi

# 2. Download any missing models
uv run python scripts/download-model-zoo.py

# 3. Run benchmark
uv run python scripts/benchmark_model_zoo.py

# 4. Review and commit
cat docs/benchmarks/model-zoo-benchmark.md
git add docs/benchmarks/model-zoo-benchmark.md
git commit -m "docs: refresh model zoo benchmark results"
```

---

## Adding New Benchmarks

When creating new benchmark files:

1. Use date-prefixed filenames if version-specific: `YYYY-MM-DD-benchmark-name.md`
2. Include GPU/hardware information
3. Document success criteria
4. Include both summary and detailed results
5. Update this README with the new entry

---

## Related Documentation

- [AGENTS.md](AGENTS.md) - Agent guide for this directory
- [AI Pipeline Architecture](../architecture/ai-pipeline.md) - AI pipeline design
- [AI Performance Tuning](../operator/ai-performance.md) - Operator guide for AI optimization

---

[Back to Documentation Index](../README.md)
