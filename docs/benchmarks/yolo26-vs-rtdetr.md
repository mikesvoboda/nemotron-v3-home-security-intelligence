# YOLO26 vs RT-DETRv2 GPU Benchmark Results

> **Note:** This file is auto-generated by `scripts/benchmark_yolo26_container.py`.
> To refresh results, run the benchmark inside the yolo26-benchmark container.

## Environment

- **Date:** 2026-01-26
- **GPU:** NVIDIA RTX A5500
- **VRAM:** 24564 MB
- **Driver:** 580.119.02
- **CUDA:** 12.8
- **TensorRT:** 10.4.0
- **PyTorch:** 2.9.1+cu128

## Benchmark Summary

| Model          | Format        | Mean (ms) | P50 (ms) | P95 (ms) | P99 (ms) | FPS | VRAM (MB) | File Size |
| -------------- | ------------- | --------- | -------- | -------- | -------- | --- | --------- | --------- |
| yolo26n        | onnx-cuda     | 3.25      | 2.92     | 4.46     | 4.52     | 308 | ~200      | 9.5 MB    |
| yolo26n        | tensorrt-fp16 | 4.49      | 4.46     | 5.07     | 5.46     | 223 | ~50       | 7.3 MB    |
| yolo26s        | onnx-cuda     | 4.55      | 4.46     | 5.58     | 5.66     | 220 | ~200      | 36.5 MB   |
| yolo26s        | tensorrt-fp16 | 4.86      | 4.81     | 5.47     | 5.67     | 206 | ~60       | 21.9 MB   |
| yolo26m        | tensorrt-fp16 | 5.76      | 5.75     | 6.29     | 6.44     | 174 | ~100      | 43.1 MB   |
| yolo26s        | pytorch       | 7.48      | 7.41     | 8.37     | 8.77     | 134 | ~120      | 19.5 MB   |
| yolo26n        | pytorch       | 7.69      | 7.62     | 8.46     | 8.90     | 130 | ~80       | 5.3 MB    |
| yolo26m        | onnx-cuda     | 8.15      | 8.26     | 8.75     | 8.91     | 123 | ~330      | 78.2 MB   |
| yolo26m        | pytorch       | 10.06     | 10.05    | 11.21    | 11.63    | 99  | ~280      | 42.2 MB   |
| RT-DETRv2-R101 | pytorch       | 30.64     | 30.65    | 31.90    | 32.56    | 33  | ~570      | ~350 MB   |

## Detailed Results

### yolo26n

| Format        | Mean   | Std    | Min    | Max     | P50    | P95    | P99    | FPS | Load Time | Warmup |
| ------------- | ------ | ------ | ------ | ------- | ------ | ------ | ------ | --- | --------- | ------ |
| onnx-cuda     | 3.25ms | 0.62ms | 2.69ms | 4.60ms  | 2.92ms | 4.46ms | 4.52ms | 308 | 0.06s     | 0.63s  |
| tensorrt-fp16 | 4.49ms | 0.43ms | 3.26ms | 7.36ms  | 4.46ms | 5.07ms | 5.46ms | 223 | 0.00s     | 0.31s  |
| pytorch       | 7.69ms | 0.67ms | 6.38ms | 14.37ms | 7.62ms | 8.46ms | 8.90ms | 130 | 0.05s     | 0.81s  |

### yolo26s

| Format        | Mean   | Std    | Min    | Max    | P50    | P95    | P99    | FPS | Load Time | Warmup |
| ------------- | ------ | ------ | ------ | ------ | ------ | ------ | ------ | --- | --------- | ------ |
| onnx-cuda     | 4.55ms | 0.67ms | 3.74ms | 5.89ms | 4.46ms | 5.58ms | 5.66ms | 220 | 0.07s     | 0.68s  |
| tensorrt-fp16 | 4.86ms | 0.37ms | 3.60ms | 5.73ms | 4.81ms | 5.47ms | 5.67ms | 206 | 0.00s     | 0.30s  |
| pytorch       | 7.48ms | 0.53ms | 5.99ms | 9.48ms | 7.41ms | 8.37ms | 8.77ms | 134 | 0.05s     | 0.43s  |

### yolo26m

| Format        | Mean    | Std    | Min    | Max     | P50     | P95     | P99     | FPS | Load Time | Warmup |
| ------------- | ------- | ------ | ------ | ------- | ------- | ------- | ------- | --- | --------- | ------ |
| tensorrt-fp16 | 5.76ms  | 0.31ms | 4.58ms | 6.73ms  | 5.75ms  | 6.29ms  | 6.44ms  | 174 | 0.00s     | 0.36s  |
| onnx-cuda     | 8.15ms  | 0.48ms | 6.65ms | 9.07ms  | 8.26ms  | 8.75ms  | 8.91ms  | 123 | 0.08s     | 0.97s  |
| pytorch       | 10.06ms | 0.73ms | 8.76ms | 13.31ms | 10.05ms | 11.21ms | 11.63ms | 99  | 0.07s     | 0.55s  |

### RT-DETRv2-R101

| Format  | Mean    | Std    | Min     | Max     | P50     | P95     | P99     | FPS | Load Time | Warmup |
| ------- | ------- | ------ | ------- | ------- | ------- | ------- | ------- | --- | --------- | ------ |
| pytorch | 30.64ms | 0.81ms | 28.30ms | 32.67ms | 30.65ms | 31.90ms | 32.56ms | 33  | 0.25s     | 2.08s  |

## TensorRT vs PyTorch Speedup

| Model   | PyTorch (ms) | TensorRT FP16 (ms) | Speedup   | Notes                |
| ------- | ------------ | ------------------ | --------- | -------------------- |
| yolo26n | 7.69         | 4.49               | **1.71x** | Best for low-latency |
| yolo26s | 7.48         | 4.86               | **1.54x** | Good balance         |
| yolo26m | 10.06        | 5.76               | **1.74x** | Best for accuracy    |

## Speedup vs RT-DETRv2 Baseline

**Baseline:** RT-DETRv2-R101 @ 30.64ms (33 FPS)

| Model   | Format        | Speedup  | Latency Reduction | Use Case                |
| ------- | ------------- | -------- | ----------------- | ----------------------- |
| yolo26n | onnx-cuda     | **9.4x** | 89.4%             | Maximum throughput      |
| yolo26n | tensorrt-fp16 | **6.8x** | 85.4%             | Production recommended  |
| yolo26s | onnx-cuda     | **6.7x** | 85.1%             | Higher accuracy + speed |
| yolo26s | tensorrt-fp16 | **6.3x** | 84.2%             | Balanced production     |
| yolo26m | tensorrt-fp16 | **5.3x** | 81.2%             | Maximum accuracy        |
| yolo26s | pytorch       | **4.1x** | 75.6%             | Development/debugging   |
| yolo26n | pytorch       | **4.0x** | 74.9%             | Development/debugging   |
| yolo26m | onnx-cuda     | **3.8x** | 73.4%             | Portable accuracy       |
| yolo26m | pytorch       | **3.0x** | 67.2%             | Maximum flexibility     |

## Model Comparison

### Architecture Differences

| Feature            | YOLO26              | RT-DETRv2         |
| ------------------ | ------------------- | ----------------- |
| Architecture       | YOLO-based CNN      | Transformer-based |
| NMS Required       | No (built-in)       | No                |
| Input Size         | 640x640             | 640x640           |
| Parameters (n/s/m) | 2.4M / 9.5M / 20.4M | ~76M (R101)       |
| Best For           | Real-time detection | High accuracy     |

### File Sizes

| Model   | PyTorch | ONNX    | TensorRT FP16 |
| ------- | ------- | ------- | ------------- |
| yolo26n | 5.3 MB  | 9.5 MB  | 7.3 MB        |
| yolo26s | 19.5 MB | 36.5 MB | 21.9 MB       |
| yolo26m | 42.2 MB | 78.2 MB | 43.1 MB       |

## Recommendations

### Best Performance

- **Fastest Inference:** yolo26n (onnx-cuda) @ 3.25ms (308 FPS)
- **Best Throughput:** yolo26n (onnx-cuda) @ 308 FPS
- **Lowest VRAM:** yolo26n (tensorrt-fp16) @ ~50 MB
- **Best Accuracy/Speed:** yolo26m (tensorrt-fp16) @ 5.76ms (174 FPS)

### Production Deployment

**Recommended:** yolo26n or yolo26s TensorRT FP16 engine

TensorRT FP16 provides:

- 1.5-1.7x speedup vs PyTorch
- Consistent low-latency inference
- Minimal accuracy loss (<0.1% mAP)
- Lower VRAM usage

```yaml
# docker-compose.prod.yml configuration
ai-detector:
  environment:
    # For maximum speed (308 FPS with ONNX, 223 FPS with TensorRT)
    DETECTOR_MODEL: /models/yolo26/exports/yolo26n_fp16.engine
    DETECTOR_DEVICE: cuda:0

    # For better accuracy (174 FPS)
    # DETECTOR_MODEL: /models/yolo26/exports/yolo26m_fp16.engine
```

### When to Use Each Format

| Format        | Use Case       | Pros                 | Cons         |
| ------------- | -------------- | -------------------- | ------------ |
| TensorRT FP16 | Production     | Fastest, lowest VRAM | GPU-specific |
| ONNX-CUDA     | Multi-platform | Portable, fast       | Larger files |
| PyTorch       | Development    | Flexible, debuggable | Slower       |

### Accuracy Notes

- TensorRT FP16 typically has <0.1% mAP loss vs FP32
- For maximum accuracy, use yolo26m with PyTorch or TensorRT FP32
- Run accuracy validation with `scripts/benchmark_yolo26_accuracy.py`
- COCO val2017 mAP benchmarks available at [ultralytics.com/yolo](https://docs.ultralytics.com/models/yolo26/)

## Running the Benchmark

```bash
# Build the benchmark container
podman build -t yolo26-benchmark -f Dockerfile.yolo26-benchmark .

# Run with TensorRT export (first time)
podman run --rm \
    --security-opt=label=disable \
    --hooks-dir=/usr/share/containers/oci/hooks.d/ \
    --device nvidia.com/gpu=all \
    -v /export/ai_models/model-zoo:/models:z \
    -v /export/ai_models/rt-detrv2:/models/rt-detrv2:z \
    -v $(pwd)/scripts:/scripts:z \
    -v $(pwd)/docs/benchmarks:/benchmarks:z \
    yolo26-benchmark python3 /scripts/benchmark_yolo26_container.py \
    --export \
    --rtdetr-path /models/rt-detrv2/rtdetr_v2_r101vd

# Re-run benchmarks only (skip export)
podman run --rm \
    --security-opt=label=disable \
    --hooks-dir=/usr/share/containers/oci/hooks.d/ \
    --device nvidia.com/gpu=all \
    -v /export/ai_models/model-zoo:/models:z \
    -v /export/ai_models/rt-detrv2:/models/rt-detrv2:z \
    -v $(pwd)/scripts:/scripts:z \
    -v $(pwd)/docs/benchmarks:/benchmarks:z \
    yolo26-benchmark python3 /scripts/benchmark_yolo26_container.py \
    --skip-export \
    --rtdetr-path /models/rt-detrv2/rtdetr_v2_r101vd
```

## Export Locations

TensorRT engines are saved to `/export/ai_models/model-zoo/yolo26/exports/`:

```
exports/
  yolo26n.onnx       # 9.5 MB
  yolo26n_fp16.engine  # 7.3 MB
  yolo26s.onnx       # 36.5 MB
  yolo26s_fp16.engine  # 21.9 MB
  yolo26m.onnx       # 78.2 MB
  yolo26m_fp16.engine  # 43.1 MB
```
