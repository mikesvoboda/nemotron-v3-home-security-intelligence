# Benchmarks Directory - Agent Guide

## Purpose

This directory contains benchmark results and performance measurements for the Home Security Intelligence system. Benchmarks document AI model performance, VRAM usage, load times, and inference speeds.

## Directory Contents

```
benchmarks/
  AGENTS.md                  # This file
  model-zoo-benchmark.md     # Model Zoo benchmark results
```

## Key Files

### model-zoo-benchmark.md

**Purpose:** Benchmark results for AI models in the Model Zoo.

**Content:**

- Date and GPU information (NVIDIA RTX A5500)
- Summary table with model performance metrics
- Statistics on models benchmarked, success/failure rates
- Success criteria and pass/fail status
- Detailed results per model including:
  - Load time (seconds)
  - VRAM usage (MB)
  - Inference time (ms)
  - VRAM recovery status

**Models Benchmarked:**

| Model                  | Status | VRAM  | Inference |
| ---------------------- | ------ | ----- | --------- |
| fashion-clip           | ERROR  | N/A   | N/A       |
| weather-classification | OK     | 444MB | 274ms     |
| pet-classifier         | OK     | 2MB   | N/A       |

**Success Criteria:**

- Max VRAM per model: <1500MB
- Max load time: <5s
- VRAM recovered after unload

**When to use:** Evaluating model performance, deciding which models to use, debugging VRAM issues.

## Benchmark Format

Benchmark files follow this structure:

```markdown
# [Model/Component] Benchmark Results

**Date:** YYYY-MM-DD HH:MM:SS
**GPU:** [GPU Model]

## Summary

| Model | Load Time | VRAM Used | Inference | Recovered | Status |
| ----- | --------- | --------- | --------- | --------- | ------ |
| ...   | ...       | ...       | ...       | ...       | ...    |

## Statistics

- Models benchmarked: X
- Successful: X
- Failed: X
- Total VRAM: XMB

## Success Criteria

| Criteria | Target | Actual | Pass |
| -------- | ------ | ------ | ---- |
| ...      | ...    | ...    | ...  |

## Detailed Results

### [Model Name]

- Load time: Xs
- VRAM before/after: XMB
- Inference time: Xms
- Unload time: Xs
```

## Running Benchmarks

Benchmarks are generated by running the benchmark scripts:

```bash
# Model Zoo benchmark (requires GPU)
uv run python scripts/benchmark_model_zoo.py

# Benchmark specific models
uv run python scripts/benchmark_model_zoo.py --models yolo11-license-plate,clip-vit-l

# Results are written to docs/benchmarks/model-zoo-benchmark.md
```

## Refresh Procedure

Benchmark results should be refreshed when:

1. **New models added:** After adding new models to the Model Zoo
2. **Model updates:** After updating model versions or configurations
3. **Hardware changes:** When deploying to new GPU hardware
4. **Quarterly review:** At minimum, refresh benchmarks quarterly

To refresh benchmark results:

```bash
# 1. Ensure GPU is available and models are downloaded
nvidia-smi  # Verify GPU access

# 2. Download any missing models
uv run python scripts/download-model-zoo.py

# 3. Run the benchmark script
uv run python scripts/benchmark_model_zoo.py

# 4. Review the results
cat docs/benchmarks/model-zoo-benchmark.md

# 5. Commit the updated results
git add docs/benchmarks/model-zoo-benchmark.md
git commit -m "docs: refresh model zoo benchmark results"
```

**Note:** Running benchmarks requires a GPU with sufficient VRAM. The benchmark script will skip models that fail to load.

## Adding New Benchmarks

When adding new benchmark results:

1. Use date-prefixed filenames if version-specific: `YYYY-MM-DD-benchmark-name.md`
2. Include GPU/hardware information
3. Document success criteria
4. Include both summary and detailed results
5. Update this AGENTS.md with the new file

## Related Documentation

- **docs/AGENTS.md:** Documentation directory overview
- **docs/architecture/ai-pipeline.md:** AI pipeline architecture
- **ai/AGENTS.md:** AI services implementation
- **docs/operator/ai-performance.md:** AI performance tuning
