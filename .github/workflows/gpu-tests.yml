name: GPU Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  gpu-integration:
    name: GPU Integration Tests
    runs-on: [self-hosted, gpu, rtx-a5500]
    # Non-blocking: GPU tests are informational and should not block PRs
    # The self-hosted GPU runner may have variable memory availability
    continue-on-error: true
    # Fork protection - only run for trusted sources
    if: >
      github.event.pull_request.head.repo.full_name == github.repository ||
      github.event_name == 'push'
    timeout-minutes: 30
    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Verify GPU availability
        run: |
          nvidia-smi
          echo "GPU Memory: $(nvidia-smi --query-gpu=memory.total --format=csv,noheader)"

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: '0.9.18'

      - name: Set up Python with uv
        run: |
          uv sync --extra dev

      - name: Run GPU tests
        run: |
          # Exit code 5 means no tests collected - treat as success
          # Only run tests from gpu/ folder to avoid collecting unit tests that
          # import backend.services (which requires DATABASE_URL at collection time)
          uv run pytest backend/tests/gpu/ -m "gpu" -v --tb=short || [ $? -eq 5 ]
        env:
          CUDA_VISIBLE_DEVICES: '0'

      - name: Run AI inference benchmarks
        # Optional: benchmarks require PostgreSQL which may not be on GPU runner
        continue-on-error: true
        run: |
          # Disable xdist and override default opts to allow benchmarks
          # (xdist auto-disables benchmarking, and pytest.ini has xdist options)
          uv run pytest backend/tests/benchmarks/ \
            -p no:xdist \
            -o "addopts=-v --strict-markers --tb=short --timeout=30" \
            --benchmark-only \
            --benchmark-json=gpu-benchmark.json \
            -v || [ $? -eq 5 ]
        env:
          CUDA_VISIBLE_DEVICES: '0'

      - name: Upload benchmark results
        uses: actions/upload-artifact@v6
        with:
          name: gpu-benchmark-results
          path: gpu-benchmark.json

      - name: Check GPU memory usage
        if: always()
        run: |
          nvidia-smi --query-compute-apps=pid,used_memory --format=csv
          nvidia-smi
