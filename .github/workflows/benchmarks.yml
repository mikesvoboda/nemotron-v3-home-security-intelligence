name: Benchmarks

# Performance benchmarks for regression detection.
# Runs pytest-benchmark on backend tests and fails on >20% regression from baseline.
#
# Runs on merge to main only (not PRs) to save runner capacity.
# Creates Linear issue on failure for main branch pushes to ensure follow-up.

on:
  push:
    branches: [main]
    paths:
      - 'backend/**'
      - '.github/workflows/benchmarks.yml'
  # Manual trigger for on-demand benchmarking
  workflow_dispatch:
    inputs:
      compare_baseline:
        description: 'Compare against baseline'
        required: false
        default: true
        type: boolean
      regression_threshold:
        description: 'Regression threshold percentage'
        required: false
        default: '20'
        type: string

concurrency:
  group: benchmarks-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.14'
  UV_VERSION: '0.9.18'

jobs:
  # ============================================================================
  # Performance Benchmarks
  # ============================================================================
  benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    # Non-blocking: benchmark variance can cause false failures
    continue-on-error: true
    timeout-minutes: 15

    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres # pragma: allowlist secret
          POSTGRES_DB: security_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    env:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      TEST_DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      REDIS_URL: redis://localhost:6379
      REDIS_HOST: localhost
      REDIS_PORT: 6379

    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Determine benchmark options
        id: options
        run: |
          # Set regression threshold (default 20%)
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            THRESHOLD="${{ github.event.inputs.regression_threshold }}"
            COMPARE="${{ github.event.inputs.compare_baseline }}"
          else
            THRESHOLD="20"
            COMPARE="true"
          fi
          echo "threshold=$THRESHOLD" >> $GITHUB_OUTPUT
          echo "compare=$COMPARE" >> $GITHUB_OUTPUT
          echo "Using regression threshold: $THRESHOLD%"

      - name: Download baseline benchmark results
        if: steps.options.outputs.compare == 'true'
        uses: actions/cache@9255dc7a253b0ccc959486e2bca901246202afeb # v5
        id: benchmark-cache
        with:
          path: .benchmarks
          key: benchmark-baseline-${{ runner.os }}-${{ hashFiles('backend/tests/benchmarks/**/*.py') }}
          restore-keys: |
            benchmark-baseline-${{ runner.os }}-

      - name: Run performance benchmarks
        id: benchmarks
        run: |
          mkdir -p .benchmarks benchmark-results

          # Build benchmark command
          BENCHMARK_CMD="uv run pytest backend/tests/benchmarks/ \
            -v \
            --benchmark-only \
            --benchmark-autosave \
            --benchmark-save-data \
            --benchmark-json=benchmark-results/benchmark-results.json \
            --benchmark-min-rounds=5 \
            --benchmark-warmup=on \
            --timeout=120 \
            -n0"

          # Add comparison if baseline exists
          if [ "${{ steps.options.outputs.compare }}" == "true" ] && [ -d ".benchmarks" ] && [ "$(ls -A .benchmarks 2>/dev/null)" ]; then
            echo "Comparing against baseline with ${{ steps.options.outputs.threshold }}% threshold"
            BENCHMARK_CMD="$BENCHMARK_CMD --benchmark-compare --benchmark-compare-fail=mean:${{ steps.options.outputs.threshold }}%"
          else
            echo "No baseline found, running without comparison"
          fi

          # Run benchmarks
          echo "Running: $BENCHMARK_CMD"
          $BENCHMARK_CMD 2>&1 | tee benchmark-results/benchmark-output.txt || echo "exit_code=$?" >> $GITHUB_OUTPUT

      - name: Generate benchmark summary
        if: always()
        run: |
          echo "## Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** \`${{ github.sha }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** \`${{ github.ref_name }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Regression Threshold:** ${{ steps.options.outputs.threshold }}%" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "benchmark-results/benchmark-results.json" ]; then
            echo "### Benchmark Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Extract key metrics from JSON
            uv run python -c "
          import json
          import sys

          try:
              with open('benchmark-results/benchmark-results.json') as f:
                  data = json.load(f)

              benchmarks = data.get('benchmarks', [])
              if not benchmarks:
                  print('No benchmark results found.')
                  sys.exit(0)

              print('| Test | Mean (ms) | Std Dev | Min | Max | Rounds |')
              print('|------|-----------|---------|-----|-----|--------|')

              for bench in benchmarks:
                  name = bench.get('name', 'Unknown')
                  stats = bench.get('stats', {})
                  mean_ms = stats.get('mean', 0) * 1000
                  std_ms = stats.get('stddev', 0) * 1000
                  min_ms = stats.get('min', 0) * 1000
                  max_ms = stats.get('max', 0) * 1000
                  rounds = stats.get('rounds', 0)

                  # Truncate long test names
                  if len(name) > 50:
                      name = name[:47] + '...'

                  print(f'| {name} | {mean_ms:.3f} | {std_ms:.3f} | {min_ms:.3f} | {max_ms:.3f} | {rounds} |')

              print('')
              print(f'**Total benchmarks:** {len(benchmarks)}')
          except Exception as e:
              print(f'Error parsing results: {e}')
          " >> $GITHUB_STEP_SUMMARY

            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          if [ -f "benchmark-results/benchmark-output.txt" ]; then
            echo "### Test Output" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo '<details><summary>Full benchmark output</summary>' >> $GITHUB_STEP_SUMMARY
            echo '' >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -100 benchmark-results/benchmark-output.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo '</details>' >> $GITHUB_STEP_SUMMARY
          fi

      - name: Save baseline for future comparisons
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: actions/cache/save@v5
        with:
          path: .benchmarks
          key: benchmark-baseline-${{ runner.os }}-${{ hashFiles('backend/tests/benchmarks/**/*.py') }}-${{ github.sha }}

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: benchmark-results
          path: |
            benchmark-results/
            .benchmarks/
          retention-days: 30

  # ============================================================================
  # Summary and Issue Creation
  # ============================================================================
  benchmark-summary:
    name: Benchmark Summary
    runs-on: ubuntu-latest
    needs: [benchmarks]
    if: always()
    steps:
      - name: Check benchmark results
        run: |
          if [ "${{ needs.benchmarks.result }}" == "failure" ]; then
            echo "::warning::Performance benchmarks detected a regression. Review the artifacts for details."
            echo ""
            echo "To run benchmarks locally:"
            echo "  uv run pytest backend/tests/benchmarks/ --benchmark-only -v"
            echo ""
            echo "To compare against baseline:"
            echo "  uv run pytest backend/tests/benchmarks/ --benchmark-only --benchmark-compare"
          else
            echo "Performance benchmarks passed!"
          fi

      - name: Create Linear issue on failure
        # Create Linear issue for any failure (PRs and main pushes)
        if: needs.benchmarks.result == 'failure'
        env:
          LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Determine context (PR or push)
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            CONTEXT="PR #${{ github.event.pull_request.number }}"
            REF="${{ github.head_ref }}"
          else
            CONTEXT="main branch push"
            REF="${{ github.ref_name }}"
          fi

          # Create issue via Linear API
          curl -X POST https://api.linear.app/graphql \
            -H "Content-Type: application/json" \
            -H "Authorization: $LINEAR_API_KEY" \
            -d '{
              "query": "mutation CreateIssue($input: IssueCreateInput!) { issueCreate(input: $input) { success issue { id identifier url } } }",
              "variables": {
                "input": {
                  "teamId": "998946a2-aa75-491b-a39d-189660131392",
                  "title": "[CI] Performance benchmark regression ('"$CONTEXT"')",
                  "description": "## Performance Benchmark Regression\n\nThe pytest-benchmark tests detected a >20% performance regression.\n\n**Context:** '"$CONTEXT"'\n**Branch:** '"$REF"'\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\n### Action Required\n\n1. Review the benchmark results in the workflow artifacts\n2. Identify which benchmarks regressed\n3. Run benchmarks locally to investigate:\n   ```bash\n   uv run pytest backend/tests/benchmarks/ --benchmark-only -v\n   uv run pytest backend/tests/benchmarks/ --benchmark-only --benchmark-compare\n   ```\n4. Common causes of regressions:\n   - New database queries in hot paths\n   - Inefficient serialization\n   - Increased object allocations\n   - Missing caching\n\n### Benchmark Files\n\n- `backend/tests/benchmarks/test_api_benchmarks.py` - API endpoint response times\n- `backend/tests/benchmarks/test_performance.py` - Core operations (JSON, DB, services)\n- `backend/tests/benchmarks/test_bigo.py` - Algorithmic complexity\n- `backend/tests/benchmarks/test_memory.py` - Memory profiling",
                  "priority": 3,
                  "labelIds": ["fef74b6f-0631-42e2-87f9-3c31e4bab170"]
                }
              }
            }'
