name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '20'
  UV_VERSION: '0.9.18' # Update this single location to change uv version across all workflows
  PYROSCOPE_ENABLED: 'false' # Disable profiling in CI (pyroscope-io not installed)

jobs:
  # Change Detection Job - determines which tests to run based on changed files
  # Always runs tests on main branch pushes and when workflow file changes
  detect-changes:
    name: Detect Changed Files
    runs-on: ubuntu-latest
    outputs:
      backend: ${{ steps.filter.outputs.backend }}
      frontend: ${{ steps.filter.outputs.frontend }}
      docs-only: ${{ steps.filter.outputs.docs-only }}
      workflow: ${{ steps.filter.outputs.workflow }}
      should-run-all: ${{ github.ref == 'refs/heads/main' || steps.filter.outputs.workflow == 'true' }}
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Detect file changes
        uses: dorny/paths-filter@de90cc6fb38fc0963ad72b210f1f284cd68cea36 # v3.0.2
        id: filter
        with:
          filters: |
            backend:
              - 'backend/**'
              - 'pyproject.toml'
              - 'uv.lock'
              - 'ai/**'
              - 'scripts/**/*.py'
            frontend:
              - 'frontend/**'
              - '!frontend/**/*.md'
            docs-only:
              - 'docs/**'
              - '**/*.md'
              - '!frontend/**'
              - '!backend/**'
            workflow:
              - '.github/workflows/ci.yml'

      - name: Log detected changes
        run: |
          echo "## Change Detection Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Category | Changed |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|---------|" >> $GITHUB_STEP_SUMMARY
          echo "| Backend | ${{ steps.filter.outputs.backend }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Frontend | ${{ steps.filter.outputs.frontend }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Docs Only | ${{ steps.filter.outputs.docs-only }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Workflow | ${{ steps.filter.outputs.workflow }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Is Main Branch | ${{ github.ref == 'refs/heads/main' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ "${{ github.ref }}" == "refs/heads/main" ] || [ "${{ steps.filter.outputs.workflow }}" == "true" ]; then
            echo "**Running all tests** (main branch push or workflow changes)" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ steps.filter.outputs.backend }}" == "true" ] && [ "${{ steps.filter.outputs.frontend }}" == "true" ]; then
            echo "**Running all tests** (both backend and frontend changes)" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ steps.filter.outputs.backend }}" == "true" ]; then
            echo "**Running backend tests only** (frontend unchanged)" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ steps.filter.outputs.frontend }}" == "true" ]; then
            echo "**Running frontend tests only** (backend unchanged)" >> $GITHUB_STEP_SUMMARY
          else
            echo "**Skipping tests** (docs-only changes)" >> $GITHUB_STEP_SUMMARY
          fi

  # Backend Jobs

  # Shared backend dependencies - built once, used by all backend jobs
  build-backend-deps:
    name: Build Backend Dependencies
    runs-on: ubuntu-latest
    needs: [detect-changes]
    if: |
      needs.detect-changes.outputs.should-run-all == 'true' ||
      needs.detect-changes.outputs.backend == 'true'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true
          cache-suffix: backend-deps

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Verify installation
        run: uv run python -c "import pytest; print('Dependencies ready')"

  lint:
    name: Backend Lint (Ruff) - Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    needs: [detect-changes, build-backend-deps]
    if: |
      always() &&
      (needs.detect-changes.outputs.should-run-all == 'true' ||
       needs.detect-changes.outputs.backend == 'true') &&
      needs.build-backend-deps.result != 'failure'
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.11']
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Ruff check
        run: uv run ruff check backend/

      - name: Ruff format check
        run: uv run ruff format --check backend/

      - name: Complexity check (Radon)
        run: |
          uv run radon cc backend/ -a -nc
          uv run radon mi backend/ -nc

  typecheck:
    name: Backend Type Check (Mypy) - Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    needs: [detect-changes, build-backend-deps]
    if: |
      always() &&
      (needs.detect-changes.outputs.should-run-all == 'true' ||
       needs.detect-changes.outputs.backend == 'true') &&
      needs.build-backend-deps.result != 'failure'
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.11']
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Mypy
        run: uv run mypy backend/ --ignore-missing-imports

  # Backend unit tests split into 4 shards for faster execution
  unit-tests:
    name: Backend Unit Tests (${{ matrix.shard }}/4) - Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.11']
        shard: [1, 2, 3, 4]
    # Wait for lint to pass first - fail fast on code quality issues
    # This saves CI minutes by not running expensive tests when lint fails
    # Skip if only frontend/docs changed (checked via detect-changes)
    needs: [detect-changes, lint, build-backend-deps]
    if: |
      always() &&
      (needs.detect-changes.outputs.should-run-all == 'true' ||
       needs.detect-changes.outputs.backend == 'true') &&
      needs.lint.result != 'failure' &&
      needs.build-backend-deps.result != 'failure'
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres # pragma: allowlist secret
          POSTGRES_DB: security_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      TEST_DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      LOG_DB_ENABLED: 'false'
      ENVIRONMENT: development # Avoid production password validation in CI
      FLAKY_TEST_RESULTS_FILE: flaky-test-tracking-unit-shard-${{ matrix.shard }}.jsonl
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Run unit tests with coverage (shard ${{ matrix.shard }}/4)
        # Unit tests coverage threshold - 85% for unit tests alone
        # Combined coverage (unit + integration) should reach 95%
        # -n auto enables xdist parallelization with worksteal scheduler within shard
        # --timeout=0 disables timeout (fixture import takes >1s in CI)
        # --cov-fail-under=0 disables threshold check here (checked after combining coverage)
        # pytest-split groups tests by --group for cross-runner sharding
        run: |
          uv run pytest backend/tests/unit/ \
            -n auto --dist=worksteal \
            --splits 4 --group ${{ matrix.shard }} \
            --timeout=0 \
            --cov=backend \
            --cov-config=pyproject.toml \
            --cov-fail-under=0 \
            --cov-report=term-missing \
            --cov-report=xml:coverage-unit-shard-${{ matrix.shard }}.xml \
            --junit-xml=test-results-unit-shard-${{ matrix.shard }}.xml \
            --durations=20 \
            -v

      - name: Upload unit test results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: test-results-unit-shard-${{ matrix.shard }}-py${{ matrix.python-version }}
          path: |
            test-results-unit-shard-${{ matrix.shard }}.xml
            flaky-test-tracking-unit-shard-${{ matrix.shard }}.jsonl
          retention-days: 7

      - name: Upload unit test coverage
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: coverage-unit-shard-${{ matrix.shard }}-py${{ matrix.python-version }}
          path: coverage-unit-shard-${{ matrix.shard }}.xml
          retention-days: 7

  # Merge coverage from all unit test shards and verify threshold
  unit-tests-coverage-merge:
    name: Backend Unit Tests Coverage
    runs-on: ubuntu-latest
    needs: [detect-changes, unit-tests]
    if: |
      always() &&
      (needs.detect-changes.outputs.should-run-all == 'true' ||
       needs.detect-changes.outputs.backend == 'true')
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Download all coverage artifacts
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7
        with:
          pattern: coverage-unit-shard-*
          path: coverage-reports/
          merge-multiple: true

      - name: List coverage files
        run: find coverage-reports/ -name "*.xml" | head -20

      - name: Combine and check coverage threshold
        run: |
          # List available coverage files
          echo "Available coverage files:"
          find . -name ".coverage*" -type f || true

          # Only run combine if files exist
          if compgen -G ".coverage.*" > /dev/null 2>&1; then
            uv run coverage combine --data-file=.coverage
            uv run coverage xml -o coverage-unit.xml --rcfile=/dev/null
            echo "Coverage merged successfully"
          else
            echo "::warning::No coverage files found to merge"
            # Create empty coverage files to prevent downstream failures
            touch .coverage
            touch coverage-unit.xml
          fi

      - name: Upload merged unit test coverage
        uses: codecov/codecov-action@671740ac38dd9b0130fbe1cec585b89eea48d3de # v5
        with:
          files: coverage-unit.xml
          flags: backend-unit
          fail_ci_if_error: false

  # Summary job for branch protection - aggregates all unit test shards
  unit-tests-summary:
    name: Backend Unit Tests
    runs-on: ubuntu-latest
    needs: [detect-changes, unit-tests]
    if: always()
    steps:
      - name: Check unit test shard results
        run: |
          # Skip check if tests were skipped due to no backend changes
          if [ "${{ needs.detect-changes.outputs.should-run-all }}" != "true" ] && \
             [ "${{ needs.detect-changes.outputs.backend }}" != "true" ]; then
            echo "Backend tests skipped - no backend changes detected"
            echo "## Backend Unit Tests" >> $GITHUB_STEP_SUMMARY
            echo "Skipped - no backend changes detected" >> $GITHUB_STEP_SUMMARY
            exit 0
          fi

          if [ "${{ needs.unit-tests.result }}" == "failure" ]; then
            echo "One or more unit test shards failed"
            exit 1
          fi
          echo "All unit test shards passed"

  # Integration tests split into 4 domain-based parallel jobs for faster execution
  # Each job runs tests for a specific domain with within-shard parallelization
  integration-tests-api:
    name: Integration Tests (API) - Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    timeout-minutes: 20
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.11']
    # Run in parallel with lint - dependencies checked in integration-tests-summary
    # Skip if only frontend/docs changed (checked via detect-changes)
    needs: [detect-changes, build-backend-deps]
    if: |
      always() &&
      (needs.detect-changes.outputs.should-run-all == 'true' ||
       needs.detect-changes.outputs.backend == 'true') &&
      needs.build-backend-deps.result != 'failure'
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres # pragma: allowlist secret
          POSTGRES_DB: security_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      TEST_DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      REDIS_URL: redis://localhost:6379
      REDIS_HOST: localhost
      REDIS_PORT: 6379
      LOG_DB_ENABLED: 'false'
      ENVIRONMENT: development # Avoid production password validation in CI
      CI: 'true'
      FLAKY_TEST_RESULTS_FILE: flaky-test-tracking-integration-api.jsonl
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Run API integration tests with retry
        # API tests: test_*_api.py files
        # -n auto enables parallel execution with database-per-worker isolation
        # Each pytest-xdist worker gets its own database copy via template_database fixture
        # Retry loop: 3 attempts with 10s wait between retries for transient failures
        run: |
          START_TIME=$(date +%s)
          for attempt in 1 2 3; do
            echo "Attempt $attempt of 3"
            echo "::group::Test Execution (Attempt $attempt)"
            if uv run pytest backend/tests/integration/ \
              -k "test_admin_api or test_ai_audit_api or test_alerts_api or test_api_error_scenarios or test_api_errors or test_audit_api or test_cameras_api or test_detections_api or test_dlq_api or test_entities_api or test_events_api or test_http_error_codes or test_logs_api or test_media_api or test_media_security or test_metrics_api or test_notification_api or test_search_api or test_system_api or test_video_streaming or test_zones_api or test_api" \
              -n auto \
              --timeout=30 \
              --cov=backend \
              --cov-fail-under=0 \
              --cov-report=xml:coverage-integration-api.xml \
              --cov-report=term-missing \
              --junit-xml=test-results-integration-api.xml \
              --durations=20 \
              -v 2>&1 | tee test-output.log; then
              echo "::endgroup::"
              END_TIME=$(date +%s)
              DURATION=$((END_TIME - START_TIME))
              WORKERS=$(grep -oP 'gw\d+' test-output.log | sort -u | wc -l || echo "unknown")
              echo "::notice::API Integration Tests completed in ${DURATION}s using ${WORKERS} workers"
              exit 0
            fi
            echo "::endgroup::"
            if [ $attempt -lt 3 ]; then
              echo "Test failed, retrying in 10s..."
              sleep 10
            fi
          done
          exit 1

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: test-results-integration-api-py${{ matrix.python-version }}
          path: |
            test-results-integration-api.xml
            flaky-test-tracking-integration-api.jsonl
          retention-days: 7

      - name: Upload coverage artifact
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: coverage-integration-api-py${{ matrix.python-version }}
          path: coverage-integration-api.xml
          retention-days: 7

  integration-tests-websocket:
    name: Integration Tests (WebSocket) - Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    timeout-minutes: 20
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.11']
    # Run in parallel with lint - dependencies checked in integration-tests-summary
    # Skip if only frontend/docs changed (checked via detect-changes)
    needs: [detect-changes, build-backend-deps]
    if: |
      always() &&
      (needs.detect-changes.outputs.should-run-all == 'true' ||
       needs.detect-changes.outputs.backend == 'true') &&
      needs.build-backend-deps.result != 'failure'
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres # pragma: allowlist secret
          POSTGRES_DB: security_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      TEST_DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      REDIS_URL: redis://localhost:6379
      REDIS_HOST: localhost
      REDIS_PORT: 6379
      LOG_DB_ENABLED: 'false'
      ENVIRONMENT: development # Avoid production password validation in CI
      CI: 'true'
      FLAKY_TEST_RESULTS_FILE: flaky-test-tracking-integration-websocket.jsonl
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Run WebSocket integration tests with retry
        # WebSocket tests: test_websocket*.py, test_broadcast*.py, test_pubsub*.py
        # -n0 runs tests serially to avoid database deadlocks from concurrent schema creation
        # Retry loop: 3 attempts with 10s wait between retries for transient failures
        run: |
          for attempt in 1 2 3; do
            echo "Attempt $attempt of 3"
            if uv run pytest backend/tests/integration/ \
              -k "test_websocket or test_system_broadcaster or test_redis_pubsub" \
              -n0 \
              --timeout=30 \
              --cov=backend \
              --cov-fail-under=0 \
              --cov-report=xml:coverage-integration-websocket.xml \
              --cov-report=term-missing \
              --junit-xml=test-results-integration-websocket.xml \
              --durations=20 \
              -v; then
              exit 0
            fi
            if [ $attempt -lt 3 ]; then
              echo "Test failed, retrying in 10s..."
              sleep 10
            fi
          done
          exit 1

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: test-results-integration-websocket-py${{ matrix.python-version }}
          path: |
            test-results-integration-websocket.xml
            flaky-test-tracking-integration-websocket.jsonl
          retention-days: 7

      - name: Upload coverage artifact
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: coverage-integration-websocket-py${{ matrix.python-version }}
          path: coverage-integration-websocket.xml
          retention-days: 7

  integration-tests-services:
    name: Integration Tests (Services) - Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    timeout-minutes: 20
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.11']
    # Run in parallel with lint - dependencies checked in integration-tests-summary
    # Skip if only frontend/docs changed (checked via detect-changes)
    needs: [detect-changes, build-backend-deps]
    if: |
      always() &&
      (needs.detect-changes.outputs.should-run-all == 'true' ||
       needs.detect-changes.outputs.backend == 'true') &&
      needs.build-backend-deps.result != 'failure'
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres # pragma: allowlist secret
          POSTGRES_DB: security_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      TEST_DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      REDIS_URL: redis://localhost:6379
      REDIS_HOST: localhost
      REDIS_PORT: 6379
      LOG_DB_ENABLED: 'false'
      ENVIRONMENT: development # Avoid production password validation in CI
      CI: 'true'
      FLAKY_TEST_RESULTS_FILE: flaky-test-tracking-integration-services.jsonl
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Install PostgreSQL client tools
        # Required for backup/restore tests (pg_dump, pg_restore)
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Run Services integration tests with retry
        # Service tests: test_*_integration.py, test_batch*.py, test_detector*.py, test_file_watcher*.py, test_circuit*.py
        # Also includes: test_backup_restore, test_data_corruption for database operations
        # -n0 runs tests serially to avoid database deadlocks from concurrent schema creation
        # Retry loop: 3 attempts with 10s wait between retries for transient failures
        run: |
          for attempt in 1 2 3; do
            echo "Attempt $attempt of 3"
            if uv run pytest backend/tests/integration/ \
              -k "test_batch_aggregator_integration or test_detector_client_integration or test_file_watcher_filesystem or test_file_watcher_integration or test_circuit_breaker or test_cleanup_service or test_dlq_retry_handler_integration or test_health_monitor_integration or test_nemotron_analyzer_integration or test_nemotron_analyzer or test_vision_extraction_pipeline or test_pipeline_e2e or test_full_stack or test_audit or test_event_search or test_github_workflows or test_alembic_migrations or test_transaction_rollback or test_backup_restore or test_data_corruption" \
              -n0 \
              --timeout=30 \
              --cov=backend \
              --cov-fail-under=0 \
              --cov-report=xml:coverage-integration-services.xml \
              --cov-report=term-missing \
              --junit-xml=test-results-integration-services.xml \
              --durations=20 \
              -v; then
              exit 0
            fi
            if [ $attempt -lt 3 ]; then
              echo "Test failed, retrying in 10s..."
              sleep 10
            fi
          done
          exit 1

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: test-results-integration-services-py${{ matrix.python-version }}
          path: |
            test-results-integration-services.xml
            flaky-test-tracking-integration-services.jsonl
          retention-days: 7

      - name: Upload coverage artifact
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: coverage-integration-services-py${{ matrix.python-version }}
          path: coverage-integration-services.xml
          retention-days: 7

  integration-tests-models:
    name: Integration Tests (Models) - Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    timeout-minutes: 20
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.11']
    # Run in parallel with lint - dependencies checked in integration-tests-summary
    # Skip if only frontend/docs changed (checked via detect-changes)
    needs: [detect-changes, build-backend-deps]
    if: |
      always() &&
      (needs.detect-changes.outputs.should-run-all == 'true' ||
       needs.detect-changes.outputs.backend == 'true') &&
      needs.build-backend-deps.result != 'failure'
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres # pragma: allowlist secret
          POSTGRES_DB: security_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      TEST_DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      REDIS_URL: redis://localhost:6379
      REDIS_HOST: localhost
      REDIS_PORT: 6379
      LOG_DB_ENABLED: 'false'
      ENVIRONMENT: development # Avoid production password validation in CI
      CI: 'true'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Run Models integration tests with retry
        # Model tests: test_models*.py, test_database*.py, test_baseline*.py, test_alert_*.py, test_enrichment*.py, test_cache*.py
        # -n0 runs tests serially to avoid database deadlocks from concurrent schema creation
        # Retry loop: 3 attempts with 10s wait between retries for transient failures
        run: |
          for attempt in 1 2 3; do
            echo "Attempt $attempt of 3"
            if uv run pytest backend/tests/integration/ \
              -k "test_models or test_model_cascades or test_database or test_baseline or test_alert_dedup or test_alert_engine or test_alert_models or test_enrichment_pipeline or test_cache_service_integration" \
              -n0 \
              --timeout=30 \
              --cov=backend \
              --cov-fail-under=0 \
              --cov-report=xml:coverage-integration-models.xml \
              --cov-report=term-missing \
              --junit-xml=test-results-integration-models.xml \
              --durations=20 \
              -v; then
              exit 0
            fi
            if [ $attempt -lt 3 ]; then
              echo "Test failed, retrying in 10s..."
              sleep 10
            fi
          done
          exit 1

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: test-results-integration-models-py${{ matrix.python-version }}
          path: test-results-integration-models.xml
          retention-days: 7

      - name: Upload coverage artifact
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: coverage-integration-models-py${{ matrix.python-version }}
          path: coverage-integration-models.xml
          retention-days: 7

  # Merge coverage from all integration test shards
  integration-coverage-merge:
    name: Merge Integration Coverage
    runs-on: ubuntu-latest
    needs:
      [
        detect-changes,
        integration-tests-api,
        integration-tests-websocket,
        integration-tests-services,
        integration-tests-models,
      ]
    if: |
      always() &&
      (needs.detect-changes.outputs.should-run-all == 'true' ||
       needs.detect-changes.outputs.backend == 'true')
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Download all coverage artifacts
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7
        with:
          pattern: coverage-integration-*
          path: coverage-reports/
          merge-multiple: true

      - name: List coverage files
        run: find coverage-reports/ -name "*.xml" | head -20

      - name: Upload merged coverage to Codecov
        uses: codecov/codecov-action@671740ac38dd9b0130fbe1cec585b89eea48d3de # v5
        with:
          directory: coverage-reports/
          flags: backend-integration
          fail_ci_if_error: false

  # Summary job for branch protection - aggregates all integration test shards
  integration-tests-summary:
    name: Backend Integration Tests
    runs-on: ubuntu-latest
    needs:
      [
        detect-changes,
        lint,
        integration-tests-api,
        integration-tests-websocket,
        integration-tests-services,
        integration-tests-models,
      ]
    if: always()
    steps:
      - name: Check integration test results
        run: |
          # Skip check if tests were skipped due to no backend changes
          if [ "${{ needs.detect-changes.outputs.should-run-all }}" != "true" ] && \
             [ "${{ needs.detect-changes.outputs.backend }}" != "true" ]; then
            echo "Integration tests skipped - no backend changes detected"
            echo "## Backend Integration Tests" >> $GITHUB_STEP_SUMMARY
            echo "Skipped - no backend changes detected" >> $GITHUB_STEP_SUMMARY
            exit 0
          fi

          if [ "${{ needs.integration-tests-api.result }}" == "failure" ] || \
             [ "${{ needs.integration-tests-websocket.result }}" == "failure" ] || \
             [ "${{ needs.integration-tests-services.result }}" == "failure" ] || \
             [ "${{ needs.integration-tests-models.result }}" == "failure" ]; then
            echo "One or more integration test jobs failed"
            exit 1
          fi
          echo "All integration tests passed"

  # Contract tests for API schema validation
  # Main-only: redundant with api-types-check, runs post-merge for validation
  contract-tests:
    name: Contract Tests (API Schema)
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: github.ref == 'refs/heads/main'
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres # pragma: allowlist secret
          POSTGRES_DB: security_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Run contract tests
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
          REDIS_URL: redis://localhost:6379/15
          REDIS_HOST: localhost
          REDIS_PORT: 6379
          LOG_DB_ENABLED: 'false'
          ENVIRONMENT: development # Avoid production password validation in CI
        run: |
          uv run pytest backend/tests/contracts/ \
            -n0 \
            --timeout=30 \
            -v \
            --junit-xml=test-results-contracts.xml \
            --durations=10

      - name: Upload contract test results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: test-results-contracts
          path: test-results-contracts.xml
          retention-days: 7

      - name: Create Linear issue on failure
        if: failure()
        env:
          LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}
        run: |
          TITLE="[CI] Contract Tests failed on main"

          # Check for existing open issue with same title
          EXISTING=$(curl -s -X POST https://api.linear.app/graphql \
            -H "Content-Type: application/json" \
            -H "Authorization: $LINEAR_API_KEY" \
            -d '{
              "query": "query SearchIssues($filter: IssueFilter) { issues(filter: $filter) { nodes { id identifier } } }",
              "variables": {
                "filter": {
                  "team": { "key": { "eq": "NEM" } },
                  "title": { "eq": "'"$TITLE"'" },
                  "state": { "type": { "nin": ["completed", "canceled"] } }
                }
              }
            }')

          EXISTING_ID=$(echo "$EXISTING" | jq -r '.data.issues.nodes[0].id // empty')

          if [ -n "$EXISTING_ID" ]; then
            # Add comment to existing issue
            COMMENT="**Another failure detected**\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            curl -s -X POST https://api.linear.app/graphql \
              -H "Content-Type: application/json" \
              -H "Authorization: $LINEAR_API_KEY" \
              -d '{
                "query": "mutation CreateComment($input: CommentCreateInput!) { commentCreate(input: $input) { success } }",
                "variables": {
                  "input": {
                    "issueId": "'"$EXISTING_ID"'",
                    "body": "'"$COMMENT"'"
                  }
                }
              }'
            echo "Added comment to existing issue"
          else
            # Create new issue
            curl -s -X POST https://api.linear.app/graphql \
              -H "Content-Type: application/json" \
              -H "Authorization: $LINEAR_API_KEY" \
              -d '{
                "query": "mutation CreateIssue($input: IssueCreateInput!) { issueCreate(input: $input) { success issue { identifier url } } }",
                "variables": {
                  "input": {
                    "teamId": "998946a2-aa75-491b-a39d-189660131392",
                    "title": "'"$TITLE"'",
                    "description": "Contract tests failed after merge to main.\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\nPlease investigate and fix.",
                    "priority": 2
                  }
                }
              }'
            echo "Created new issue"
          fi

  # Main-only: dead code detection runs post-merge for code hygiene
  dead-code:
    name: Dead Code Detection
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Run Vulture (dead code detection)
        run: uv run vulture backend/ vulture_whitelist.py --config pyproject.toml

      - name: Create Linear issue on failure
        if: failure()
        env:
          LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}
        run: |
          TITLE="[CI] Dead Code Detection failed on main"

          # Check for existing open issue with same title
          EXISTING=$(curl -s -X POST https://api.linear.app/graphql \
            -H "Content-Type: application/json" \
            -H "Authorization: $LINEAR_API_KEY" \
            -d '{
              "query": "query SearchIssues($filter: IssueFilter) { issues(filter: $filter) { nodes { id identifier } } }",
              "variables": {
                "filter": {
                  "team": { "key": { "eq": "NEM" } },
                  "title": { "eq": "'"$TITLE"'" },
                  "state": { "type": { "nin": ["completed", "canceled"] } }
                }
              }
            }')

          EXISTING_ID=$(echo "$EXISTING" | jq -r '.data.issues.nodes[0].id // empty')

          if [ -n "$EXISTING_ID" ]; then
            # Add comment to existing issue
            COMMENT="**Another failure detected**\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            curl -s -X POST https://api.linear.app/graphql \
              -H "Content-Type: application/json" \
              -H "Authorization: $LINEAR_API_KEY" \
              -d '{
                "query": "mutation CreateComment($input: CommentCreateInput!) { commentCreate(input: $input) { success } }",
                "variables": {
                  "input": {
                    "issueId": "'"$EXISTING_ID"'",
                    "body": "'"$COMMENT"'"
                  }
                }
              }'
            echo "Added comment to existing issue"
          else
            # Create new issue
            curl -s -X POST https://api.linear.app/graphql \
              -H "Content-Type: application/json" \
              -H "Authorization: $LINEAR_API_KEY" \
              -d '{
                "query": "mutation CreateIssue($input: IssueCreateInput!) { issueCreate(input: $input) { success issue { identifier url } } }",
                "variables": {
                  "input": {
                    "teamId": "998946a2-aa75-491b-a39d-189660131392",
                    "title": "'"$TITLE"'",
                    "description": "Dead code detection (Vulture) found issues after merge to main.\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\nPlease investigate and remove dead code.",
                    "priority": 3
                  }
                }
              }'
            echo "Created new issue"
          fi

  # Frontend Jobs

  api-types-check:
    name: API Types Contract Check
    runs-on: ubuntu-latest
    needs: [detect-changes]
    # Run when backend or frontend changes (contract spans both)
    if: |
      needs.detect-changes.outputs.should-run-all == 'true' ||
      needs.detect-changes.outputs.backend == 'true' ||
      needs.detect-changes.outputs.frontend == 'true'
    env:
      DATABASE_URL: postgresql://user:pass@localhost/db # pragma: allowlist secret
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install backend dependencies
        run: uv sync --extra dev --frozen

      - name: Set up Node.js
        uses: actions/setup-node@6044e13b5dc448c55e2357c09f80417699197238 # v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install frontend dependencies
        run: cd frontend && npm ci

      - name: Generate types from OpenAPI and check for diffs
        run: ./scripts/generate-types.sh --check

      - name: Check WebSocket types are current
        run: uv run python scripts/generate-ws-types.py --check

      - name: Check OpenAPI spec is current
        run: uv run python scripts/generate-openapi.py --check

      - name: Validate API type contract consistency
        run: ./scripts/validate-api-types.sh --verbose

  frontend-lint:
    name: Frontend Lint (ESLint)
    runs-on: ubuntu-latest
    needs: [detect-changes]
    if: |
      needs.detect-changes.outputs.should-run-all == 'true' ||
      needs.detect-changes.outputs.frontend == 'true'
    env:
      NODE_VERSION: '20'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up Node.js
        uses: actions/setup-node@6044e13b5dc448c55e2357c09f80417699197238 # v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: cd frontend && npm ci

      - name: ESLint
        run: cd frontend && npm run lint

      - name: Dead code detection (Knip)
        # Non-blocking initially - reports issues without failing CI
        continue-on-error: true
        run: cd frontend && npx knip

  frontend-typecheck:
    name: Frontend Type Check (TypeScript)
    runs-on: ubuntu-latest
    needs: [detect-changes]
    if: |
      needs.detect-changes.outputs.should-run-all == 'true' ||
      needs.detect-changes.outputs.frontend == 'true'
    env:
      NODE_VERSION: '20'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up Node.js
        uses: actions/setup-node@6044e13b5dc448c55e2357c09f80417699197238 # v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: cd frontend && npm ci

      - name: TypeScript check
        run: cd frontend && npm run typecheck

  frontend-tests:
    name: Frontend Tests (Vitest ${{ matrix.shard }}/16)
    runs-on: ubuntu-latest
    timeout-minutes: 10 # Increased from 5 to accommodate cleanup overhead
    # Wait for lint to pass first - fail fast on code quality issues
    # Skip if only backend/docs changed (checked via detect-changes)
    needs: [detect-changes, frontend-lint]
    if: |
      always() &&
      (needs.detect-changes.outputs.should-run-all == 'true' ||
       needs.detect-changes.outputs.frontend == 'true') &&
      needs.frontend-lint.result != 'failure'
    strategy:
      fail-fast: false
      matrix:
        # Reduced from 24 to 16 shards - better balance of parallelism vs overhead
        # Each shard runs ~40-50 tests with acceptable memory usage
        shard: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
    env:
      NODE_VERSION: '20'
      # 6GB heap with GC exposure - needed for Vitest itself
      # GitHub Actions runner has 7GB RAM
      NODE_OPTIONS: '--max-old-space-size=6144 --expose-gc'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up Node.js
        uses: actions/setup-node@6044e13b5dc448c55e2357c09f80417699197238 # v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Restore npm cache with fallback
        uses: actions/cache@9255dc7a253b0ccc959486e2bca901246202afeb # v5
        with:
          path: |
            ~/.npm
            node_modules
          key: npm-${{ runner.os }}-node-${{ env.NODE_VERSION }}-${{ hashFiles('frontend/package-lock.json') }}
          restore-keys: |
            npm-${{ runner.os }}-node-${{ env.NODE_VERSION }}-
            npm-${{ runner.os }}-

      - name: Install dependencies with retry
        # npm ci can fail due to network issues, add retry logic for resilience
        run: |
          for attempt in 1 2 3; do
            echo "Attempt $attempt of 3: Installing npm dependencies..."
            if cd frontend && npm ci; then
              echo "npm dependencies installed successfully"
              exit 0
            fi
            if [ $attempt -lt 3 ]; then
              echo "Installation failed, retrying in 10s..."
              sleep 10
            fi
          done
          echo "ERROR: Failed to install npm dependencies after 3 attempts"
          exit 1

      - name: Run tests (shard ${{ matrix.shard }}/16)
        # Shard tests across 16 runners for better memory distribution
        # Coverage collected per-shard and merged in frontend-coverage-merge job
        # NOTE: Do NOT add --coverage here - it triggers per-shard threshold checks
        # that fail because each shard only has ~6% of total coverage (with 16 shards)
        # Tests complete in ~20s but cleanup takes 5+ minutes and OOMs.
        # We use a fifo to monitor output and kill after test summary is printed.
        run: |
          cd frontend

          # Create a temp file for output
          tmpfile=$(mktemp)
          trap "rm -f $tmpfile" EXIT

          # Start vitest in background
          npx vitest run --shard=${{ matrix.shard }}/16 --reporter=verbose --teardownTimeout=1000 2>&1 | tee "$tmpfile" &
          vitest_pid=$!

          # Monitor for test completion (look for "passed" in output)
          # Kill the process 5 seconds after tests complete to avoid OOM in cleanup
          while kill -0 $vitest_pid 2>/dev/null; do
            if grep -qE "passed.*\([0-9]+\)" "$tmpfile" 2>/dev/null; then
              echo ""
              echo "Tests completed, waiting 5 seconds then killing cleanup..."
              sleep 5
              # Force kill vitest and all child processes
              pkill -9 -P $vitest_pid 2>/dev/null || true
              kill -9 $vitest_pid 2>/dev/null || true
              echo "Cleanup killed to prevent OOM"
              exit 0
            fi
            sleep 1
          done

          # If we get here, vitest exited on its own
          wait $vitest_pid
          exit $?

      - name: Upload coverage artifact
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: frontend-coverage-shard-${{ matrix.shard }}-of-16-node-${{ env.NODE_VERSION }}
          path: frontend/coverage/
          retention-days: 7

  # Merge frontend coverage from all shards and upload to Codecov
  frontend-coverage-merge:
    name: Merge Frontend Coverage
    runs-on: ubuntu-latest
    needs: [detect-changes, frontend-tests]
    if: |
      always() &&
      (needs.detect-changes.outputs.should-run-all == 'true' ||
       needs.detect-changes.outputs.frontend == 'true')
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Download all coverage artifacts
        # Downloads artifacts from all Node version + shard combinations
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7
        with:
          pattern: frontend-coverage-shard-*-node-*
          path: coverage-reports/
          merge-multiple: true

      - name: List coverage files
        run: find coverage-reports/ -name "*.json" -o -name "*.xml" | head -20

      - name: Upload frontend coverage to Codecov
        uses: codecov/codecov-action@671740ac38dd9b0130fbe1cec585b89eea48d3de # v5
        with:
          directory: coverage-reports/
          flags: frontend
          fail_ci_if_error: false

  # Summary job for branch protection - aggregates all Vitest shards
  frontend-tests-summary:
    name: Frontend Tests (Vitest)
    runs-on: ubuntu-latest
    needs: [detect-changes, frontend-tests]
    if: always()
    steps:
      - name: Check shard results
        run: |
          # Skip check if tests were skipped due to no frontend changes
          if [ "${{ needs.detect-changes.outputs.should-run-all }}" != "true" ] && \
             [ "${{ needs.detect-changes.outputs.frontend }}" != "true" ]; then
            echo "Frontend tests skipped - no frontend changes detected"
            echo "## Frontend Tests (Vitest)" >> $GITHUB_STEP_SUMMARY
            echo "Skipped - no frontend changes detected" >> $GITHUB_STEP_SUMMARY
            exit 0
          fi

          if [ "${{ needs.frontend-tests.result }}" == "failure" ]; then
            echo "One or more Vitest shards failed"
            exit 1
          fi
          echo "All Vitest shards passed"

  api-coverage:
    name: API Endpoint Coverage
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Check API coverage
        run: ./scripts/check-api-coverage.sh

  # Primary E2E tests (Chromium) - required for merge
  # Uses sharding to split tests across 6 parallel runners for 6x speed
  frontend-e2e:
    name: E2E Tests (chromium ${{ matrix.shard }})
    runs-on: ubuntu-latest
    # Wait for lint to pass first - fail fast on code quality issues
    # Skip if only backend/docs changed (checked via detect-changes)
    needs: [detect-changes, frontend-lint]
    if: |
      always() &&
      (needs.detect-changes.outputs.should-run-all == 'true' ||
       needs.detect-changes.outputs.frontend == 'true') &&
      needs.frontend-lint.result != 'failure'
    strategy:
      fail-fast: false
      matrix:
        shard: [1/6, 2/6, 3/6, 4/6, 5/6, 6/6]
    env:
      NODE_VERSION: '20'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up Node.js
        uses: actions/setup-node@6044e13b5dc448c55e2357c09f80417699197238 # v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: cd frontend && npm ci

      - name: Cache Playwright browsers
        uses: actions/cache@9255dc7a253b0ccc959486e2bca901246202afeb # v5
        id: playwright-cache
        with:
          path: ~/.cache/ms-playwright
          key: playwright-chromium-${{ runner.os }}-node-${{ env.NODE_VERSION }}-${{ hashFiles('frontend/package-lock.json') }}

      - name: Install Playwright Chromium with retry
        # Retry loop for browser installation (3 attempts with exponential backoff)
        # Handles transient network failures and concurrent installation conflicts
        run: |
          for attempt in 1 2 3; do
            echo "Attempt $attempt of 3: Installing Playwright Chromium..."
            # Clean up any partial installation on retry
            if [ $attempt -gt 1 ]; then
              echo "Cleaning up partial installation before retry..."
              rm -rf ~/.cache/ms-playwright || true
            fi
            if cd frontend && npx playwright install chromium --with-deps 2>&1 | tee /tmp/playwright-install.log; then
              echo "Playwright Chromium installed successfully"
              exit 0
            fi
            if [ $attempt -lt 3 ]; then
              wait_time=$((10 * attempt))
              echo "Installation failed, waiting ${wait_time}s before retry..."
              sleep $wait_time
            fi
          done
          echo "ERROR: Failed to install Playwright Chromium after 3 attempts"
          cat /tmp/playwright-install.log || true
          exit 1

      - name: Verify Playwright Chromium installation
        run: |
          if npx playwright --version; then
            echo "Playwright CLI verified"
          fi
          if [ -d ~/.cache/ms-playwright ]; then
            echo "Playwright cache directory exists:"
            ls -lah ~/.cache/ms-playwright/chromium-*/
          else
            echo "ERROR: Playwright cache directory not found"
            exit 1
          fi

      - name: Install Playwright system dependencies (if cached)
        # Only needed if browsers were already cached
        if: steps.playwright-cache.outputs.cache-hit == 'true'
        run: cd frontend && npx playwright install-deps chromium

      - name: Run E2E tests (Chromium shard ${{ matrix.shard }}) with retry
        # Retry loop: 3 attempts with 10s wait between retries for transient failures
        # Browser installation issues are caught by separate installation step
        # Uses subshell () to ensure cd doesn't affect subsequent retry iterations
        run: |
          for attempt in 1 2 3; do
            echo "Attempt $attempt of 3"
            if (cd frontend && npx playwright test --project=chromium --shard=${{ matrix.shard }}); then
              exit 0
            fi
            if [ $attempt -lt 3 ]; then
              echo "Test failed, retrying in 10s..."
              sleep 10
            fi
          done
          exit 1

      - name: Upload Playwright report
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: playwright-report-chromium-${{ strategy.job-index }}-node-${{ env.NODE_VERSION }}
          path: frontend/playwright-report/
          retention-days: 7

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: playwright-test-results-chromium-${{ strategy.job-index }}-node-${{ env.NODE_VERSION }}
          path: frontend/test-results/
          retention-days: 7

  # Summary job for branch protection - aggregates all E2E shards
  frontend-e2e-summary:
    name: E2E Tests (Chromium)
    runs-on: ubuntu-latest
    needs: [detect-changes, frontend-e2e]
    if: always()
    steps:
      - name: Check shard results
        run: |
          # Skip check if tests were skipped due to no frontend changes
          if [ "${{ needs.detect-changes.outputs.should-run-all }}" != "true" ] && \
             [ "${{ needs.detect-changes.outputs.frontend }}" != "true" ]; then
            echo "E2E tests skipped - no frontend changes detected"
            echo "## E2E Tests (Chromium)" >> $GITHUB_STEP_SUMMARY
            echo "Skipped - no frontend changes detected" >> $GITHUB_STEP_SUMMARY
            exit 0
          fi

          if [ "${{ needs.frontend-e2e.result }}" == "failure" ]; then
            echo "One or more E2E shards failed"
            exit 1
          fi
          echo "All E2E shards passed"

  # Secondary E2E tests (Firefox/WebKit) - nightly schedule only to improve PR merge velocity
  # Intentionally kept on Node 20 only (no matrix) to avoid excessive CI time
  # Nightly-only: Firefox/WebKit have known infrastructure flakiness (timeouts)
  # Move to nightly - too slow for every main push (20+ minutes)
  frontend-e2e-secondary:
    name: E2E Tests (${{ matrix.browser }})
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    strategy:
      fail-fast: false
      matrix:
        browser: [firefox, webkit]
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up Node.js
        uses: actions/setup-node@6044e13b5dc448c55e2357c09f80417699197238 # v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: cd frontend && npm ci

      - name: Cache Playwright browsers
        uses: actions/cache@9255dc7a253b0ccc959486e2bca901246202afeb # v5
        id: playwright-cache
        with:
          path: ~/.cache/ms-playwright
          key: playwright-${{ matrix.browser }}-${{ runner.os }}-${{ hashFiles('frontend/package-lock.json') }}

      - name: Install Playwright ${{ matrix.browser }} with retry
        # Retry loop for browser installation (3 attempts with exponential backoff)
        # Firefox/WebKit tend to have more infrastructure flakiness than Chromium
        run: |
          for attempt in 1 2 3; do
            echo "Attempt $attempt of 3: Installing Playwright ${{ matrix.browser }}..."
            # Clean up any partial installation on retry
            if [ $attempt -gt 1 ]; then
              echo "Cleaning up partial installation before retry..."
              rm -rf ~/.cache/ms-playwright || true
            fi
            if cd frontend && npx playwright install ${{ matrix.browser }} --with-deps 2>&1 | tee /tmp/playwright-install.log; then
              echo "Playwright ${{ matrix.browser }} installed successfully"
              exit 0
            fi
            if [ $attempt -lt 3 ]; then
              wait_time=$((10 * attempt))
              echo "Installation failed, waiting ${wait_time}s before retry..."
              sleep $wait_time
            fi
          done
          echo "ERROR: Failed to install Playwright ${{ matrix.browser }} after 3 attempts"
          cat /tmp/playwright-install.log || true
          exit 1

      - name: Verify Playwright ${{ matrix.browser }} installation
        run: |
          if npx playwright --version; then
            echo "Playwright CLI verified"
          fi
          if [ -d ~/.cache/ms-playwright ]; then
            echo "Playwright cache directory exists:"
            ls -lah ~/.cache/ms-playwright/ | grep -E "${{ matrix.browser }}|chromium"
          else
            echo "ERROR: Playwright cache directory not found"
            exit 1
          fi

      - name: Install Playwright system dependencies (if cached)
        # Only needed if browsers were already cached
        if: steps.playwright-cache.outputs.cache-hit == 'true'
        run: cd frontend && npx playwright install-deps ${{ matrix.browser }}

      - name: Run E2E tests (${{ matrix.browser }}) with retry
        # Retry loop: 3 attempts with 10s wait between retries for transient test failures
        # Browser installation issues are caught by separate installation step
        # Uses subshell () to ensure cd doesn't affect subsequent retry iterations
        run: |
          for attempt in 1 2 3; do
            echo "Attempt $attempt of 3"
            if (cd frontend && npx playwright test --project=${{ matrix.browser }}); then
              exit 0
            fi
            if [ $attempt -lt 3 ]; then
              wait_time=$((10 * attempt))
              echo "Test failed, waiting ${wait_time}s before retry..."
              sleep $wait_time
            fi
          done
          exit 1

      - name: Upload Playwright report
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: playwright-report-${{ matrix.browser }}
          path: frontend/playwright-report/
          retention-days: 7

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: playwright-test-results-${{ matrix.browser }}
          path: frontend/test-results/
          retention-days: 7

      - name: Create Linear issue on failure
        if: failure()
        env:
          LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}
        run: |
          TITLE="[CI] E2E Tests (${{ matrix.browser }}) failed on main"

          # Check for existing open issue with same title
          EXISTING=$(curl -s -X POST https://api.linear.app/graphql \
            -H "Content-Type: application/json" \
            -H "Authorization: $LINEAR_API_KEY" \
            -d '{
              "query": "query SearchIssues($filter: IssueFilter) { issues(filter: $filter) { nodes { id identifier } } }",
              "variables": {
                "filter": {
                  "team": { "key": { "eq": "NEM" } },
                  "title": { "eq": "'"$TITLE"'" },
                  "state": { "type": { "nin": ["completed", "canceled"] } }
                }
              }
            }')

          EXISTING_ID=$(echo "$EXISTING" | jq -r '.data.issues.nodes[0].id // empty')

          if [ -n "$EXISTING_ID" ]; then
            # Add comment to existing issue
            COMMENT="**Another failure detected**\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            curl -s -X POST https://api.linear.app/graphql \
              -H "Content-Type: application/json" \
              -H "Authorization: $LINEAR_API_KEY" \
              -d '{
                "query": "mutation CreateComment($input: CommentCreateInput!) { commentCreate(input: $input) { success } }",
                "variables": {
                  "input": {
                    "issueId": "'"$EXISTING_ID"'",
                    "body": "'"$COMMENT"'"
                  }
                }
              }'
            echo "Added comment to existing issue"
          else
            # Create new issue
            curl -s -X POST https://api.linear.app/graphql \
              -H "Content-Type: application/json" \
              -H "Authorization: $LINEAR_API_KEY" \
              -d '{
                "query": "mutation CreateIssue($input: IssueCreateInput!) { issueCreate(input: $input) { success issue { identifier url } } }",
                "variables": {
                  "input": {
                    "teamId": "998946a2-aa75-491b-a39d-189660131392",
                    "title": "'"$TITLE"'",
                    "description": "E2E tests for ${{ matrix.browser }} failed after merge to main.\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\nPlease investigate - may be infrastructure flakiness or real test failure.",
                    "priority": 3
                  }
                }
              }'
            echo "Created new issue"
          fi

  # Build Jobs (main-only - verify Docker builds after merge)
  # Separate jobs for backend/frontend to avoid disk space issues

  build-backend:
    name: Build Docker (backend)
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Free disk space (minimal)
        # Remove only what's needed for Docker builds
        # Minimal cleanup is faster than full jlumbroso/free-disk-space action
        run: |
          # Remove only what's needed for Docker builds
          sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc
          docker system prune -af --volumes
          df -h

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@8d2750c68a42422c14e847fe6c8ac0403b4cbd6f # v3

      - name: Build backend image with retry
        # Retry logic for transient registry 502 errors
        # Docker registry can return 502 during layer blob writes due to infrastructure issues
        uses: nick-fields/retry@ce71cc2ab81d554ebbe88c79ab5975992d79ba08 # v3
        with:
          timeout_minutes: 30
          max_attempts: 3
          retry_wait_seconds: 60
          command: |
            docker buildx build \
              --file ./backend/Dockerfile \
              --target prod \
              --tag backend:${{ github.sha }} \
              --cache-from type=gha,scope=backend \
              --cache-to type=gha,mode=max,scope=backend \
              --load \
              .

      - name: Create Linear issue on failure
        if: failure()
        env:
          LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}
        run: |
          TITLE="[CI] Backend Docker build failed on main"

          # Check for existing open issue with same title
          EXISTING=$(curl -s -X POST https://api.linear.app/graphql \
            -H "Content-Type: application/json" \
            -H "Authorization: $LINEAR_API_KEY" \
            -d '{
              "query": "query SearchIssues($filter: IssueFilter) { issues(filter: $filter) { nodes { id identifier } } }",
              "variables": {
                "filter": {
                  "team": { "key": { "eq": "NEM" } },
                  "title": { "eq": "'"$TITLE"'" },
                  "state": { "type": { "nin": ["completed", "canceled"] } }
                }
              }
            }')

          EXISTING_ID=$(echo "$EXISTING" | jq -r '.data.issues.nodes[0].id // empty')

          if [ -n "$EXISTING_ID" ]; then
            # Add comment to existing issue
            COMMENT="**Another failure detected**\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            curl -s -X POST https://api.linear.app/graphql \
              -H "Content-Type: application/json" \
              -H "Authorization: $LINEAR_API_KEY" \
              -d '{
                "query": "mutation CreateComment($input: CommentCreateInput!) { commentCreate(input: $input) { success } }",
                "variables": {
                  "input": {
                    "issueId": "'"$EXISTING_ID"'",
                    "body": "'"$COMMENT"'"
                  }
                }
              }'
            echo "Added comment to existing issue"
          else
            # Create new issue
            curl -s -X POST https://api.linear.app/graphql \
              -H "Content-Type: application/json" \
              -H "Authorization: $LINEAR_API_KEY" \
              -d '{
                "query": "mutation CreateIssue($input: IssueCreateInput!) { issueCreate(input: $input) { success issue { identifier url } } }",
                "variables": {
                  "input": {
                    "teamId": "998946a2-aa75-491b-a39d-189660131392",
                    "title": "'"$TITLE"'",
                    "description": "Backend Docker build failed after merge to main.\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\nPlease investigate - Docker build must work for deployment.",
                    "priority": 1
                  }
                }
              }'
            echo "Created new issue"
          fi

  build-frontend:
    name: Build Docker (frontend)
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@8d2750c68a42422c14e847fe6c8ac0403b4cbd6f # v3

      - name: Build frontend image with retry
        # Retry logic for transient registry 502 errors
        # Docker registry can return 502 during layer blob writes due to infrastructure issues
        uses: nick-fields/retry@ce71cc2ab81d554ebbe88c79ab5975992d79ba08 # v3
        with:
          timeout_minutes: 20
          max_attempts: 3
          retry_wait_seconds: 60
          command: |
            docker buildx build \
              --file ./frontend/Dockerfile \
              --target prod \
              --tag frontend:${{ github.sha }} \
              --cache-from type=gha,scope=frontend \
              --cache-to type=gha,mode=max,scope=frontend \
              --load \
              ./frontend

      - name: Create Linear issue on failure
        if: failure()
        env:
          LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}
        run: |
          TITLE="[CI] Frontend Docker build failed on main"

          # Check for existing open issue with same title
          EXISTING=$(curl -s -X POST https://api.linear.app/graphql \
            -H "Content-Type: application/json" \
            -H "Authorization: $LINEAR_API_KEY" \
            -d '{
              "query": "query SearchIssues($filter: IssueFilter) { issues(filter: $filter) { nodes { id identifier } } }",
              "variables": {
                "filter": {
                  "team": { "key": { "eq": "NEM" } },
                  "title": { "eq": "'"$TITLE"'" },
                  "state": { "type": { "nin": ["completed", "canceled"] } }
                }
              }
            }')

          EXISTING_ID=$(echo "$EXISTING" | jq -r '.data.issues.nodes[0].id // empty')

          if [ -n "$EXISTING_ID" ]; then
            # Add comment to existing issue
            COMMENT="**Another failure detected**\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            curl -s -X POST https://api.linear.app/graphql \
              -H "Content-Type: application/json" \
              -H "Authorization: $LINEAR_API_KEY" \
              -d '{
                "query": "mutation CreateComment($input: CommentCreateInput!) { commentCreate(input: $input) { success } }",
                "variables": {
                  "input": {
                    "issueId": "'"$EXISTING_ID"'",
                    "body": "'"$COMMENT"'"
                  }
                }
              }'
            echo "Added comment to existing issue"
          else
            # Create new issue
            curl -s -X POST https://api.linear.app/graphql \
              -H "Content-Type: application/json" \
              -H "Authorization: $LINEAR_API_KEY" \
              -d '{
                "query": "mutation CreateIssue($input: IssueCreateInput!) { issueCreate(input: $input) { success issue { identifier url } } }",
                "variables": {
                  "input": {
                    "teamId": "998946a2-aa75-491b-a39d-189660131392",
                    "title": "'"$TITLE"'",
                    "description": "Frontend Docker build failed after merge to main.\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\nPlease investigate - Docker build must work for deployment.",
                    "priority": 1
                  }
                }
              }'
            echo "Created new issue"
          fi
  # Test Performance Audit Job (main-only)
  test-performance-audit:
    name: Test Performance Audit
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    needs:
      - unit-tests
      - integration-tests-api
      - integration-tests-websocket
      - integration-tests-services
      - integration-tests-models
      - frontend-e2e
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Download all test results
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7
        with:
          pattern: test-results-*
          path: test-results/
          merge-multiple: true
        continue-on-error: true

      - name: Download E2E test results
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7
        with:
          pattern: playwright-test-results-*
          path: test-results/
          merge-multiple: true
        continue-on-error: true

      - name: List downloaded artifacts
        run: |
          if [ -d "test-results/" ]; then
            echo "Test results directory exists"
            find test-results/ -type f -name "*.xml" | head -30 || echo "No XML files found"
          else
            echo "Warning: test-results/ directory not found"
            mkdir -p test-results/
          fi

      - name: Analyze test durations
        run: |
          if [ -d "test-results/" ] && [ -n "$(find test-results/ -type f -name '*.xml' 2>/dev/null)" ]; then
            uv run python scripts/audit-test-durations.py test-results/
          else
            echo " No test results found - skipping performance audit"
            echo "This may indicate that test jobs did not complete or upload artifacts"
            exit 0
          fi
        env:
          UNIT_TEST_THRESHOLD: '1.0'
          INTEGRATION_TEST_THRESHOLD: '5.0'
          E2E_TEST_THRESHOLD: '10.0'
          WARN_THRESHOLD_PERCENT: '80'

      - name: Create Linear issue on failure
        if: failure()
        env:
          LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}
        run: |
          TITLE="[CI] Test Performance Audit failed on main"

          # Check for existing open issue with same title
          EXISTING=$(curl -s -X POST https://api.linear.app/graphql \
            -H "Content-Type: application/json" \
            -H "Authorization: $LINEAR_API_KEY" \
            -d '{
              "query": "query SearchIssues($filter: IssueFilter) { issues(filter: $filter) { nodes { id identifier } } }",
              "variables": {
                "filter": {
                  "team": { "key": { "eq": "NEM" } },
                  "title": { "eq": "'"$TITLE"'" },
                  "state": { "type": { "nin": ["completed", "canceled"] } }
                }
              }
            }')

          EXISTING_ID=$(echo "$EXISTING" | jq -r '.data.issues.nodes[0].id // empty')

          if [ -n "$EXISTING_ID" ]; then
            # Add comment to existing issue
            COMMENT="**Another failure detected**\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            curl -s -X POST https://api.linear.app/graphql \
              -H "Content-Type: application/json" \
              -H "Authorization: $LINEAR_API_KEY" \
              -d '{
                "query": "mutation CreateComment($input: CommentCreateInput!) { commentCreate(input: $input) { success } }",
                "variables": {
                  "input": {
                    "issueId": "'"$EXISTING_ID"'",
                    "body": "'"$COMMENT"'"
                  }
                }
              }'
            echo "Added comment to existing issue"
          else
            # Create new issue
            curl -s -X POST https://api.linear.app/graphql \
              -H "Content-Type: application/json" \
              -H "Authorization: $LINEAR_API_KEY" \
              -d '{
                "query": "mutation CreateIssue($input: IssueCreateInput!) { issueCreate(input: $input) { success issue { identifier url } } }",
                "variables": {
                  "input": {
                    "teamId": "998946a2-aa75-491b-a39d-189660131392",
                    "title": "'"$TITLE"'",
                    "description": "Test performance audit found slow tests after merge to main.\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\nPlease review and optimize slow tests.",
                    "priority": 3
                  }
                }
              }'
            echo "Created new issue"
          fi

  # Test Count Verification Job (weekly schedule)
  # Prevents accidental test deletion by comparing against baseline counts
  # Run weekly instead of every main push to reduce CI load

  test-count-verification:
    name: Test Count Verification
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Set up Node.js
        uses: actions/setup-node@6044e13b5dc448c55e2357c09f80417699197238 # v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install frontend dependencies
        run: cd frontend && npm ci

      - name: Count and verify backend tests
        run: |
          # Count backend tests using pytest collection
          echo "Counting backend tests..."
          UNIT_COUNT=$(uv run pytest backend/tests/unit/ --collect-only -q 2>/dev/null | tail -1 | grep -oE '[0-9]+' | head -1 || echo "0")
          INTEGRATION_COUNT=$(uv run pytest backend/tests/integration/ --collect-only -q 2>/dev/null | tail -1 | grep -oE '[0-9]+' | head -1 || echo "0")

          echo "Backend Unit Tests: $UNIT_COUNT"
          echo "Backend Integration Tests: $INTEGRATION_COUNT"

          # Baseline thresholds - update these as tests are added
          # These represent minimum acceptable test counts
          UNIT_MIN=2900
          INTEGRATION_MIN=600

          if [ "$UNIT_COUNT" -lt "$UNIT_MIN" ]; then
            echo "::error::Backend unit test count ($UNIT_COUNT) is below minimum threshold ($UNIT_MIN). Tests may have been accidentally deleted."
            exit 1
          fi

          if [ "$INTEGRATION_COUNT" -lt "$INTEGRATION_MIN" ]; then
            echo "::error::Backend integration test count ($INTEGRATION_COUNT) is below minimum threshold ($INTEGRATION_MIN). Tests may have been accidentally deleted."
            exit 1
          fi

          echo "Test count verification passed!"

      - name: Count and verify frontend tests
        run: |
          # Count frontend tests by scanning test files
          echo "Counting frontend tests..."
          cd frontend

          # Count test files and estimate tests
          TEST_FILES=$(find src -name "*.test.ts" -o -name "*.test.tsx" | wc -l)
          E2E_SPECS=$(find tests/e2e -name "*.spec.ts" | wc -l)

          echo "Frontend Test Files: $TEST_FILES"
          echo "Frontend E2E Spec Files: $E2E_SPECS"

          # Baseline thresholds
          TEST_FILES_MIN=50
          E2E_SPECS_MIN=15

          if [ "$TEST_FILES" -lt "$TEST_FILES_MIN" ]; then
            echo "::error::Frontend test file count ($TEST_FILES) is below minimum threshold ($TEST_FILES_MIN). Tests may have been accidentally deleted."
            exit 1
          fi

          if [ "$E2E_SPECS" -lt "$E2E_SPECS_MIN" ]; then
            echo "::error::Frontend E2E spec count ($E2E_SPECS) is below minimum threshold ($E2E_SPECS_MIN). Tests may have been accidentally deleted."
            exit 1
          fi

          echo "Frontend test count verification passed!"

      - name: Create Linear issue on failure
        if: failure()
        env:
          LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}
        run: |
          TITLE="[CI] Test Count Verification failed on main"

          # Check for existing open issue with same title
          EXISTING=$(curl -s -X POST https://api.linear.app/graphql \
            -H "Content-Type: application/json" \
            -H "Authorization: $LINEAR_API_KEY" \
            -d '{
              "query": "query SearchIssues($filter: IssueFilter) { issues(filter: $filter) { nodes { id identifier } } }",
              "variables": {
                "filter": {
                  "team": { "key": { "eq": "NEM" } },
                  "title": { "eq": "'"$TITLE"'" },
                  "state": { "type": { "nin": ["completed", "canceled"] } }
                }
              }
            }')

          EXISTING_ID=$(echo "$EXISTING" | jq -r '.data.issues.nodes[0].id // empty')

          if [ -n "$EXISTING_ID" ]; then
            # Add comment to existing issue
            COMMENT="**Another failure detected**\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            curl -s -X POST https://api.linear.app/graphql \
              -H "Content-Type: application/json" \
              -H "Authorization: $LINEAR_API_KEY" \
              -d '{
                "query": "mutation CreateComment($input: CommentCreateInput!) { commentCreate(input: $input) { success } }",
                "variables": {
                  "input": {
                    "issueId": "'"$EXISTING_ID"'",
                    "body": "'"$COMMENT"'"
                  }
                }
              }'
            echo "Added comment to existing issue"
          else
            # Create new issue
            curl -s -X POST https://api.linear.app/graphql \
              -H "Content-Type: application/json" \
              -H "Authorization: $LINEAR_API_KEY" \
              -d '{
                "query": "mutation CreateIssue($input: IssueCreateInput!) { issueCreate(input: $input) { success issue { identifier url } } }",
                "variables": {
                  "input": {
                    "teamId": "998946a2-aa75-491b-a39d-189660131392",
                    "title": "'"$TITLE"'",
                    "description": "Test count verification failed after merge to main - tests may have been accidentally deleted.\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\nPlease investigate and restore any missing tests.",
                    "priority": 2
                  }
                }
              }'
            echo "Created new issue"
          fi

  # Security Scanning Jobs (Matrix Strategy for Parallelization)

  # Consolidated Trivy scans using matrix for parallel execution
  trivy-scan:
    name: Trivy Scan (${{ matrix.target.name }})
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        target:
          - name: backend-deps
            scan-type: fs
            scan-ref: '.'
            skip-dirs: 'frontend,node_modules,.git'
            is-dockerfile: false
          - name: backend-dockerfile
            scan-type: config
            scan-ref: 'backend/Dockerfile'
            skip-dirs: ''
            is-dockerfile: true
          - name: frontend-deps
            scan-type: fs
            scan-ref: 'frontend'
            skip-dirs: ''
            is-dockerfile: false
          - name: frontend-dockerfile
            scan-type: config
            scan-ref: 'frontend/Dockerfile'
            skip-dirs: ''
            is-dockerfile: true
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Run Trivy vulnerability scanner
        if: ${{ !matrix.target.is-dockerfile }}
        uses: aquasecurity/trivy-action@b6643a29fecd7f34b3597bc6acb0a98b03d33ff8 # 0.33.1
        with:
          scan-type: ${{ matrix.target.scan-type }}
          scan-ref: ${{ matrix.target.scan-ref }}
          scanners: 'vuln'
          severity: 'HIGH,CRITICAL'
          exit-code: '1'
          ignore-unfixed: true
          format: 'table'
          skip-dirs: ${{ matrix.target.skip-dirs }}

      - name: Run Trivy config scanner
        if: ${{ matrix.target.is-dockerfile }}
        uses: aquasecurity/trivy-action@b6643a29fecd7f34b3597bc6acb0a98b03d33ff8 # 0.33.1
        with:
          scan-type: ${{ matrix.target.scan-type }}
          scan-ref: ${{ matrix.target.scan-ref }}
          exit-code: '1'
          severity: 'HIGH,CRITICAL'
          format: 'table'

  npm-audit:
    name: npm Audit (Frontend)
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up Node.js
        uses: actions/setup-node@6044e13b5dc448c55e2357c09f80417699197238 # v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: cd frontend && npm ci

      - name: Run npm audit
        run: cd frontend && npm audit --audit-level=high
        continue-on-error: false

  security-tests:
    name: Security Test Suite
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres # pragma: allowlist secret
          POSTGRES_DB: security_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      TEST_DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      REDIS_URL: redis://localhost:6379
      REDIS_HOST: localhost
      REDIS_PORT: 6379
      LOG_DB_ENABLED: 'false'
      ENVIRONMENT: development # Avoid production password validation in CI
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Run security tests
        run: |
          uv run pytest backend/tests/security/ \
            -n0 \
            --timeout=30 \
            -v \
            --tb=short

  # TDD Compliance Tracking (PR-only, non-blocking)
  # Analyzes commit patterns to detect TDD practice compliance
  # Reports warnings for potential test-after-implementation patterns
  tdd-compliance:
    name: TDD Compliance Tracking
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    continue-on-error: true
    permissions:
      pull-requests: write
      contents: read
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4
        with:
          fetch-depth: 0

      - name: Analyze TDD compliance
        id: tdd-analysis
        run: |
          echo "## TDD Compliance Analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Get base and head commits for PR
          BASE_SHA="${{ github.event.pull_request.base.sha }}"
          HEAD_SHA="${{ github.event.pull_request.head.sha }}"

          # Count commits in PR
          COMMIT_COUNT=$(git rev-list --count $BASE_SHA..$HEAD_SHA)
          echo "**PR Commits:** $COMMIT_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Analyze each commit for test vs implementation file changes
          echo "### Commit Analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          TDD_COMPLIANT=0
          TDD_WARNING=0
          MIXED_COMMITS=0

          for commit in $(git rev-list $BASE_SHA..$HEAD_SHA); do
            COMMIT_MSG=$(git log -1 --pretty=format:"%s" $commit)
            COMMIT_SHORT=$(git rev-parse --short $commit)

            # Get changed files in commit
            TEST_FILES=$(git diff-tree --no-commit-id --name-only -r $commit | grep -E "(test_|\.test\.|\.spec\.)" | wc -l)
            IMPL_FILES=$(git diff-tree --no-commit-id --name-only -r $commit | grep -v -E "(test_|\.test\.|\.spec\.)" | grep -E "\.(py|ts|tsx)$" | wc -l)

            # Determine pattern
            if [ "$TEST_FILES" -gt 0 ] && [ "$IMPL_FILES" -eq 0 ]; then
              echo " **$COMMIT_SHORT**: Tests only - \`$COMMIT_MSG\`" >> $GITHUB_STEP_SUMMARY
              TDD_COMPLIANT=$((TDD_COMPLIANT + 1))
            elif [ "$TEST_FILES" -gt 0 ] && [ "$IMPL_FILES" -gt 0 ]; then
              echo "  **$COMMIT_SHORT**: Mixed (tests: $TEST_FILES, impl: $IMPL_FILES) - \`$COMMIT_MSG\`" >> $GITHUB_STEP_SUMMARY
              MIXED_COMMITS=$((MIXED_COMMITS + 1))
            elif [ "$TEST_FILES" -eq 0 ] && [ "$IMPL_FILES" -gt 0 ]; then
              echo "  **$COMMIT_SHORT**: Implementation only - \`$COMMIT_MSG\`" >> $GITHUB_STEP_SUMMARY
              TDD_WARNING=$((TDD_WARNING + 1))
            else
              echo "  **$COMMIT_SHORT**: Other changes - \`$COMMIT_MSG\`" >> $GITHUB_STEP_SUMMARY
            fi
          done

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "-  Test-only commits: $TDD_COMPLIANT" >> $GITHUB_STEP_SUMMARY
          echo "-   Mixed commits: $MIXED_COMMITS" >> $GITHUB_STEP_SUMMARY
          echo "-   Implementation-only commits: $TDD_WARNING" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Calculate test-to-implementation ratio
          TOTAL_TEST_FILES=$(git diff --name-only $BASE_SHA...$HEAD_SHA | grep -E "(test_|\.test\.|\.spec\.)" | wc -l)
          TOTAL_IMPL_FILES=$(git diff --name-only $BASE_SHA...$HEAD_SHA | grep -v -E "(test_|\.test\.|\.spec\.)" | grep -E "\.(py|ts|tsx)$" | wc -l)

          echo "### PR File Changes" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- Test files changed: $TOTAL_TEST_FILES" >> $GITHUB_STEP_SUMMARY
          echo "- Implementation files changed: $TOTAL_IMPL_FILES" >> $GITHUB_STEP_SUMMARY

          if [ "$TOTAL_IMPL_FILES" -gt 0 ]; then
            RATIO=$(awk "BEGIN {printf \"%.2f\", $TOTAL_TEST_FILES / $TOTAL_IMPL_FILES}")
            echo "- Test-to-implementation ratio: **${RATIO}:1**" >> $GITHUB_STEP_SUMMARY

            if (( $(echo "$RATIO < 0.5" | bc -l) )); then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "  **Warning**: Low test-to-implementation ratio. Consider adding more test coverage." >> $GITHUB_STEP_SUMMARY
            fi
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### TDD Best Practices Reminder" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "1. **RED**: Write failing tests first" >> $GITHUB_STEP_SUMMARY
          echo "2. **GREEN**: Implement minimum code to pass tests" >> $GITHUB_STEP_SUMMARY
          echo "3. **REFACTOR**: Improve code while keeping tests green" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Mixed commits (tests + implementation) may indicate test-after-implementation." >> $GITHUB_STEP_SUMMARY
          echo "Consider separate commits for test creation and implementation phases." >> $GITHUB_STEP_SUMMARY

          # Output for comment
          echo "tdd_compliant=$TDD_COMPLIANT" >> $GITHUB_OUTPUT
          echo "tdd_warning=$TDD_WARNING" >> $GITHUB_OUTPUT
          echo "mixed_commits=$MIXED_COMMITS" >> $GITHUB_OUTPUT
          echo "test_files=$TOTAL_TEST_FILES" >> $GITHUB_OUTPUT
          echo "impl_files=$TOTAL_IMPL_FILES" >> $GITHUB_OUTPUT

  # CI Gate - Aggregates all required checks for branch protection
  # This job provides a single status check that depends on all required jobs.
  # Use this as the required check in branch protection to simplify configuration.
  # Non-blocking workflows can use workflow_run trigger on CI completion.
  # Note: Jobs may be skipped based on changed files (detect-changes job)
  ci-gate:
    name: CI Gate (Required Checks)
    runs-on: ubuntu-latest
    needs:
      - detect-changes
      - lint
      - typecheck
      - unit-tests-summary
      - integration-tests-summary
      - frontend-lint
      - frontend-typecheck
      - frontend-tests-summary
      - frontend-e2e-summary
      - api-types-check
    if: always()
    steps:
      - name: Check required job results
        run: |
          echo "## Required CI Checks Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check each required job
          FAILED=false

          check_job() {
            local name="$1"
            local result="$2"
            if [ "$result" == "success" ] || [ "$result" == "skipped" ]; then
              echo " $name: $result" >> $GITHUB_STEP_SUMMARY
            else
              echo " $name: $result" >> $GITHUB_STEP_SUMMARY
              FAILED=true
            fi
          }

          check_job "Backend Lint (Ruff)" "${{ needs.lint.result }}"
          check_job "Backend Type Check (Mypy)" "${{ needs.typecheck.result }}"
          check_job "Backend Unit Tests" "${{ needs.unit-tests-summary.result }}"
          check_job "Backend Integration Tests" "${{ needs.integration-tests-summary.result }}"
          check_job "Frontend Lint (ESLint)" "${{ needs.frontend-lint.result }}"
          check_job "Frontend Type Check (TypeScript)" "${{ needs.frontend-typecheck.result }}"
          check_job "Frontend Tests (Vitest)" "${{ needs.frontend-tests-summary.result }}"
          check_job "E2E Tests (Chromium)" "${{ needs.frontend-e2e-summary.result }}"
          check_job "API Types Contract Check" "${{ needs.api-types-check.result }}"

          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "$FAILED" == "true" ]; then
            echo "**Status:  FAILED** - One or more required checks failed" >> $GITHUB_STEP_SUMMARY
            exit 1
          else
            echo "**Status:  PASSED** - All required checks passed" >> $GITHUB_STEP_SUMMARY
          fi
