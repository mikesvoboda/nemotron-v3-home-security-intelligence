name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.14'
  NODE_VERSION: '20'
  UV_VERSION: '0.9.18' # Update this single location to change uv version across all workflows

jobs:
  # Backend Jobs

  lint:
    name: Backend Lint (Ruff) - Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.14']
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Ruff check
        run: uv run ruff check backend/

      - name: Ruff format check
        run: uv run ruff format --check backend/

      - name: Complexity check (Radon)
        run: |
          uv run radon cc backend/ -a -nc
          uv run radon mi backend/ -nc

  typecheck:
    name: Backend Type Check (Mypy) - Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.14']
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Mypy
        run: uv run mypy backend/ --ignore-missing-imports

  unit-tests:
    name: Backend Unit Tests - Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.14']
    # Wait for lint to pass first - fail fast on code quality issues
    # This saves CI minutes by not running expensive tests when lint fails
    needs: [lint]
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres # pragma: allowlist secret
          POSTGRES_DB: security_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      TEST_DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      LOG_DB_ENABLED: 'false'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Run unit tests with coverage (parallelized)
        # Unit tests coverage threshold - 85% for unit tests alone
        # Combined coverage (unit + integration) should reach 95%
        # -n auto enables xdist parallelization with worksteal scheduler
        # --timeout=0 disables timeout (fixture import takes >1s in CI)
        # --cov-fail-under=0 disables threshold check here (checked after combining coverage)
        run: |
          uv run pytest backend/tests/unit/ \
            -n auto --dist=worksteal \
            --timeout=0 \
            --cov=backend \
            --cov-config=pyproject.toml \
            --cov-fail-under=0 \
            --cov-report=term-missing \
            --junit-xml=test-results-unit-parallel.xml \
            --durations=20 \
            -v

      - name: Generate combined coverage report
        # Generate XML and check 85% threshold for unit tests
        # Note: --rcfile=/dev/null ignores pyproject.toml's fail_under=95 setting
        # We use 85% for unit tests alone; combined coverage (unit+integration) should reach 95%
        # IMPORTANT: coverage xml also reads fail_under from pyproject.toml, so add --rcfile=/dev/null
        run: |
          uv run coverage xml -o coverage-unit.xml --rcfile=/dev/null
          uv run coverage report --fail-under=85 --rcfile=/dev/null

      - name: Upload unit test results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: test-results-unit-py${{ matrix.python-version }}
          path: test-results-unit-parallel.xml
          retention-days: 7

      - name: Upload unit test coverage
        uses: codecov/codecov-action@671740ac38dd9b0130fbe1cec585b89eea48d3de # v5
        with:
          files: coverage-unit.xml
          flags: backend-unit-py${{ matrix.python-version }}
          fail_ci_if_error: false

  # Integration tests split into 4 domain-based parallel jobs for faster execution
  # Each job runs tests for a specific domain with within-shard parallelization
  integration-tests-api:
    name: Integration Tests (API) - Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    timeout-minutes: 20
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.14']
    # Wait for lint to pass first - fail fast on code quality issues
    needs: [lint]
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres # pragma: allowlist secret
          POSTGRES_DB: security_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      TEST_DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      REDIS_URL: redis://localhost:6379
      REDIS_HOST: localhost
      REDIS_PORT: 6379
      LOG_DB_ENABLED: 'false'
      CI: 'true'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Run API integration tests with retry
        # API tests: test_*_api.py files
        # -n0 runs tests serially to avoid database deadlocks from concurrent schema creation
        # Retry loop: 3 attempts with 10s wait between retries for transient failures
        run: |
          for attempt in 1 2 3; do
            echo "Attempt $attempt of 3"
            if uv run pytest backend/tests/integration/ \
              -k "test_admin_api or test_ai_audit_api or test_alerts_api or test_api_error_scenarios or test_api_errors or test_audit_api or test_cameras_api or test_detections_api or test_dlq_api or test_entities_api or test_events_api or test_http_error_codes or test_logs_api or test_media_api or test_media_security or test_metrics_api or test_notification_api or test_search_api or test_system_api or test_video_streaming or test_zones_api or test_api" \
              -n0 \
              --timeout=30 \
              --cov=backend \
              --cov-fail-under=0 \
              --cov-report=xml:coverage-integration-api.xml \
              --cov-report=term-missing \
              --junit-xml=test-results-integration-api.xml \
              --durations=20 \
              -v; then
              exit 0
            fi
            if [ $attempt -lt 3 ]; then
              echo "Test failed, retrying in 10s..."
              sleep 10
            fi
          done
          exit 1

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: test-results-integration-api-py${{ matrix.python-version }}
          path: test-results-integration-api.xml
          retention-days: 7

      - name: Upload coverage artifact
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: coverage-integration-api-py${{ matrix.python-version }}
          path: coverage-integration-api.xml
          retention-days: 7

  integration-tests-websocket:
    name: Integration Tests (WebSocket) - Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    timeout-minutes: 20
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.14']
    # Wait for lint to pass first - fail fast on code quality issues
    needs: [lint]
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres # pragma: allowlist secret
          POSTGRES_DB: security_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      TEST_DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      REDIS_URL: redis://localhost:6379
      REDIS_HOST: localhost
      REDIS_PORT: 6379
      LOG_DB_ENABLED: 'false'
      CI: 'true'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Run WebSocket integration tests with retry
        # WebSocket tests: test_websocket*.py, test_broadcast*.py, test_pubsub*.py
        # -n0 runs tests serially to avoid database deadlocks from concurrent schema creation
        # Retry loop: 3 attempts with 10s wait between retries for transient failures
        run: |
          for attempt in 1 2 3; do
            echo "Attempt $attempt of 3"
            if uv run pytest backend/tests/integration/ \
              -k "test_websocket or test_system_broadcaster or test_redis_pubsub" \
              -n0 \
              --timeout=30 \
              --cov=backend \
              --cov-fail-under=0 \
              --cov-report=xml:coverage-integration-websocket.xml \
              --cov-report=term-missing \
              --junit-xml=test-results-integration-websocket.xml \
              --durations=20 \
              -v; then
              exit 0
            fi
            if [ $attempt -lt 3 ]; then
              echo "Test failed, retrying in 10s..."
              sleep 10
            fi
          done
          exit 1

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: test-results-integration-websocket-py${{ matrix.python-version }}
          path: test-results-integration-websocket.xml
          retention-days: 7

      - name: Upload coverage artifact
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: coverage-integration-websocket-py${{ matrix.python-version }}
          path: coverage-integration-websocket.xml
          retention-days: 7

  integration-tests-services:
    name: Integration Tests (Services) - Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    timeout-minutes: 20
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.14']
    # Wait for lint to pass first - fail fast on code quality issues
    needs: [lint]
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres # pragma: allowlist secret
          POSTGRES_DB: security_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      TEST_DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      REDIS_URL: redis://localhost:6379
      REDIS_HOST: localhost
      REDIS_PORT: 6379
      LOG_DB_ENABLED: 'false'
      CI: 'true'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Install PostgreSQL client tools
        # Required for backup/restore tests (pg_dump, pg_restore)
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Run Services integration tests with retry
        # Service tests: test_*_integration.py, test_batch*.py, test_detector*.py, test_file_watcher*.py, test_circuit*.py
        # Also includes: test_backup_restore, test_data_corruption for database operations
        # -n0 runs tests serially to avoid database deadlocks from concurrent schema creation
        # Retry loop: 3 attempts with 10s wait between retries for transient failures
        run: |
          for attempt in 1 2 3; do
            echo "Attempt $attempt of 3"
            if uv run pytest backend/tests/integration/ \
              -k "test_batch_aggregator_integration or test_detector_client_integration or test_file_watcher_filesystem or test_file_watcher_integration or test_circuit_breaker or test_cleanup_service or test_dlq_retry_handler_integration or test_health_monitor_integration or test_nemotron_analyzer_integration or test_nemotron_analyzer or test_vision_extraction_pipeline or test_pipeline_e2e or test_full_stack or test_audit or test_event_search or test_github_workflows or test_alembic_migrations or test_transaction_rollback or test_backup_restore or test_data_corruption" \
              -n0 \
              --timeout=30 \
              --cov=backend \
              --cov-fail-under=0 \
              --cov-report=xml:coverage-integration-services.xml \
              --cov-report=term-missing \
              --junit-xml=test-results-integration-services.xml \
              --durations=20 \
              -v; then
              exit 0
            fi
            if [ $attempt -lt 3 ]; then
              echo "Test failed, retrying in 10s..."
              sleep 10
            fi
          done
          exit 1

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: test-results-integration-services-py${{ matrix.python-version }}
          path: test-results-integration-services.xml
          retention-days: 7

      - name: Upload coverage artifact
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: coverage-integration-services-py${{ matrix.python-version }}
          path: coverage-integration-services.xml
          retention-days: 7

  integration-tests-models:
    name: Integration Tests (Models) - Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    timeout-minutes: 20
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.14']
    # Wait for lint to pass first - fail fast on code quality issues
    needs: [lint]
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres # pragma: allowlist secret
          POSTGRES_DB: security_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      TEST_DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      REDIS_URL: redis://localhost:6379
      REDIS_HOST: localhost
      REDIS_PORT: 6379
      LOG_DB_ENABLED: 'false'
      CI: 'true'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Run Models integration tests with retry
        # Model tests: test_models*.py, test_database*.py, test_baseline*.py, test_alert_*.py, test_enrichment*.py, test_cache*.py
        # -n0 runs tests serially to avoid database deadlocks from concurrent schema creation
        # Retry loop: 3 attempts with 10s wait between retries for transient failures
        run: |
          for attempt in 1 2 3; do
            echo "Attempt $attempt of 3"
            if uv run pytest backend/tests/integration/ \
              -k "test_models or test_model_cascades or test_database or test_baseline or test_alert_dedup or test_alert_engine or test_alert_models or test_enrichment_pipeline or test_cache_service_integration" \
              -n0 \
              --timeout=30 \
              --cov=backend \
              --cov-fail-under=0 \
              --cov-report=xml:coverage-integration-models.xml \
              --cov-report=term-missing \
              --junit-xml=test-results-integration-models.xml \
              --durations=20 \
              -v; then
              exit 0
            fi
            if [ $attempt -lt 3 ]; then
              echo "Test failed, retrying in 10s..."
              sleep 10
            fi
          done
          exit 1

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: test-results-integration-models-py${{ matrix.python-version }}
          path: test-results-integration-models.xml
          retention-days: 7

      - name: Upload coverage artifact
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: coverage-integration-models-py${{ matrix.python-version }}
          path: coverage-integration-models.xml
          retention-days: 7

  # Merge coverage from all integration test shards
  integration-coverage-merge:
    name: Merge Integration Coverage
    runs-on: ubuntu-latest
    needs:
      [
        integration-tests-api,
        integration-tests-websocket,
        integration-tests-services,
        integration-tests-models,
      ]
    if: always()
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Download all coverage artifacts
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7
        with:
          pattern: coverage-integration-*
          path: coverage-reports/
          merge-multiple: true

      - name: List coverage files
        run: find coverage-reports/ -name "*.xml" | head -20

      - name: Upload merged coverage to Codecov
        uses: codecov/codecov-action@671740ac38dd9b0130fbe1cec585b89eea48d3de # v5
        with:
          directory: coverage-reports/
          flags: backend-integration
          fail_ci_if_error: false

  # Summary job for branch protection - aggregates all integration test shards
  integration-tests-summary:
    name: Backend Integration Tests
    runs-on: ubuntu-latest
    needs:
      [
        integration-tests-api,
        integration-tests-websocket,
        integration-tests-services,
        integration-tests-models,
      ]
    if: always()
    steps:
      - name: Check integration test results
        run: |
          if [ "${{ needs.integration-tests-api.result }}" == "failure" ] || \
             [ "${{ needs.integration-tests-websocket.result }}" == "failure" ] || \
             [ "${{ needs.integration-tests-services.result }}" == "failure" ] || \
             [ "${{ needs.integration-tests-models.result }}" == "failure" ]; then
            echo "One or more integration test jobs failed"
            exit 1
          fi
          echo "All integration tests passed"

  # Contract tests for API schema validation
  # Main-only: redundant with api-types-check, runs post-merge for validation
  contract-tests:
    name: Contract Tests (API Schema)
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: github.ref == 'refs/heads/main'
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres # pragma: allowlist secret
          POSTGRES_DB: security_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Run contract tests
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
          REDIS_URL: redis://localhost:6379/15
          REDIS_HOST: localhost
          REDIS_PORT: 6379
          LOG_DB_ENABLED: 'false'
        run: |
          uv run pytest backend/tests/contracts/ \
            -n0 \
            --timeout=30 \
            -v \
            --junit-xml=test-results-contracts.xml \
            --durations=10

      - name: Upload contract test results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: test-results-contracts
          path: test-results-contracts.xml
          retention-days: 7

      - name: Create Linear issue on failure
        if: failure()
        env:
          LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}
        run: |
          curl -s -X POST https://api.linear.app/graphql \
            -H "Content-Type: application/json" \
            -H "Authorization: $LINEAR_API_KEY" \
            -d '{
              "query": "mutation CreateIssue($input: IssueCreateInput!) { issueCreate(input: $input) { success issue { identifier url } } }",
              "variables": {
                "input": {
                  "teamId": "998946a2-aa75-491b-a39d-189660131392",
                  "title": "[CI] Contract Tests failed on main",
                  "description": "Contract tests failed after merge to main.\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\nPlease investigate and fix.",
                  "priority": 2
                }
              }
            }'

  # Main-only: dead code detection runs post-merge for code hygiene
  dead-code:
    name: Dead Code Detection
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Run Vulture (dead code detection)
        run: uv run vulture backend/ vulture_whitelist.py --min-confidence 80

      - name: Create Linear issue on failure
        if: failure()
        env:
          LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}
        run: |
          curl -s -X POST https://api.linear.app/graphql \
            -H "Content-Type: application/json" \
            -H "Authorization: $LINEAR_API_KEY" \
            -d '{
              "query": "mutation CreateIssue($input: IssueCreateInput!) { issueCreate(input: $input) { success issue { identifier url } } }",
              "variables": {
                "input": {
                  "teamId": "998946a2-aa75-491b-a39d-189660131392",
                  "title": "[CI] Dead Code Detection failed on main",
                  "description": "Dead code detection (Vulture) found issues after merge to main.\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\nPlease investigate and remove dead code.",
                  "priority": 3
                }
              }
            }'

  # Frontend Jobs

  api-types-check:
    name: API Types Contract Check
    runs-on: ubuntu-latest
    env:
      DATABASE_URL: postgresql://user:pass@localhost/db # pragma: allowlist secret
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install backend dependencies
        run: uv sync --extra dev --frozen

      - name: Set up Node.js
        uses: actions/setup-node@395ad3262231945c25e8478fd5baf05154b1d79f # v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install frontend dependencies
        run: cd frontend && npm ci

      - name: Generate types from OpenAPI and check for diffs
        run: ./scripts/generate-types.sh --check

      - name: Check WebSocket types are current
        run: uv run python scripts/generate-ws-types.py --check

      - name: Check OpenAPI spec is current
        run: uv run python scripts/generate-openapi.py --check

      - name: Validate API type contract consistency
        run: ./scripts/validate-api-types.sh --verbose

  frontend-lint:
    name: Frontend Lint (ESLint)
    runs-on: ubuntu-latest
    env:
      NODE_VERSION: '20'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up Node.js
        uses: actions/setup-node@395ad3262231945c25e8478fd5baf05154b1d79f # v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: cd frontend && npm ci

      - name: ESLint
        run: cd frontend && npm run lint

      - name: Dead code detection (Knip)
        # Non-blocking initially - reports issues without failing CI
        continue-on-error: true
        run: cd frontend && npx knip

  frontend-typecheck:
    name: Frontend Type Check (TypeScript)
    runs-on: ubuntu-latest
    env:
      NODE_VERSION: '20'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up Node.js
        uses: actions/setup-node@395ad3262231945c25e8478fd5baf05154b1d79f # v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: cd frontend && npm ci

      - name: TypeScript check
        run: cd frontend && npm run typecheck

  frontend-tests:
    name: Frontend Tests (Vitest ${{ matrix.shard }}/8)
    runs-on: ubuntu-latest
    timeout-minutes: 5
    # Wait for lint to pass first - fail fast on code quality issues
    needs: [frontend-lint]
    strategy:
      fail-fast: false
      matrix:
        shard: [1, 2, 3, 4, 5, 6, 7, 8]
    env:
      NODE_VERSION: '20'
      # Increase Node.js heap size to prevent OOM errors during test cleanup
      # Shard 4 requires >6GB due to memory-intensive tests
      NODE_OPTIONS: '--max-old-space-size=8192'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up Node.js
        uses: actions/setup-node@395ad3262231945c25e8478fd5baf05154b1d79f # v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Restore npm cache with fallback
        uses: actions/cache@9255dc7a253b0ccc959486e2bca901246202afeb # v5
        with:
          path: |
            ~/.npm
            node_modules
          key: npm-${{ runner.os }}-node-${{ env.NODE_VERSION }}-${{ hashFiles('frontend/package-lock.json') }}
          restore-keys: |
            npm-${{ runner.os }}-node-${{ env.NODE_VERSION }}-
            npm-${{ runner.os }}-

      - name: Install dependencies
        run: cd frontend && npm ci

      - name: Run tests (shard ${{ matrix.shard }}/8)
        # Shard tests across 8 runners for faster execution and isolation
        # Coverage collected per-shard and merged in frontend-coverage-merge job
        # NOTE: Do NOT add --coverage here - it triggers per-shard threshold checks
        # that fail because each shard only has ~12% of total coverage
        # --teardownTimeout is now configured in vite.config.ts (5000ms)
        # --reporter=verbose provides detailed output for debugging
        run: cd frontend && npx vitest run --shard=${{ matrix.shard }}/8 --reporter=verbose

      - name: Upload coverage artifact
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: frontend-coverage-shard-${{ matrix.shard }}-node-${{ env.NODE_VERSION }}
          path: frontend/coverage/
          retention-days: 7

  # Merge frontend coverage from all shards and upload to Codecov
  frontend-coverage-merge:
    name: Merge Frontend Coverage
    runs-on: ubuntu-latest
    needs: [frontend-tests]
    if: always()
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Download all coverage artifacts
        # Downloads artifacts from all Node version + shard combinations
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7
        with:
          pattern: frontend-coverage-shard-*-node-*
          path: coverage-reports/
          merge-multiple: true

      - name: List coverage files
        run: find coverage-reports/ -name "*.json" -o -name "*.xml" | head -20

      - name: Upload frontend coverage to Codecov
        uses: codecov/codecov-action@671740ac38dd9b0130fbe1cec585b89eea48d3de # v5
        with:
          directory: coverage-reports/
          flags: frontend
          fail_ci_if_error: false

  # Summary job for branch protection - aggregates all Vitest shards
  frontend-tests-summary:
    name: Frontend Tests (Vitest)
    runs-on: ubuntu-latest
    needs: [frontend-tests]
    if: always()
    steps:
      - name: Check shard results
        run: |
          if [ "${{ needs.frontend-tests.result }}" == "failure" ]; then
            echo "One or more Vitest shards failed"
            exit 1
          fi
          echo "All Vitest shards passed"

  api-coverage:
    name: API Endpoint Coverage
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Check API coverage
        run: ./scripts/check-api-coverage.sh

  # Primary E2E tests (Chromium) - required for merge
  # Uses sharding to split tests across 4 parallel runners for 4x speed
  frontend-e2e:
    name: E2E Tests (chromium ${{ matrix.shard }})
    runs-on: ubuntu-latest
    # Wait for lint to pass first - fail fast on code quality issues
    needs: [frontend-lint]
    strategy:
      fail-fast: false
      matrix:
        shard: [1/4, 2/4, 3/4, 4/4]
    env:
      NODE_VERSION: '20'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up Node.js
        uses: actions/setup-node@395ad3262231945c25e8478fd5baf05154b1d79f # v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: cd frontend && npm ci

      - name: Cache Playwright browsers
        uses: actions/cache@9255dc7a253b0ccc959486e2bca901246202afeb # v5
        id: playwright-cache
        with:
          path: ~/.cache/ms-playwright
          key: playwright-chromium-${{ runner.os }}-node-${{ env.NODE_VERSION }}-${{ hashFiles('frontend/package-lock.json') }}

      - name: Install Playwright Chromium
        if: steps.playwright-cache.outputs.cache-hit != 'true'
        run: cd frontend && npx playwright install chromium --with-deps

      - name: Install Playwright deps (cached)
        if: steps.playwright-cache.outputs.cache-hit == 'true'
        run: cd frontend && npx playwright install-deps chromium

      - name: Run E2E tests (Chromium shard ${{ matrix.shard }}) with retry
        # Retry loop: 3 attempts with 10s wait between retries for transient failures
        run: |
          for attempt in 1 2 3; do
            echo "Attempt $attempt of 3"
            if cd frontend && npx playwright test --project=chromium --shard=${{ matrix.shard }}; then
              exit 0
            fi
            if [ $attempt -lt 3 ]; then
              echo "Test failed, retrying in 10s..."
              sleep 10
            fi
          done
          exit 1

      - name: Upload Playwright report
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: playwright-report-chromium-${{ strategy.job-index }}-node-${{ env.NODE_VERSION }}
          path: frontend/playwright-report/
          retention-days: 7

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: playwright-test-results-chromium-${{ strategy.job-index }}-node-${{ env.NODE_VERSION }}
          path: frontend/test-results/
          retention-days: 7

  # Summary job for branch protection - aggregates all E2E shards
  frontend-e2e-summary:
    name: E2E Tests (Chromium)
    runs-on: ubuntu-latest
    needs: [frontend-e2e]
    if: always()
    steps:
      - name: Check shard results
        run: |
          if [ "${{ needs.frontend-e2e.result }}" == "failure" ]; then
            echo "One or more E2E shards failed"
            exit 1
          fi
          echo "All E2E shards passed"

  # Secondary E2E tests (Firefox/WebKit) - main-only to improve PR merge velocity
  # Intentionally kept on Node 20 only (no matrix) to avoid excessive CI time
  # Main-only: Firefox/WebKit have known infrastructure flakiness (timeouts)
  frontend-e2e-secondary:
    name: E2E Tests (${{ matrix.browser }})
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.ref == 'refs/heads/main'
    strategy:
      fail-fast: false
      matrix:
        browser: [firefox, webkit]
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up Node.js
        uses: actions/setup-node@395ad3262231945c25e8478fd5baf05154b1d79f # v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: cd frontend && npm ci

      - name: Cache Playwright browsers
        uses: actions/cache@9255dc7a253b0ccc959486e2bca901246202afeb # v5
        id: playwright-cache
        with:
          path: ~/.cache/ms-playwright
          key: playwright-${{ matrix.browser }}-${{ runner.os }}-${{ hashFiles('frontend/package-lock.json') }}

      - name: Install Playwright ${{ matrix.browser }}
        if: steps.playwright-cache.outputs.cache-hit != 'true'
        run: cd frontend && npx playwright install ${{ matrix.browser }} --with-deps

      - name: Install Playwright deps (cached)
        if: steps.playwright-cache.outputs.cache-hit == 'true'
        run: cd frontend && npx playwright install-deps ${{ matrix.browser }}

      - name: Run E2E tests (${{ matrix.browser }})
        run: cd frontend && npx playwright test --project=${{ matrix.browser }}

      - name: Upload Playwright report
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: playwright-report-${{ matrix.browser }}
          path: frontend/playwright-report/
          retention-days: 7

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6
        with:
          name: playwright-test-results-${{ matrix.browser }}
          path: frontend/test-results/
          retention-days: 7

      - name: Create Linear issue on failure
        if: failure()
        env:
          LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}
        run: |
          curl -s -X POST https://api.linear.app/graphql \
            -H "Content-Type: application/json" \
            -H "Authorization: $LINEAR_API_KEY" \
            -d '{
              "query": "mutation CreateIssue($input: IssueCreateInput!) { issueCreate(input: $input) { success issue { identifier url } } }",
              "variables": {
                "input": {
                  "teamId": "998946a2-aa75-491b-a39d-189660131392",
                  "title": "[CI] E2E Tests (${{ matrix.browser }}) failed on main",
                  "description": "E2E tests for ${{ matrix.browser }} failed after merge to main.\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\nPlease investigate - may be infrastructure flakiness or real test failure.",
                  "priority": 3
                }
              }
            }'

  # Build Jobs (main-only - verify Docker builds after merge)
  # Separate jobs for backend/frontend to avoid disk space issues

  build-backend:
    name: Build Docker (backend)
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Free up disk space
        # Backend image includes heavy ML dependencies (PyTorch, transformers)
        # which require more disk space than default GitHub runners provide
        uses: jlumbroso/free-disk-space@54081f138730dfa15788a46383842cd2f914a1be # v1.3.1
        with:
          tool-cache: true
          android: true
          dotnet: true
          haskell: true
          large-packages: true
          docker-images: true
          swap-storage: true

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@8d2750c68a42422c14e847fe6c8ac0403b4cbd6f # v3

      - name: Build backend image
        uses: docker/build-push-action@ca052bb54ab0790a636c9b5f226502c73d547a25 # v5
        with:
          context: .
          file: ./backend/Dockerfile
          target: prod
          push: false
          tags: backend:${{ github.sha }}
          cache-from: type=gha,scope=backend
          cache-to: type=gha,mode=max,scope=backend

      - name: Create Linear issue on failure
        if: failure()
        env:
          LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}
        run: |
          curl -s -X POST https://api.linear.app/graphql \
            -H "Content-Type: application/json" \
            -H "Authorization: $LINEAR_API_KEY" \
            -d '{
              "query": "mutation CreateIssue($input: IssueCreateInput!) { issueCreate(input: $input) { success issue { identifier url } } }",
              "variables": {
                "input": {
                  "teamId": "998946a2-aa75-491b-a39d-189660131392",
                  "title": "[CI] Backend Docker build failed on main",
                  "description": "Backend Docker build failed after merge to main.\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\nPlease investigate - Docker build must work for deployment.",
                  "priority": 1
                }
              }
            }'

  build-frontend:
    name: Build Docker (frontend)
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@8d2750c68a42422c14e847fe6c8ac0403b4cbd6f # v3

      - name: Build frontend image
        uses: docker/build-push-action@ca052bb54ab0790a636c9b5f226502c73d547a25 # v5
        with:
          context: ./frontend
          file: ./frontend/Dockerfile
          target: prod
          push: false
          tags: frontend:${{ github.sha }}
          cache-from: type=gha,scope=frontend
          cache-to: type=gha,mode=max,scope=frontend

      - name: Create Linear issue on failure
        if: failure()
        env:
          LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}
        run: |
          curl -s -X POST https://api.linear.app/graphql \
            -H "Content-Type: application/json" \
            -H "Authorization: $LINEAR_API_KEY" \
            -d '{
              "query": "mutation CreateIssue($input: IssueCreateInput!) { issueCreate(input: $input) { success issue { identifier url } } }",
              "variables": {
                "input": {
                  "teamId": "998946a2-aa75-491b-a39d-189660131392",
                  "title": "[CI] Frontend Docker build failed on main",
                  "description": "Frontend Docker build failed after merge to main.\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\nPlease investigate - Docker build must work for deployment.",
                  "priority": 1
                }
              }
            }'

  # Security Validation Job (main-only)

  # Main-only: security tests verify DEBUG=false enforcement post-merge
  security-validation:
    name: Admin Endpoint Security Validation
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres # pragma: allowlist secret
          POSTGRES_DB: security_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      TEST_DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      REDIS_URL: redis://localhost:6379
      REDIS_HOST: localhost
      REDIS_PORT: 6379
      DEBUG: 'false'
      LOG_DB_ENABLED: 'false'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Validate admin endpoints require DEBUG mode
        run: |
          # Run specific tests that verify admin endpoints return 403 when DEBUG=false
          # These tests are critical security validation
          # -n0 disables xdist (security tests modify global settings)
          # --timeout=0 disables timeout (fixture import takes >1s in CI)
          uv run pytest backend/tests/integration/test_admin_api.py \
            -n0 \
            --timeout=0 \
            -k "requires_debug_mode" \
            -v \
            --tb=short

      - name: Verify DEBUG=false is enforced
        run: |
          # Double-check that DEBUG is actually false in this environment
          uv run python -c "
          import os
          from backend.core.config import get_settings
          settings = get_settings()
          assert settings.debug == False, f'DEBUG should be False but is {settings.debug}'
          print('SUCCESS: DEBUG mode is correctly set to False')
          "

      - name: Create Linear issue on failure
        if: failure()
        env:
          LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}
        run: |
          curl -s -X POST https://api.linear.app/graphql \
            -H "Content-Type: application/json" \
            -H "Authorization: $LINEAR_API_KEY" \
            -d '{
              "query": "mutation CreateIssue($input: IssueCreateInput!) { issueCreate(input: $input) { success issue { identifier url } } }",
              "variables": {
                "input": {
                  "teamId": "998946a2-aa75-491b-a39d-189660131392",
                  "title": "[CI] Security Validation failed on main",
                  "description": "Security validation (DEBUG=false enforcement) failed after merge to main.\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\n**CRITICAL:** Admin endpoints may be exposed. Investigate immediately.",
                  "priority": 1
                }
              }
            }'

  # Test Performance Audit Job (main-only)

  test-performance-audit:
    name: Test Performance Audit
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    needs:
      - unit-tests
      - integration-tests-api
      - integration-tests-websocket
      - integration-tests-services
      - integration-tests-models
      - frontend-e2e
      - frontend-e2e-secondary
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Download all test results
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7
        with:
          pattern: test-results-*
          path: test-results/
          merge-multiple: true
        continue-on-error: true

      - name: Download E2E test results
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7
        with:
          pattern: playwright-test-results-*
          path: test-results/
          merge-multiple: true
        continue-on-error: true

      - name: List downloaded artifacts
        run: |
          if [ -d "test-results/" ]; then
            echo "Test results directory exists"
            find test-results/ -type f -name "*.xml" | head -30 || echo "No XML files found"
          else
            echo "Warning: test-results/ directory not found"
            mkdir -p test-results/
          fi

      - name: Analyze test durations
        run: |
          if [ -d "test-results/" ] && [ -n "$(find test-results/ -type f -name '*.xml' 2>/dev/null)" ]; then
            uv run python scripts/audit-test-durations.py test-results/
          else
            echo " No test results found - skipping performance audit"
            echo "This may indicate that test jobs did not complete or upload artifacts"
            exit 0
          fi
        env:
          UNIT_TEST_THRESHOLD: '1.0'
          INTEGRATION_TEST_THRESHOLD: '5.0'
          E2E_TEST_THRESHOLD: '10.0'
          WARN_THRESHOLD_PERCENT: '80'

      - name: Create Linear issue on failure
        if: failure()
        env:
          LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}
        run: |
          curl -s -X POST https://api.linear.app/graphql \
            -H "Content-Type: application/json" \
            -H "Authorization: $LINEAR_API_KEY" \
            -d '{
              "query": "mutation CreateIssue($input: IssueCreateInput!) { issueCreate(input: $input) { success issue { identifier url } } }",
              "variables": {
                "input": {
                  "teamId": "998946a2-aa75-491b-a39d-189660131392",
                  "title": "[CI] Test Performance Audit failed on main",
                  "description": "Test performance audit found slow tests after merge to main.\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\nPlease review and optimize slow tests.",
                  "priority": 3
                }
              }
            }'

  # Test Count Verification Job (main-only)
  # Prevents accidental test deletion by comparing against baseline counts

  test-count-verification:
    name: Test Count Verification
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Set up Node.js
        uses: actions/setup-node@395ad3262231945c25e8478fd5baf05154b1d79f # v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install frontend dependencies
        run: cd frontend && npm ci

      - name: Count and verify backend tests
        run: |
          # Count backend tests using pytest collection
          echo "Counting backend tests..."
          UNIT_COUNT=$(uv run pytest backend/tests/unit/ --collect-only -q 2>/dev/null | tail -1 | grep -oE '[0-9]+' | head -1 || echo "0")
          INTEGRATION_COUNT=$(uv run pytest backend/tests/integration/ --collect-only -q 2>/dev/null | tail -1 | grep -oE '[0-9]+' | head -1 || echo "0")

          echo "Backend Unit Tests: $UNIT_COUNT"
          echo "Backend Integration Tests: $INTEGRATION_COUNT"

          # Baseline thresholds - update these as tests are added
          # These represent minimum acceptable test counts
          UNIT_MIN=2900
          INTEGRATION_MIN=600

          if [ "$UNIT_COUNT" -lt "$UNIT_MIN" ]; then
            echo "::error::Backend unit test count ($UNIT_COUNT) is below minimum threshold ($UNIT_MIN). Tests may have been accidentally deleted."
            exit 1
          fi

          if [ "$INTEGRATION_COUNT" -lt "$INTEGRATION_MIN" ]; then
            echo "::error::Backend integration test count ($INTEGRATION_COUNT) is below minimum threshold ($INTEGRATION_MIN). Tests may have been accidentally deleted."
            exit 1
          fi

          echo "Test count verification passed!"

      - name: Count and verify frontend tests
        run: |
          # Count frontend tests by scanning test files
          echo "Counting frontend tests..."
          cd frontend

          # Count test files and estimate tests
          TEST_FILES=$(find src -name "*.test.ts" -o -name "*.test.tsx" | wc -l)
          E2E_SPECS=$(find tests/e2e -name "*.spec.ts" | wc -l)

          echo "Frontend Test Files: $TEST_FILES"
          echo "Frontend E2E Spec Files: $E2E_SPECS"

          # Baseline thresholds
          TEST_FILES_MIN=50
          E2E_SPECS_MIN=15

          if [ "$TEST_FILES" -lt "$TEST_FILES_MIN" ]; then
            echo "::error::Frontend test file count ($TEST_FILES) is below minimum threshold ($TEST_FILES_MIN). Tests may have been accidentally deleted."
            exit 1
          fi

          if [ "$E2E_SPECS" -lt "$E2E_SPECS_MIN" ]; then
            echo "::error::Frontend E2E spec count ($E2E_SPECS) is below minimum threshold ($E2E_SPECS_MIN). Tests may have been accidentally deleted."
            exit 1
          fi

          echo "Frontend test count verification passed!"

      - name: Create Linear issue on failure
        if: failure()
        env:
          LINEAR_API_KEY: ${{ secrets.LINEAR_API_KEY }}
        run: |
          curl -s -X POST https://api.linear.app/graphql \
            -H "Content-Type: application/json" \
            -H "Authorization: $LINEAR_API_KEY" \
            -d '{
              "query": "mutation CreateIssue($input: IssueCreateInput!) { issueCreate(input: $input) { success issue { identifier url } } }",
              "variables": {
                "input": {
                  "teamId": "998946a2-aa75-491b-a39d-189660131392",
                  "title": "[CI] Test Count Verification failed on main",
                  "description": "Test count verification failed after merge to main - tests may have been accidentally deleted.\n\n**Commit:** ${{ github.sha }}\n**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\nPlease investigate and restore any missing tests.",
                  "priority": 2
                }
              }
            }'

  # Security Scanning Jobs (Matrix Strategy for Parallelization)

  # Consolidated Trivy scans using matrix for parallel execution
  trivy-scan:
    name: Trivy Scan (${{ matrix.target.name }})
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        target:
          - name: backend-deps
            scan-type: fs
            scan-ref: '.'
            skip-dirs: 'frontend,node_modules,.git'
            is-dockerfile: false
          - name: backend-dockerfile
            scan-type: config
            scan-ref: 'backend/Dockerfile'
            skip-dirs: ''
            is-dockerfile: true
          - name: frontend-deps
            scan-type: fs
            scan-ref: 'frontend'
            skip-dirs: ''
            is-dockerfile: false
          - name: frontend-dockerfile
            scan-type: config
            scan-ref: 'frontend/Dockerfile'
            skip-dirs: ''
            is-dockerfile: true
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Run Trivy vulnerability scanner
        if: ${{ !matrix.target.is-dockerfile }}
        uses: aquasecurity/trivy-action@b6643a29fecd7f34b3597bc6acb0a98b03d33ff8 # 0.33.1
        with:
          scan-type: ${{ matrix.target.scan-type }}
          scan-ref: ${{ matrix.target.scan-ref }}
          scanners: 'vuln'
          severity: 'HIGH,CRITICAL'
          exit-code: '1'
          ignore-unfixed: true
          format: 'table'
          skip-dirs: ${{ matrix.target.skip-dirs }}

      - name: Run Trivy config scanner
        if: ${{ matrix.target.is-dockerfile }}
        uses: aquasecurity/trivy-action@b6643a29fecd7f34b3597bc6acb0a98b03d33ff8 # 0.33.1
        with:
          scan-type: ${{ matrix.target.scan-type }}
          scan-ref: ${{ matrix.target.scan-ref }}
          exit-code: '1'
          severity: 'HIGH,CRITICAL'
          format: 'table'

  npm-audit:
    name: npm Audit (Frontend)
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up Node.js
        uses: actions/setup-node@395ad3262231945c25e8478fd5baf05154b1d79f # v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: cd frontend && npm ci

      - name: Run npm audit
        run: cd frontend && npm audit --audit-level=high
        continue-on-error: false

  security-tests:
    name: Security Test Suite
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres # pragma: allowlist secret
          POSTGRES_DB: security_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      TEST_DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/security_test # pragma: allowlist secret
      REDIS_URL: redis://localhost:6379
      REDIS_HOST: localhost
      REDIS_PORT: 6379
      LOG_DB_ENABLED: 'false'
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Set up uv
        uses: astral-sh/setup-uv@e4db8464a088ece1b920f60402e813ea4de65b8f # v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev --frozen

      - name: Run security tests
        run: |
          uv run pytest backend/tests/security/ \
            -n0 \
            --timeout=30 \
            -v \
            --tb=short

  # TDD Compliance Tracking (PR-only, non-blocking)
  # Analyzes commit patterns to detect TDD practice compliance
  # Reports warnings for potential test-after-implementation patterns
  tdd-compliance:
    name: TDD Compliance Tracking
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    continue-on-error: true
    permissions:
      pull-requests: write
      contents: read
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4
        with:
          fetch-depth: 0

      - name: Analyze TDD compliance
        id: tdd-analysis
        run: |
          echo "## TDD Compliance Analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Get base and head commits for PR
          BASE_SHA="${{ github.event.pull_request.base.sha }}"
          HEAD_SHA="${{ github.event.pull_request.head.sha }}"

          # Count commits in PR
          COMMIT_COUNT=$(git rev-list --count $BASE_SHA..$HEAD_SHA)
          echo "**PR Commits:** $COMMIT_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Analyze each commit for test vs implementation file changes
          echo "### Commit Analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          TDD_COMPLIANT=0
          TDD_WARNING=0
          MIXED_COMMITS=0

          for commit in $(git rev-list $BASE_SHA..$HEAD_SHA); do
            COMMIT_MSG=$(git log -1 --pretty=format:"%s" $commit)
            COMMIT_SHORT=$(git rev-parse --short $commit)

            # Get changed files in commit
            TEST_FILES=$(git diff-tree --no-commit-id --name-only -r $commit | grep -E "(test_|\.test\.|\.spec\.)" | wc -l)
            IMPL_FILES=$(git diff-tree --no-commit-id --name-only -r $commit | grep -v -E "(test_|\.test\.|\.spec\.)" | grep -E "\.(py|ts|tsx)$" | wc -l)

            # Determine pattern
            if [ "$TEST_FILES" -gt 0 ] && [ "$IMPL_FILES" -eq 0 ]; then
              echo " **$COMMIT_SHORT**: Tests only - \`$COMMIT_MSG\`" >> $GITHUB_STEP_SUMMARY
              TDD_COMPLIANT=$((TDD_COMPLIANT + 1))
            elif [ "$TEST_FILES" -gt 0 ] && [ "$IMPL_FILES" -gt 0 ]; then
              echo "  **$COMMIT_SHORT**: Mixed (tests: $TEST_FILES, impl: $IMPL_FILES) - \`$COMMIT_MSG\`" >> $GITHUB_STEP_SUMMARY
              MIXED_COMMITS=$((MIXED_COMMITS + 1))
            elif [ "$TEST_FILES" -eq 0 ] && [ "$IMPL_FILES" -gt 0 ]; then
              echo "  **$COMMIT_SHORT**: Implementation only - \`$COMMIT_MSG\`" >> $GITHUB_STEP_SUMMARY
              TDD_WARNING=$((TDD_WARNING + 1))
            else
              echo "  **$COMMIT_SHORT**: Other changes - \`$COMMIT_MSG\`" >> $GITHUB_STEP_SUMMARY
            fi
          done

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "-  Test-only commits: $TDD_COMPLIANT" >> $GITHUB_STEP_SUMMARY
          echo "-   Mixed commits: $MIXED_COMMITS" >> $GITHUB_STEP_SUMMARY
          echo "-   Implementation-only commits: $TDD_WARNING" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Calculate test-to-implementation ratio
          TOTAL_TEST_FILES=$(git diff --name-only $BASE_SHA...$HEAD_SHA | grep -E "(test_|\.test\.|\.spec\.)" | wc -l)
          TOTAL_IMPL_FILES=$(git diff --name-only $BASE_SHA...$HEAD_SHA | grep -v -E "(test_|\.test\.|\.spec\.)" | grep -E "\.(py|ts|tsx)$" | wc -l)

          echo "### PR File Changes" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- Test files changed: $TOTAL_TEST_FILES" >> $GITHUB_STEP_SUMMARY
          echo "- Implementation files changed: $TOTAL_IMPL_FILES" >> $GITHUB_STEP_SUMMARY

          if [ "$TOTAL_IMPL_FILES" -gt 0 ]; then
            RATIO=$(awk "BEGIN {printf \"%.2f\", $TOTAL_TEST_FILES / $TOTAL_IMPL_FILES}")
            echo "- Test-to-implementation ratio: **${RATIO}:1**" >> $GITHUB_STEP_SUMMARY

            if (( $(echo "$RATIO < 0.5" | bc -l) )); then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "  **Warning**: Low test-to-implementation ratio. Consider adding more test coverage." >> $GITHUB_STEP_SUMMARY
            fi
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### TDD Best Practices Reminder" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "1. **RED**: Write failing tests first" >> $GITHUB_STEP_SUMMARY
          echo "2. **GREEN**: Implement minimum code to pass tests" >> $GITHUB_STEP_SUMMARY
          echo "3. **REFACTOR**: Improve code while keeping tests green" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Mixed commits (tests + implementation) may indicate test-after-implementation." >> $GITHUB_STEP_SUMMARY
          echo "Consider separate commits for test creation and implementation phases." >> $GITHUB_STEP_SUMMARY

          # Output for comment
          echo "tdd_compliant=$TDD_COMPLIANT" >> $GITHUB_OUTPUT
          echo "tdd_warning=$TDD_WARNING" >> $GITHUB_OUTPUT
          echo "mixed_commits=$MIXED_COMMITS" >> $GITHUB_OUTPUT
          echo "test_files=$TOTAL_TEST_FILES" >> $GITHUB_OUTPUT
          echo "impl_files=$TOTAL_IMPL_FILES" >> $GITHUB_OUTPUT

  # CI Gate - Aggregates all required checks for branch protection
  # This job provides a single status check that depends on all required jobs.
  # Use this as the required check in branch protection to simplify configuration.
  # Non-blocking workflows can use workflow_run trigger on CI completion.
  ci-gate:
    name: CI Gate (Required Checks)
    runs-on: ubuntu-latest
    needs:
      - lint
      - typecheck
      - unit-tests
      - integration-tests-summary
      - frontend-lint
      - frontend-typecheck
      - frontend-tests-summary
      - frontend-e2e-summary
      - api-types-check
    if: always()
    steps:
      - name: Check required job results
        run: |
          echo "## Required CI Checks Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check each required job
          FAILED=false

          check_job() {
            local name="$1"
            local result="$2"
            if [ "$result" == "success" ] || [ "$result" == "skipped" ]; then
              echo " $name: $result" >> $GITHUB_STEP_SUMMARY
            else
              echo " $name: $result" >> $GITHUB_STEP_SUMMARY
              FAILED=true
            fi
          }

          check_job "Backend Lint (Ruff)" "${{ needs.lint.result }}"
          check_job "Backend Type Check (Mypy)" "${{ needs.typecheck.result }}"
          check_job "Backend Unit Tests" "${{ needs.unit-tests.result }}"
          check_job "Backend Integration Tests" "${{ needs.integration-tests-summary.result }}"
          check_job "Frontend Lint (ESLint)" "${{ needs.frontend-lint.result }}"
          check_job "Frontend Type Check (TypeScript)" "${{ needs.frontend-typecheck.result }}"
          check_job "Frontend Tests (Vitest)" "${{ needs.frontend-tests-summary.result }}"
          check_job "E2E Tests (Chromium)" "${{ needs.frontend-e2e-summary.result }}"
          check_job "API Types Contract Check" "${{ needs.api-types-check.result }}"

          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "$FAILED" == "true" ]; then
            echo "**Status:  FAILED** - One or more required checks failed" >> $GITHUB_STEP_SUMMARY
            exit 1
          else
            echo "**Status:  PASSED** - All required checks passed" >> $GITHUB_STEP_SUMMARY
          fi
