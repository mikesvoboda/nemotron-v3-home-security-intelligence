# Prometheus configuration for Home Security Intelligence
# Scrapes native Prometheus metrics from backend /api/metrics endpoint
# Also scrapes JSON endpoints via JSON exporter for health/telemetry/stats

global:
  scrape_interval: 15s
  evaluation_interval: 15s

# Alerting rules for AI pipeline monitoring
# See prometheus_rules.yml for rule definitions and severity documentation
rule_files:
  - 'prometheus_rules.yml'
  - 'prometheus-rules.yml'
  - 'alerting-rules.yml'
  - 'profiling-recording-rules.yml'
  - 'profiling-regression-alerts.yml'
  - 'gpu-alerts.yml'

# Alertmanager configuration for alert delivery
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093
      timeout: 10s
      api_version: v2

scrape_configs:
  # ==========================================================================
  # Direct Native Prometheus Metrics Scraping (Preferred)
  # ==========================================================================
  # Scrapes the native Prometheus metrics endpoint directly.
  # This is the recommended approach as it:
  # - Preserves native histogram buckets for accurate percentile calculations
  # - Eliminates the JSON exporter as an intermediate failure point
  # - Reduces latency in metrics collection
  # - Properly types all metrics (counters, gauges, histograms)
  - job_name: 'hsi-backend-metrics'
    metrics_path: /api/metrics
    scrape_interval: 15s
    static_configs:
      - targets:
          - 'backend:8000'
    # Relabel to add service identification
    relabel_configs:
      - target_label: service
        replacement: 'home-security-intelligence'

  # ==========================================================================
  # AI Service Native Prometheus Metrics
  # ==========================================================================
  # Nemotron (llama.cpp) exposes native Prometheus metrics when started with
  # --metrics flag. Includes token counts, timing, and inference statistics.
  #
  # Key metrics:
  # - llama_tokens_predicted_total: Total tokens generated
  # - llama_tokens_prompt_total: Total tokens in prompts
  # - llama_generation_time_seconds: Generation time histogram
  # - llama_prompt_eval_time_seconds: Prompt evaluation time histogram
  # - llama_kv_cache_usage_ratio: KV cache utilization
  # - llama_requests_processing: Currently processing requests
  #
  # Calculate tokens per second: rate(llama_tokens_predicted_total[1m])
  - job_name: 'ai-llm-metrics'
    metrics_path: /metrics
    scrape_interval: 15s
    scrape_timeout: 10s
    static_configs:
      - targets:
          - 'ai-llm:8091'
        labels:
          service: nemotron
          model_type: llm
    # Handle case where container is not running
    honor_labels: true

  # YOLO26 Object Detection metrics (ai-yolo26)
  # Key metrics:
  # - yolo26_inference_requests_total: Total inference requests by endpoint/status
  # - yolo26_inference_latency_seconds: Inference latency histogram
  # - yolo26_detections_per_image: Number of detections per image
  # - yolo26_model_loaded: Model status (1=loaded)
  # - yolo26_gpu_*: GPU utilization, memory, temperature, power
  - job_name: 'ai-yolo26-metrics'
    metrics_path: /metrics
    scrape_interval: 15s
    scrape_timeout: 10s
    static_configs:
      - targets:
          - 'ai-yolo26:8095'
        labels:
          service: yolo26
          model_type: detection
    honor_labels: true

  # Florence-2 Vision-Language metrics (ai-florence)
  # Key metrics:
  # - florence_inference_requests_total: Total inference requests by endpoint/status
  # - florence_inference_latency_seconds: Inference latency histogram
  # - florence_model_loaded: Model status (1=loaded)
  # - florence_gpu_memory_used_gb: GPU memory usage
  - job_name: 'ai-florence-metrics'
    metrics_path: /metrics
    scrape_interval: 15s
    scrape_timeout: 10s
    static_configs:
      - targets:
          - 'ai-florence:8092'
        labels:
          service: florence
          model_type: vlm
    honor_labels: true

  # CLIP Embedding metrics (ai-clip)
  # Key metrics:
  # - clip_inference_requests_total: Total inference requests by endpoint/status
  # - clip_inference_latency_seconds: Inference latency histogram
  # - clip_model_loaded: Model status (1=loaded)
  # - clip_gpu_memory_used_gb: GPU memory usage
  - job_name: 'ai-clip-metrics'
    metrics_path: /metrics
    scrape_interval: 15s
    scrape_timeout: 10s
    static_configs:
      - targets:
          - 'ai-clip:8093'
        labels:
          service: clip
          model_type: embedding
    honor_labels: true

  # Enrichment Service metrics (ai-enrichment)
  # Combined service for vehicle, pet, clothing classification and depth estimation
  # Key metrics:
  # - enrichment_inference_requests_total: Total requests by endpoint/status
  # - enrichment_inference_latency_seconds: Latency histogram by endpoint
  # - enrichment_*_model_loaded: Model status for vehicle/pet/clothing/depth
  # - enrichment_gpu_memory_used_gb: GPU memory usage
  - job_name: 'ai-enrichment-metrics'
    metrics_path: /metrics
    scrape_interval: 15s
    scrape_timeout: 10s
    static_configs:
      - targets:
          - 'ai-enrichment:8094'
        labels:
          service: enrichment
          model_type: classification
    honor_labels: true

  # YOLO26 TensorRT Object Detection metrics (ai-yolo26)
  # Alternative detector to YOLO26 with 5.3x faster inference
  # ==========================================================================
  # JSON Exporter Scraped Endpoints (For non-Prometheus format data)
  # ==========================================================================
  # These endpoints return JSON data that needs conversion to Prometheus format.
  # Used for health checks, telemetry summaries, and stats that don't have
  # native Prometheus equivalents.

  # Backend health metrics via JSON exporter
  - job_name: 'hsi-health'
    metrics_path: /probe
    scrape_interval: 10s
    params:
      module: [health]
    static_configs:
      - targets:
          - 'http://backend:8000/api/system/health'
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: json-exporter:7979

  # Backend telemetry metrics via JSON exporter
  - job_name: 'hsi-telemetry'
    metrics_path: /probe
    scrape_interval: 10s
    params:
      module: [telemetry]
    static_configs:
      - targets:
          - 'http://backend:8000/api/system/telemetry'
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: json-exporter:7979

  # Backend stats metrics via JSON exporter
  - job_name: 'hsi-stats'
    metrics_path: /probe
    scrape_interval: 30s
    params:
      module: [stats]
    static_configs:
      - targets:
          - 'http://backend:8000/api/system/stats'
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: json-exporter:7979

  # GPU stats via JSON exporter
  - job_name: 'hsi-gpu'
    metrics_path: /probe
    scrape_interval: 10s
    params:
      module: [gpu]
    static_configs:
      - targets:
          - 'http://backend:8000/api/system/gpu'
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: json-exporter:7979

  # ==========================================================================
  # LLM (llama.cpp) Native Prometheus Metrics
  # ==========================================================================
  # Scrapes native Prometheus metrics from the Nemotron/llama.cpp server.
  # llama.cpp exposes metrics at /metrics when started with --metrics flag.
  # Key metrics for LLM performance monitoring:
  #   - llama_tokens_predicted_total: Total tokens generated
  #   - llama_generation_time_seconds_bucket: Token generation latency histogram
  #   - llama_prompt_eval_time_seconds_bucket: Prompt evaluation latency histogram
  #   - llama_requests_processing: Currently processing requests
  #   - llama_kv_cache_usage_ratio: KV cache utilization
  - job_name: 'llama-cpp-metrics'
    metrics_path: /metrics
    scrape_interval: 15s
    scrape_timeout: 10s
    static_configs:
      - targets:
          - 'ai-llm:8091'
    relabel_configs:
      - target_label: service
        replacement: 'nemotron-llm'
      - target_label: ai_model
        replacement: 'nemotron-3-nano-30b'

  # Redis metrics via redis_exporter
  - job_name: 'redis'
    scrape_interval: 15s
    static_configs:
      - targets:
          - 'redis-exporter:9121'

  # Prometheus self-monitoring
  - job_name: 'prometheus'
    static_configs:
      - targets:
          - 'localhost:9090'

  # JSON exporter health
  - job_name: 'json-exporter'
    static_configs:
      - targets:
          - 'json-exporter:7979'

  # Alertmanager metrics
  # Exposes alertmanager_alerts, alertmanager_notifications_*, alertmanager_config_*
  - job_name: 'alertmanager'
    scrape_interval: 15s
    static_configs:
      - targets:
          - 'alertmanager:9093'

  # =============================================================================
  # Blackbox Exporter - Synthetic Monitoring (NEM-1637)
  # =============================================================================
  # External endpoint probing for availability, latency, and health validation
  # Access probe UI at http://blackbox-exporter:9115

  # Blackbox exporter self-monitoring
  - job_name: 'blackbox-exporter'
    static_configs:
      - targets:
          - 'blackbox-exporter:9115'

  # ==========================================================================
  # Pyroscope Continuous Profiling Metrics (NEM-3925)
  # ==========================================================================
  # Scrapes internal metrics from Pyroscope server for monitoring profiling health.
  # Key metrics:
  #   - pyroscope_distributor_received_samples_total: Total samples received
  #   - pyroscope_ingester_profiles_received_total: Total profiles ingested
  #   - pyroscope_distributor_bytes_received_total: Bytes received from agents
  #   - up{job="pyroscope"}: Server availability (1 = healthy, 0 = down)
  - job_name: 'pyroscope'
    metrics_path: /metrics
    scrape_interval: 15s
    scrape_timeout: 10s
    static_configs:
      - targets:
          - 'pyroscope:4040'
        labels:
          service: pyroscope
          component: profiling
    honor_labels: true

  # HTTP Health probes - validates JSON health response body
  - job_name: 'blackbox-http-health'
    metrics_path: /probe
    params:
      module: [http_health]
    scrape_interval: 15s
    scrape_timeout: 10s
    static_configs:
      - targets:
          - http://backend:8000/api/system/health
        labels:
          probe_type: health
          service: backend
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter:9115

  # HTTP Readiness probes - stricter check for load balancing decisions
  - job_name: 'blackbox-http-ready'
    metrics_path: /probe
    params:
      module: [http_ready]
    scrape_interval: 15s
    scrape_timeout: 10s
    static_configs:
      - targets:
          - http://backend:8000/api/system/health/ready
        labels:
          probe_type: readiness
          service: backend
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter:9115

  # HTTP Liveness probes - simple availability checks
  # This is the canonical backend liveness check (replaces removed backend-liveness job)
  # The /health endpoint returns JSON, so we use blackbox-exporter for proper probing
  - job_name: 'blackbox-http-live'
    metrics_path: /probe
    params:
      module: [http_live]
    scrape_interval: 10s
    scrape_timeout: 5s
    static_configs:
      - targets:
          - http://backend:8000/health
          - http://frontend:8080
        labels:
          probe_type: liveness
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter:9115
      # Extract service name from URL for labeling
      - source_labels: [__param_target]
        regex: 'http://([^:]+):.*'
        target_label: service
        replacement: '${1}'

  # HTTP 2xx probes - basic availability for AI services
  - job_name: 'blackbox-http-2xx'
    metrics_path: /probe
    params:
      module: [http_2xx]
    scrape_interval: 30s
    scrape_timeout: 5s
    static_configs:
      - targets:
          - http://ai-llm:8091/health
          - http://ai-florence:8092/health
          - http://ai-clip:8093/health
          - http://ai-enrichment:8094/health
          - http://ai-yolo26:8095/health
        labels:
          probe_type: availability
          service_type: ai
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter:9115
      # Extract AI service name from URL
      - source_labels: [__param_target]
        regex: 'http://([^:]+):.*'
        target_label: ai_service
        replacement: '${1}'

  # TCP connectivity probes - database and cache connections
  - job_name: 'blackbox-tcp'
    metrics_path: /probe
    params:
      module: [tcp_connect]
    scrape_interval: 15s
    scrape_timeout: 5s
    static_configs:
      - targets:
          - postgres:5432
          - redis:6379
        labels:
          probe_type: tcp
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter:9115
      # Extract service name from target
      - source_labels: [__param_target]
        regex: '([^:]+):.*'
        target_label: service
        replacement: '${1}'

  # ==========================================================================
  # NVIDIA DCGM Exporter - GPU Hardware Metrics (NEM-4132)
  # ==========================================================================
  # Scrapes low-level GPU hardware metrics from NVIDIA DCGM (Data Center GPU Manager).
  # DCGM provides detailed metrics that nvidia-smi cannot:
  #   - SM (Streaming Multiprocessor) utilization: actual compute usage
  #   - Memory bandwidth utilization: memory-bound vs compute-bound analysis
  #   - Power consumption: power efficiency monitoring
  #   - Temperature: thermal throttling detection
  #   - PCIe throughput: host<->device transfer rates
  #
  # Key metrics:
  #   - DCGM_FI_DEV_GPU_UTIL: GPU utilization percentage
  #   - DCGM_FI_DEV_MEM_COPY_UTIL: Memory bandwidth utilization
  #   - DCGM_FI_DEV_FB_USED: Framebuffer (VRAM) memory used
  #   - DCGM_FI_DEV_FB_FREE: Framebuffer memory free
  #   - DCGM_FI_DEV_GPU_TEMP: GPU temperature in Celsius
  #   - DCGM_FI_DEV_POWER_USAGE: Power consumption in Watts
  #   - DCGM_FI_DEV_SM_CLOCK: SM clock frequency
  #   - DCGM_FI_DEV_MEM_CLOCK: Memory clock frequency
  #   - DCGM_FI_DEV_PCIE_TX_THROUGHPUT: PCIe TX bytes/sec
  #   - DCGM_FI_DEV_PCIE_RX_THROUGHPUT: PCIe RX bytes/sec
  - job_name: 'dcgm-exporter'
    static_configs:
      - targets: ['dcgm-exporter:9400']
    scrape_interval: 15s
    scrape_timeout: 10s
