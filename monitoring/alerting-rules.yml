# Alerting Rules for Home Security Intelligence
# Defines critical, warning, and info alerts for pipeline health and anomaly detection

groups:
  # Pipeline Health Alerts
  - name: pipeline_health_alerts
    rules:
      # Critical: Pipeline is completely down
      - alert: HSIPipelineDown
        expr: probe_success{job="blackbox-http-live", service="backend"} == 0
        for: 1m
        labels:
          severity: critical
          component: pipeline
        annotations:
          summary: 'HSI Pipeline is down'
          description: 'The HSI backend service has been unreachable for more than 1 minute.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/pipeline-down'

      # Critical: Pipeline health check failing
      # Uses hsi_system_healthy_unhealthy gauge (1 when unhealthy, 0 when healthy)
      - alert: HSIPipelineUnhealthy
        expr: hsi_system_healthy_unhealthy > 0
        for: 2m
        labels:
          severity: critical
          component: pipeline
        annotations:
          summary: 'HSI Pipeline health check failing'
          description: 'The HSI pipeline health check has been failing for more than 2 minutes.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/pipeline-unhealthy'

  # Database Alerts
  - name: database_alerts
    rules:
      # Critical: Database connection failures
      # Uses hsi_database_healthy_unhealthy gauge (1 when unhealthy, 0 when healthy)
      - alert: HSIDatabaseUnhealthy
        expr: hsi_database_healthy_unhealthy > 0
        for: 2m
        labels:
          severity: critical
          component: database
        annotations:
          summary: 'HSI Database is unhealthy'
          description: 'Database health check has been failing for more than 2 minutes.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/database-unhealthy'

      # Warning: Database query latency high
      - alert: HSIDatabaseSlowQueries
        expr: |
          histogram_quantile(0.95, rate(hsi_db_query_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: 'HSI Database queries are slow'
          description: 'P95 database query latency exceeds 1 second for more than 5 minutes.'

  # Redis Alerts
  - name: redis_alerts
    rules:
      # Critical: Redis unavailable
      - alert: HSIRedisUnhealthy
        expr: up{job="redis"} == 0
        for: 2m
        labels:
          severity: critical
          component: redis
        annotations:
          summary: 'HSI Redis is unhealthy'
          description: 'Redis has been unreachable for more than 2 minutes.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/redis-unhealthy'

      # Warning: Redis memory high
      # Note: Only fires when maxmemory is configured (> 0), avoids false positives from division by zero
      - alert: HSIRedisMemoryHigh
        expr: redis_memory_max_bytes > 0 and redis_memory_used_bytes / redis_memory_max_bytes > 0.8
        for: 5m
        labels:
          severity: warning
          component: redis
        annotations:
          summary: 'HSI Redis memory usage high'
          description: 'Redis memory usage is above 80% for more than 5 minutes.'

      # Warning: Redis slow commands detected (>10ms)
      # Monitors redis_slowlog_length which counts commands exceeding slowlog-log-slower-than threshold (10000 microseconds = 10ms)
      # Alert fires when new slow commands are being added to the log
      - alert: HSIRedisSlowCommands
        expr: increase(redis_slowlog_length[5m]) > 0
        for: 2m
        labels:
          severity: warning
          component: redis
        annotations:
          summary: 'HSI Redis slow commands detected'
          description: 'Redis has logged {{ $value | printf "%.0f" }} slow commands (>10ms) in the last 5 minutes. Investigate command patterns causing latency.'

      # Critical: High rate of slow Redis commands
      # Fires when slow commands are accumulating rapidly
      - alert: HSIRedisSlowCommandsCritical
        expr: increase(redis_slowlog_length[5m]) > 10
        for: 2m
        labels:
          severity: critical
          component: redis
        annotations:
          summary: 'HSI Redis experiencing many slow commands'
          description: 'Redis has logged {{ $value | printf "%.0f" }} slow commands (>10ms) in the last 5 minutes. This indicates serious Redis performance degradation.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/redis-slow-commands'

  # GPU Alerts
  - name: gpu_alerts
    rules:
      # Warning: GPU memory elevated (early warning)
      - alert: HSIGPUMemoryElevated
        expr: hsi:gpu:memory_utilization > 0.75
        for: 10m
        labels:
          severity: warning
          component: gpu
        annotations:
          summary: 'HSI GPU memory usage elevated'
          description: 'GPU memory utilization is above 75% for more than 10 minutes.'

      # Critical: GPU memory critically high
      - alert: HSIGPUMemoryHigh
        expr: hsi:gpu:memory_utilization > 0.9
        for: 5m
        labels:
          severity: critical
          component: gpu
        annotations:
          summary: 'HSI GPU memory usage critically high'
          description: 'GPU memory utilization is above 90% for more than 5 minutes. Risk of OOM errors.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/gpu-memory-high'

      # Warning: GPU utilization low (potential issue)
      # Uses hsi_total_detections (not hsi_detection_total which doesn't exist)
      - alert: HSIGPUUtilizationLow
        expr: hsi:gpu:utilization < 10 and hsi_total_detections > 0
        for: 15m
        labels:
          severity: warning
          component: gpu
        annotations:
          summary: 'HSI GPU utilization unexpectedly low'
          description: 'GPU utilization is below 10% while detections are happening. May indicate processing issues.'

      # Critical: GPU temperature throttling
      - alert: AIGPUThrottling
        expr: hsi_gpu_temperature > 83
        for: 1m
        labels:
          severity: critical
          component: gpu
        annotations:
          summary: 'GPU temperature causing thermal throttling'
          description: 'GPU temperature is {{ $value }}°C, exceeding the 83°C throttling threshold. Performance is degraded.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/gpu-throttling'

      # Warning: GPU clock degradation (>20% below baseline)
      # Uses hsi_gpu_sm_clock_mhz (not hsi_gpu_clock_speed_mhz which doesn't exist)
      - alert: AIGPUClockDegradation
        expr: |
          hsi_gpu_sm_clock_mhz < 1200
          and
          hsi_gpu_utilization > 50
        for: 5m
        labels:
          severity: warning
          component: gpu
        annotations:
          summary: 'GPU clock speed significantly degraded'
          description: 'GPU clock speed is {{ $value }}MHz while under load (>50% utilization). Expected ~1500MHz. Possible thermal or power throttling.'

      # Critical: GPU active throttling (excludes idle throttle)
      # Throttle reason bitmask values:
      #   1 = GPU Idle (normal power-saving, not a problem)
      #   2 = Applications Clocks Setting
      #   4 = SW Power Cap
      #   8 = HW Slowdown (thermal/power - serious)
      # Alert only when throttle_reasons > 1 to exclude idle-only throttling
      - alert: AIGPUActiveThrottle
        expr: hsi_gpu_throttle_reasons > 1
        for: 1m
        labels:
          severity: critical
          component: gpu
        annotations:
          summary: 'GPU is actively throttling'
          description: 'GPU throttle reasons bitmask is {{ $value }} (excludes idle=1). Check for thermal, power, or other throttling. Common values: 4=SW Power Cap, 8=HW Slowdown.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/gpu-throttling'

      # Critical: GPU PCIe replay errors (hardware issues)
      - alert: AIGPUPCIeErrors
        expr: increase(hsi_gpu_pcie_replay_counter[5m]) > 10
        for: 1m
        labels:
          severity: critical
          component: gpu
        annotations:
          summary: 'GPU PCIe replay errors increasing'
          description: 'GPU experiencing {{ $value }} PCIe replay errors in 5 minutes. Indicates hardware/bus issues.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/gpu-pcie-errors'

      # Critical: GPU fan failure (0% when temp high)
      - alert: AIGPUFanFailure
        expr: hsi_gpu_fan_speed == 0 and hsi_gpu_temperature > 50
        for: 2m
        labels:
          severity: critical
          component: gpu
        annotations:
          summary: 'GPU fan appears to have failed'
          description: 'GPU fan speed is 0% while temperature is {{ $value }}°C. Risk of thermal damage.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/gpu-fan-failure'

      # Warning: GPU not boosting under load (P-state stuck high)
      - alert: AIGPUNotBoosting
        expr: |
          hsi_gpu_pstate > 2
          and
          hsi_gpu_utilization > 50
        for: 5m
        labels:
          severity: warning
          component: gpu
        annotations:
          summary: 'GPU not boosting under load'
          description: 'GPU is in P-state {{ $value }} (low power) despite >50% utilization. Performance may be limited.'

  # Queue Alerts
  - name: queue_alerts
    rules:
      # Warning: Detection queue backing up
      - alert: HSIDetectionQueueHigh
        expr: hsi_detection_queue_depth > 100
        for: 5m
        labels:
          severity: warning
          component: detection
        annotations:
          summary: 'HSI Detection queue is backing up'
          description: 'More than 100 items in the detection queue for more than 5 minutes.'

      # Warning: Analysis queue backing up
      - alert: HSIAnalysisQueueHigh
        expr: hsi_analysis_queue_depth > 50
        for: 5m
        labels:
          severity: warning
          component: analysis
        annotations:
          summary: 'HSI Analysis queue is backing up'
          description: 'More than 50 items in the analysis queue for more than 5 minutes.'

      # Critical: Queues severely backed up
      - alert: HSIQueueCritical
        expr: hsi_detection_queue_depth > 500 or hsi_analysis_queue_depth > 200
        for: 2m
        labels:
          severity: critical
          component: queue
        annotations:
          summary: 'HSI Processing queues critically backed up'
          description: 'Detection or analysis queue has reached critical levels. Processing may be stalled.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/queue-critical'

  # Latency Alerts
  # NOTE: Disabled - depends on hsi:detection_latency:p95_5m and hsi:analysis_latency:p95_5m
  # recording rules which require hsi_stage_duration_seconds_bucket metrics (not implemented)
  # Re-enable when metrics are added to backend/core/metrics.py
  # - name: latency_alerts
  #   rules:
  #     - alert: HSISlowDetection
  #       expr: hsi:detection_latency:p95_5m > 2
  #       ...
  #     - alert: HSISlowAnalysis
  #       expr: hsi:analysis_latency:p95_5m > 30
  #       ...
  #     - alert: HSIExtremeLatency
  #       expr: hsi:detection_latency:p95_5m > 10 or hsi:analysis_latency:p95_5m > 120
  #       ...

  # Error Rate Alerts
  - name: error_rate_alerts
    rules:
      # Warning: Elevated error rate
      - alert: HSIHighErrorRate
        expr: |
          1 - hsi:api_requests:success_rate_5m > 0.05
        for: 5m
        labels:
          severity: warning
          component: api
        annotations:
          summary: 'HSI API error rate elevated'
          description: 'API error rate is above 5% for more than 5 minutes.'

      # Critical: High error rate
      - alert: HSICriticalErrorRate
        expr: |
          1 - hsi:api_requests:success_rate_5m > 0.1
        for: 2m
        labels:
          severity: critical
          component: api
        annotations:
          summary: 'HSI API error rate critical'
          description: 'API error rate is above 10% for more than 2 minutes.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/high-error-rate'

      # NOTE: Detection and analysis failure rate alerts disabled
      # These depend on hsi:detection:success_rate_5m and hsi:analysis:success_rate_5m recording rules
      # which require hsi_pipeline_errors_total to have a 'stage' label (it has 'error_type' instead)
      # Re-enable when metrics are updated in backend/core/metrics.py
      # - alert: HSIDetectionFailureRate
      #   expr: 1 - hsi:detection:success_rate_5m > 0.1
      #   for: 5m
      #   ...
      # - alert: HSIAnalysisFailureRate
      #   expr: 1 - hsi:analysis:success_rate_5m > 0.1
      #   for: 5m
      #   ...

  # SLO Burn Rate Alerts (Multi-window)
  - name: slo_burn_rate_alerts
    rules:
      # Critical: Fast burn on API availability (14.4x burn rate over 1h)
      - alert: HSIAPIAvailabilityFastBurn
        expr: |
          hsi:burn_rate:api_availability_1h > 14.4
          and
          hsi:burn_rate:api_availability_6h > 6
        for: 2m
        labels:
          severity: critical
          component: slo
          slo: api_availability
        annotations:
          summary: 'HSI API Availability SLO fast burn detected'
          description: 'API availability is burning through error budget at 14.4x rate. Will exhaust 30-day budget in ~2 hours.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/slo-fast-burn'

      # Warning: Slow burn on API availability (3x burn rate over 1d)
      - alert: HSIAPIAvailabilitySlowBurn
        expr: |
          hsi:burn_rate:api_availability_1d > 3
        for: 1h
        labels:
          severity: warning
          component: slo
          slo: api_availability
        annotations:
          summary: 'HSI API Availability SLO slow burn detected'
          description: 'API availability is burning through error budget at 3x rate. Will exhaust 30-day budget in ~10 days.'

      # NOTE: Detection and analysis latency burn rate alerts disabled
      # Depends on hsi:burn_rate:detection_latency_* and hsi:burn_rate:analysis_latency_*
      # recording rules which require hsi_stage_duration_seconds_bucket metrics (not implemented)
      # - alert: HSIDetectionLatencyFastBurn
      #   expr: hsi:burn_rate:detection_latency_1h > 14.4 and hsi:burn_rate:detection_latency_6h > 6
      #   ...
      # - alert: HSIAnalysisLatencyFastBurn
      #   expr: hsi:burn_rate:analysis_latency_1h > 14.4 and hsi:burn_rate:analysis_latency_6h > 6
      #   ...

  # WebSocket Alerts
  # NOTE: Disabled - WebSocket metrics (hsi_websocket_*) not yet implemented
  # Re-enable when metrics are added to backend/core/metrics.py
  # - name: websocket_alerts
  #   rules:
  #     # Warning: WebSocket connection failures
  #     - alert: HSIWebSocketConnectionFailures
  #       expr: hsi:websocket:connection_success_rate_5m < 0.95
  #       for: 5m
  #       labels:
  #         severity: warning
  #         component: websocket
  #       annotations:
  #         summary: 'HSI WebSocket connection failures elevated'
  #         description: 'WebSocket connection success rate is below 95% for more than 5 minutes.'
  #
  #     # Critical: WebSocket completely failing
  #     - alert: HSIWebSocketDown
  #       expr: hsi:websocket:connection_success_rate_5m < 0.5
  #       for: 2m
  #       labels:
  #         severity: critical
  #         component: websocket
  #       annotations:
  #         summary: 'HSI WebSocket service degraded'
  #         description: 'More than 50% of WebSocket connections are failing.'
  #         runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/websocket-down'
  #
  #     # Info: No active WebSocket connections
  #     - alert: HSINoWebSocketConnections
  #       expr: hsi:websocket:active_connections == 0
  #       for: 30m
  #       labels:
  #         severity: info
  #         component: websocket
  #       annotations:
  #         summary: 'HSI No active WebSocket connections'
  #         description: 'No clients have been connected via WebSocket for 30 minutes. This may be expected during low-traffic periods.'

  # =============================================================================
  # Prometheus Self-Monitoring Alerts (NEM-2468)
  # =============================================================================
  # Monitor Prometheus itself to ensure observability infrastructure is healthy.
  # These alerts help detect issues with the monitoring system before they
  # impact visibility into the main application.
  - name: prometheus_self_monitoring_alerts
    rules:
      # Critical: Prometheus is not scraping itself
      - alert: PrometheusNotScrapingSelf
        expr: up{job="prometheus"} == 0
        for: 2m
        labels:
          severity: critical
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus is not scraping itself'
          description: 'Prometheus self-monitoring target has been down for more than 2 minutes. This indicates Prometheus may be unhealthy or misconfigured.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-self-scrape-down'

      # Critical: Prometheus configuration reload failed
      - alert: PrometheusConfigReloadFailed
        expr: prometheus_config_last_reload_successful == 0
        for: 5m
        labels:
          severity: critical
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus configuration reload failed'
          description: 'Prometheus configuration reload has been failing for more than 5 minutes. New alerting rules or scrape configurations are not being applied.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-config-reload-failed'

      # Warning: Prometheus rule evaluation failures
      - alert: PrometheusRuleEvaluationFailures
        expr: increase(prometheus_rule_evaluation_failures_total[5m]) > 0
        for: 5m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus rule evaluation failures detected'
          description: 'Prometheus has encountered rule evaluation failures in the last 5 minutes. Some recording rules or alerts may not be evaluated correctly.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-rule-eval-failures'

      # Warning: Prometheus rule evaluation slow
      - alert: PrometheusRuleEvaluationSlow
        expr: |
          prometheus_rule_group_last_duration_seconds
          >
          prometheus_rule_group_interval_seconds
        for: 10m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus rule evaluation is taking too long'
          description: 'Prometheus rule group evaluation is taking longer than the group interval for more than 10 minutes. This may cause gaps in alerting and recording rules.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-rule-eval-slow'

      # Critical: Prometheus scrape failures high
      - alert: PrometheusScrapeFailuresHigh
        expr: |
          (
            sum(increase(prometheus_target_scrape_pool_sync_total{status="failed"}[5m]))
            /
            (sum(increase(prometheus_target_scrape_pool_sync_total[5m])) + 0.001)
          ) > 0.1
        for: 5m
        labels:
          severity: critical
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus scrape sync failures are high'
          description: 'More than 10% of Prometheus scrape pool sync operations are failing for more than 5 minutes. Metrics collection may be degraded.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-scrape-failures'

      # Warning: Prometheus targets down
      - alert: PrometheusTargetsUnhealthy
        expr: |
          (
            count(up == 0)
            /
            count(up)
          ) > 0.2
        for: 5m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'High percentage of Prometheus targets are down'
          description: 'More than 20% of Prometheus scrape targets are down for more than 5 minutes. Current unhealthy: {{ $value | humanizePercentage }}.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-targets-unhealthy'

      # Warning: Prometheus notification queue full
      - alert: PrometheusNotificationQueueFull
        expr: prometheus_notifications_queue_length > prometheus_notifications_queue_capacity * 0.9
        for: 5m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus notification queue is nearly full'
          description: 'Prometheus notification queue is above 90% capacity for more than 5 minutes. Alerts may be delayed or dropped.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-notification-queue-full'

      # Critical: Prometheus notification failures
      - alert: PrometheusNotificationsFailing
        expr: increase(prometheus_notifications_errors_total[5m]) > 5
        for: 5m
        labels:
          severity: critical
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus is failing to send notifications'
          description: 'Prometheus has failed to send more than 5 notifications to Alertmanager in the last 5 minutes. Alerts may not be delivered.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-notifications-failing'

      # Warning: Prometheus TSDB compaction failures
      - alert: PrometheusTSDBCompactionsFailing
        expr: increase(prometheus_tsdb_compactions_failed_total[6h]) > 0
        for: 5m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus TSDB compactions are failing'
          description: 'Prometheus TSDB has experienced compaction failures in the last 6 hours. This may lead to increased disk usage and query performance degradation.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-tsdb-compactions-failing'

      # Critical: Prometheus TSDB head truncation failures
      - alert: PrometheusTSDBHeadTruncationsFailing
        expr: increase(prometheus_tsdb_head_truncations_failed_total[1h]) > 0
        for: 5m
        labels:
          severity: critical
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus TSDB head truncations are failing'
          description: 'Prometheus TSDB head truncations have failed. This is a critical issue that may lead to memory exhaustion and data corruption.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-tsdb-head-truncations-failing'

      # Warning: Prometheus TSDB WAL corruptions
      - alert: PrometheusTSDBWALCorruptions
        expr: increase(prometheus_tsdb_wal_corruptions_total[1h]) > 0
        for: 1m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus TSDB WAL corruptions detected'
          description: 'Prometheus TSDB has detected WAL (Write-Ahead Log) corruptions. Some metrics data may be lost.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-tsdb-wal-corruptions'

      # Warning: Prometheus storage is filling up
      - alert: PrometheusStorageFillingUp
        expr: |
          (
            prometheus_tsdb_storage_blocks_bytes
            /
            (prometheus_tsdb_storage_blocks_bytes + prometheus_tsdb_storage_blocks_bytes_free)
          ) > 0.8
        for: 15m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus storage is filling up'
          description: 'Prometheus TSDB storage is more than 80% full for more than 15 minutes. Consider increasing retention or storage capacity.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-storage-filling'

      # Warning: Prometheus query load high
      - alert: PrometheusQueryLoadHigh
        expr: |
          rate(prometheus_engine_query_duration_seconds_sum[5m])
          /
          rate(prometheus_engine_query_duration_seconds_count[5m])
          > 10
        for: 10m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus query load is high'
          description: 'Average Prometheus query duration exceeds 10 seconds for more than 10 minutes. Dashboard and API queries may be slow.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-query-load-high'

      # Info: Prometheus has restarted
      - alert: PrometheusRestarted
        expr: changes(prometheus_build_info[15m]) > 0
        for: 0m
        labels:
          severity: info
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus has restarted'
          description: 'Prometheus instance has restarted within the last 15 minutes. Check for any configuration changes or issues that may have caused the restart.'

      # Warning: Alertmanager connectivity issues
      - alert: PrometheusAlertmanagerDown
        expr: prometheus_notifications_alertmanagers_discovered < 1
        for: 5m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus cannot discover Alertmanager'
          description: 'Prometheus has no discovered Alertmanager instances for more than 5 minutes. Alerts will not be delivered.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-alertmanager-down'

      # Critical: Too many samples rejected
      - alert: PrometheusSamplesRejected
        expr: |
          rate(prometheus_target_scrapes_sample_out_of_order_total[5m]) > 0
          or
          rate(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0
        for: 10m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus is rejecting samples'
          description: 'Prometheus is rejecting samples due to out-of-order timestamps or duplicates. This may indicate clock drift or incorrect metric exposition.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-samples-rejected'

  # =============================================================================
  # Pipeline Worker Health Alerts (NEM-2459)
  # =============================================================================
  # Monitor pipeline worker restarts, failures, and health state.
  # These alerts help detect worker stability issues before they impact
  # the AI processing pipeline.
  - name: pipeline_worker_alerts
    rules:
      # NOTE: HSIWorkerRestartStorm disabled - hsi_pipeline_worker_restarts_total metric not implemented
      # Re-enable when metric is added to backend/core/metrics.py
      # - alert: HSIWorkerRestartStorm
      #   expr: increase(hsi_pipeline_worker_restarts_total[5m]) > 3
      #   ...

      # Critical: Worker in failed state for 30 seconds
      - alert: HSIWorkerFailed
        expr: |
          hsi_pipeline_worker_state == 3
        for: 30s
        labels:
          severity: critical
          component: worker
          service: pipeline
        annotations:
          summary: 'Pipeline worker failed ({{ $labels.worker_name }})'
          description: 'Worker {{ $labels.worker_name }} has been in FAILED state for more than 30 seconds. The worker has exceeded its maximum restart attempts and requires manual intervention.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/worker-failed'

      # Warning: Worker not running for 2 minutes (state != 1)
      - alert: HSIWorkerNotRunning
        expr: |
          hsi_pipeline_worker_state != 1
        for: 2m
        labels:
          severity: warning
          component: worker
          service: pipeline
        annotations:
          summary: 'Pipeline worker not running ({{ $labels.worker_name }})'
          description: 'Worker {{ $labels.worker_name }} has not been in RUNNING state for more than 2 minutes. Current state: {{ $value }} (0=stopped, 1=running, 2=restarting, 3=failed).'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/worker-not-running'

      # NOTE: HSIWorkerRestartSlow disabled - hsi_pipeline_worker_restart_duration_seconds_bucket metric not implemented
      # - alert: HSIWorkerRestartSlow
      #   expr: histogram_quantile(0.95, rate(hsi_pipeline_worker_restart_duration_seconds_bucket[5m])) > 30
      #   ...

      # Warning: High consecutive failure count
      - alert: HSIWorkerConsecutiveFailures
        expr: |
          hsi_pipeline_worker_consecutive_failures >= 3
        for: 1m
        labels:
          severity: warning
          component: worker
          service: pipeline
        annotations:
          summary: 'Pipeline worker has multiple consecutive failures ({{ $labels.worker_name }})'
          description: 'Worker {{ $labels.worker_name }} has {{ $value }} consecutive failures. Worker may be approaching the max restart limit.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/worker-consecutive-failures'

      # NOTE: The following alerts are disabled - hsi_pipeline_worker_restarts_total metric not implemented
      # Re-enable when metric is added to backend/core/metrics.py
      # - alert: HSIWorkerRestarted
      # - alert: HSIWorkerConnectionIssues
      # - alert: HSIWorkerMemoryIssues

      # Critical: All workers in the supervisor are failing
      - alert: HSIAllWorkersFailing
        expr: |
          count(hsi_pipeline_worker_state == 3) == count(hsi_pipeline_worker_state)
          and
          count(hsi_pipeline_worker_state) > 0
        for: 1m
        labels:
          severity: critical
          component: worker
          service: pipeline
        annotations:
          summary: 'All pipeline workers are in failed state'
          description: 'All registered pipeline workers have failed. The AI processing pipeline is completely stopped.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/all-workers-failing'

  # =============================================================================
  # Circuit Breaker Alerts
  # =============================================================================
  # Monitor circuit breaker state to detect service degradation and cascading failures.
  # Circuit breaker states: 0=closed (healthy), 1=open (failing), 2=half-open (testing)
  # Note: Metric uses 'exported_service' label, not 'service'
  - name: circuit_breaker_alerts
    rules:
      # Critical: Circuit breaker is open (service is failing)
      - alert: HSICircuitBreakerOpen
        expr: hsi_circuit_breaker_state == 1
        for: 30s
        labels:
          severity: critical
          component: circuit_breaker
          service: '{{ $labels.exported_service }}'
        annotations:
          summary: 'Circuit breaker OPEN for {{ $labels.exported_service }}'
          description: 'Circuit breaker for {{ $labels.exported_service }} is in OPEN state, rejecting all calls. The downstream service has exceeded the failure threshold.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/circuit-breaker-open'

      # Warning: Circuit breaker stuck in half-open state
      - alert: HSICircuitBreakerHalfOpenStuck
        expr: hsi_circuit_breaker_state == 2
        for: 5m
        labels:
          severity: warning
          component: circuit_breaker
          service: '{{ $labels.exported_service }}'
        annotations:
          summary: 'Circuit breaker stuck in HALF-OPEN for {{ $labels.exported_service }}'
          description: 'Circuit breaker for {{ $labels.exported_service }} has been in HALF-OPEN state for >5 minutes. Recovery probes may be failing consistently.'

      # Critical: High circuit breaker trip rate (>3 trips in 15m)
      - alert: HSICircuitBreakerTripRate
        expr: |
          changes(hsi_circuit_breaker_state[15m]) > 6
        for: 1m
        labels:
          severity: critical
          component: circuit_breaker
          service: '{{ $labels.exported_service }}'
        annotations:
          summary: 'Circuit breaker for {{ $labels.exported_service }} is flapping'
          description: 'Circuit breaker for {{ $labels.exported_service }} has changed state {{ $value }} times in 15 minutes. Service is unstable.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/circuit-breaker-flapping'

      # Warning: Circuit breaker failure rate increasing
      # Uses circuit_breaker_calls_total with result label (not hsi_circuit_breaker_*_total)
      - alert: HSICircuitBreakerFailureRate
        expr: |
          rate(circuit_breaker_calls_total{result="failure"}[5m])
          /
          (rate(circuit_breaker_calls_total[5m]) + 0.001) > 0.3
        for: 2m
        labels:
          severity: warning
          component: circuit_breaker
          service: '{{ $labels.exported_service }}'
        annotations:
          summary: 'High failure rate for {{ $labels.exported_service }}'
          description: 'Circuit breaker for {{ $labels.exported_service }} is seeing >30% failure rate. May trip soon.'

      # Critical: Multiple circuit breakers open simultaneously
      - alert: HSIMultipleCircuitsOpen
        expr: count(hsi_circuit_breaker_state == 1) >= 2
        for: 1m
        labels:
          severity: critical
          component: circuit_breaker
        annotations:
          summary: 'Multiple circuit breakers are OPEN simultaneously'
          description: '{{ $value }} circuit breakers are in OPEN state. System is experiencing widespread service degradation.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/multiple-circuits-open'

  # =============================================================================
  # LLM/Inference Alerts
  # =============================================================================
  # Monitor LLM throughput, queue depth, and processing health.
  # Note: Uses hsi_nemotron_tokens_output_total (not hsi_nemotron_tokens_generated_total)
  # Note: Uses hsi_analysis_queue_depth (not hsi_nemotron_queue_depth which doesn't exist)
  - name: llm_inference_alerts
    rules:
      # Critical: LLM inference throughput is zero
      - alert: NemotronThroughputZero
        expr: |
          rate(hsi_nemotron_tokens_output_total[5m]) == 0
          and
          hsi_analysis_queue_depth > 0
        for: 2m
        labels:
          severity: critical
          component: llm
          service: nemotron
        annotations:
          summary: 'Nemotron LLM generating zero tokens with queued work'
          description: 'Nemotron has items queued but is not generating any tokens. LLM inference may be stuck.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/llm-throughput-zero'

      # Warning: LLM throughput degraded
      - alert: NemotronThroughputDegraded
        expr: |
          hsi_nemotron_tokens_per_second < 50
          and
          hsi_analysis_queue_depth > 5
        for: 5m
        labels:
          severity: warning
          component: llm
          service: nemotron
        annotations:
          summary: 'Nemotron LLM throughput degraded'
          description: 'Nemotron is generating only {{ $value }} tokens/sec with queue backing up.'

      # Critical: Detection processing stalled
      - alert: HSIDetectionProcessingStalled
        expr: |
          rate(hsi_detections_processed_total[5m]) == 0
          and
          hsi_detection_queue_depth > 0
        for: 3m
        labels:
          severity: critical
          component: detection
          service: pipeline
        annotations:
          summary: 'Detection processing has stalled'
          description: 'No detections processed in 5 minutes despite items in queue. Pipeline may be blocked.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/detection-stalled'

      # Warning: Low detection throughput
      - alert: HSILowDetectionThroughput
        expr: |
          rate(hsi_detections_processed_total[10m]) < 0.1
          and
          hsi_detection_queue_depth > 10
        for: 5m
        labels:
          severity: warning
          component: detection
          service: pipeline
        annotations:
          summary: 'Detection throughput below threshold'
          description: 'Processing <1 detection per 10 seconds while queue has items. Pipeline may be degraded.'

      # Critical: Analysis queue critically backed up (replacement for NemotronQueueCritical)
      - alert: HSIAnalysisQueueCritical
        expr: hsi_analysis_queue_depth > 50
        for: 2m
        labels:
          severity: critical
          component: llm
          service: nemotron
        annotations:
          summary: 'Analysis queue critically backed up'
          description: 'Analysis queue has {{ $value }} items, >50 threshold. Real-time analysis severely delayed.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/llm-queue-critical'

      # Warning: Prompt truncation rate increasing (context overflow)
      - alert: NemotronPromptTruncation
        expr: rate(hsi_prompts_truncated_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
          component: llm
          service: nemotron
        annotations:
          summary: 'Prompts being truncated due to context overflow'
          description: 'Prompt truncation rate is {{ $value }}/sec. Context window may be saturated.'

      # Warning: High context utilization
      - alert: NemotronContextUtilizationHigh
        expr: |
          histogram_quantile(0.95, rate(hsi_llm_context_utilization_bucket[5m])) > 0.9
        for: 5m
        labels:
          severity: warning
          component: llm
          service: nemotron
        annotations:
          summary: 'LLM context utilization approaching limits'
          description: 'P95 context utilization is {{ $value | humanizePercentage }}. Risk of truncation.'

      # Critical: YOLO26 model not loaded
      - alert: YOLO26ModelNotLoaded
        expr: yolo26_model_loaded == 0
        for: 1m
        labels:
          severity: critical
          component: detection
          service: yolo26
        annotations:
          summary: 'YOLO26 detection model not loaded'
          description: 'YOLO26 model is not loaded. Object detection will fail.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/model-not-loaded'

      # Critical: Florence model not loaded
      - alert: FlorenceModelNotLoaded
        expr: florence_model_loaded == 0
        for: 1m
        labels:
          severity: critical
          component: enrichment
          service: florence
        annotations:
          summary: 'Florence enrichment model not loaded'
          description: 'Florence model is not loaded. Image enrichment will fail.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/model-not-loaded'

  # =============================================================================
  # Enrichment Service Model Zoo VRAM Alerts (NEM-3149)
  # =============================================================================
  # Monitor VRAM usage and model evictions in the enrichment service's Model Zoo.
  # The Model Zoo manages 9 on-demand models with LRU eviction within a 6.8GB budget.
  - name: enrichment_model_zoo_alerts
    rules:
      # Critical: VRAM utilization exceeds 90% for extended period
      - alert: EnrichmentVRAMUtilizationCritical
        expr: enrichment_vram_utilization_percent > 90
        for: 5m
        labels:
          severity: critical
          component: enrichment
          service: model_zoo
        annotations:
          summary: 'Enrichment Model Zoo VRAM utilization critically high (>90%)'
          description: 'Model Zoo VRAM utilization is {{ $value | printf "%.1f" }}% for more than 5 minutes. Models may be evicted aggressively, degrading inference performance.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/model-zoo-vram-critical'

      # Warning: VRAM utilization elevated (>80%)
      - alert: EnrichmentVRAMUtilizationHigh
        expr: enrichment_vram_utilization_percent > 80
        for: 10m
        labels:
          severity: warning
          component: enrichment
          service: model_zoo
        annotations:
          summary: 'Enrichment Model Zoo VRAM utilization elevated (>80%)'
          description: 'Model Zoo VRAM utilization is {{ $value | printf "%.1f" }}% for more than 10 minutes. Consider increasing VRAM budget or reducing concurrent model usage.'

      # Critical: High model eviction rate (>10 evictions in 5 minutes)
      - alert: EnrichmentModelEvictionRateHigh
        expr: |
          increase(enrichment_model_evictions_total[5m]) > 10
        for: 2m
        labels:
          severity: critical
          component: enrichment
          service: model_zoo
        annotations:
          summary: 'High model eviction rate in Enrichment Model Zoo'
          description: 'More than 10 model evictions in 5 minutes. This indicates VRAM pressure causing thrashing. Current eviction rate: {{ $value | printf "%.0f" }} evictions/5min.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/model-zoo-eviction-storm'

      # Warning: Moderate model eviction rate (>5 evictions in 5 minutes)
      - alert: EnrichmentModelEvictionRateElevated
        expr: |
          increase(enrichment_model_evictions_total[5m]) > 5
        for: 5m
        labels:
          severity: warning
          component: enrichment
          service: model_zoo
        annotations:
          summary: 'Elevated model eviction rate in Enrichment Model Zoo'
          description: 'More than 5 model evictions in 5 minutes. VRAM budget may be insufficient for workload. Eviction rate: {{ $value | printf "%.0f" }} evictions/5min.'

      # Warning: CRITICAL priority model evicted (should be rare)
      - alert: EnrichmentCriticalModelEvicted
        expr: |
          increase(enrichment_model_evictions_total{priority="CRITICAL"}[15m]) > 0
        for: 1m
        labels:
          severity: warning
          component: enrichment
          service: model_zoo
        annotations:
          summary: 'CRITICAL priority model evicted from Model Zoo'
          description: 'A CRITICAL priority model (threat detection) was evicted. This should be rare and may indicate severe VRAM pressure.'

      # Warning: Model load time slow (P95 > 30s)
      - alert: EnrichmentModelLoadTimeSlow
        expr: |
          histogram_quantile(0.95, rate(enrichment_model_load_time_seconds_bucket[10m])) > 30
        for: 5m
        labels:
          severity: warning
          component: enrichment
          service: model_zoo
        annotations:
          summary: 'Enrichment Model Zoo model load times are slow'
          description: 'P95 model load time is {{ $value | printf "%.1f" }}s. This may indicate I/O bottlenecks or GPU memory fragmentation.'

      # Info: No models loaded (may be expected during low activity)
      - alert: EnrichmentNoModelsLoaded
        expr: enrichment_models_loaded == 0
        for: 30m
        labels:
          severity: info
          component: enrichment
          service: model_zoo
        annotations:
          summary: 'No models loaded in Enrichment Model Zoo'
          description: 'No models have been loaded in the Model Zoo for 30 minutes. This may be expected during low activity periods.'

  # =============================================================================
  # Database and Cache Cascade Alerts
  # =============================================================================
  # Monitor for database/cache cascade failures that could bring down the system.
  - name: database_cache_cascade_alerts
    rules:
      # Critical: Database AND Redis both unhealthy = cascade failure
      - alert: HSIDatabaseCacheCascadeFailure
        expr: |
          hsi_database_healthy_unhealthy > 0
          and
          up{job="redis"} == 0
        for: 1m
        labels:
          severity: critical
          component: infrastructure
          service: data_layer
        annotations:
          summary: 'Database AND Cache both failing - cascade failure'
          description: 'Both PostgreSQL and Redis are unhealthy simultaneously. System cannot function. Immediate intervention required.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/cascade-failure'

      # Warning: Cache hit ratio degraded
      # Uses hsi_cache_hits_total and hsi_cache_misses_total (not hsi_redis_cache_*)
      - alert: HSICacheHitRatioDegraded
        expr: |
          (
            rate(hsi_cache_hits_total[5m])
            /
            (rate(hsi_cache_hits_total[5m]) + rate(hsi_cache_misses_total[5m]) + 0.001)
          ) < 0.5
        for: 10m
        labels:
          severity: warning
          component: cache
          service: redis
        annotations:
          summary: 'Cache hit ratio below 50%'
          description: 'Redis cache hit ratio is {{ $value | humanizePercentage }}. Database load will be elevated.'

      # Critical: Cache hit ratio critically low
      - alert: HSICacheHitRatioCritical
        expr: |
          (
            rate(hsi_cache_hits_total[5m])
            /
            (rate(hsi_cache_hits_total[5m]) + rate(hsi_cache_misses_total[5m]) + 0.001)
          ) < 0.2
        for: 5m
        labels:
          severity: critical
          component: cache
          service: redis
        annotations:
          summary: 'Cache hit ratio critically low (<20%)'
          description: 'Redis cache hit ratio is {{ $value | humanizePercentage }}. Database is being hammered. Risk of cascade failure.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/cache-hit-ratio-critical'

      # Warning: Database query latency spike
      - alert: HSIDatabaseLatencySpike
        expr: |
          histogram_quantile(0.99, rate(hsi_db_query_duration_seconds_bucket[5m])) > 5
        for: 3m
        labels:
          severity: warning
          component: database
          service: postgresql
        annotations:
          summary: 'Database P99 latency spike (>5s)'
          description: 'P99 database query latency is {{ $value }}s. Slowest queries are taking too long.'

      # Critical: Database latency extreme
      - alert: HSIDatabaseLatencyExtreme
        expr: |
          histogram_quantile(0.95, rate(hsi_db_query_duration_seconds_bucket[5m])) > 10
        for: 2m
        labels:
          severity: critical
          component: database
          service: postgresql
        annotations:
          summary: 'Database P95 latency critically high (>10s)'
          description: 'P95 database query latency is {{ $value }}s. Database is severely degraded.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/database-latency-extreme'

  # =============================================================================
  # Pyroscope Continuous Profiling Alerts (NEM-3925)
  # =============================================================================
  # Monitor Pyroscope server health and profiling data ingestion.
  # Pyroscope provides continuous profiling for CPU and memory analysis across
  # all AI services (YOLO26, Nemotron, Florence, CLIP, Enrichment) and backend.
  - name: profiling_alerts
    rules:
      # Critical: Pyroscope server not available
      # This alert fires when the Pyroscope server cannot be scraped by Prometheus.
      # Without Pyroscope, continuous profiling data will not be collected.
      - alert: ProfilingDataMissing
        expr: absent(up{job="pyroscope"}) or up{job="pyroscope"} == 0
        for: 10m
        labels:
          severity: warning
          component: profiling
          service: pyroscope
        annotations:
          summary: 'Pyroscope server not available'
          description: 'Pyroscope profiling server has been unavailable for more than 10 minutes. Continuous profiling data is not being collected.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/pyroscope-unavailable'

      # Warning: Unusually high profile ingestion rate
      # May indicate a profiling misconfiguration or runaway profiling agent.
      # Normal rate depends on number of services and sample rate, but >10000/5m is excessive.
      - alert: HighProfileIngestionRate
        expr: rate(pyroscope_distributor_received_samples_total[5m]) > 10000
        for: 5m
        labels:
          severity: warning
          component: profiling
          service: pyroscope
        annotations:
          summary: 'Unusually high profiling data rate'
          description: 'Pyroscope is receiving {{ $value | printf "%.0f" }} samples/sec, which is unusually high. Check for misconfigured sample rates or runaway profiling agents.'

      # Warning: No profiles being received
      # This indicates that profiling agents may not be sending data to Pyroscope.
      # May occur if PYROSCOPE_ENABLED=false or network connectivity issues.
      - alert: NoProfilesReceived
        expr: |
          up{job="pyroscope"} == 1
          and
          rate(pyroscope_distributor_received_samples_total[10m]) == 0
        for: 15m
        labels:
          severity: warning
          component: profiling
          service: pyroscope
        annotations:
          summary: 'Pyroscope receiving no profiling data'
          description: 'Pyroscope server is up but has not received any profiling samples for 15 minutes. Check that PYROSCOPE_ENABLED=true and services can reach pyroscope:4040.'

      # Critical: Pyroscope storage issues
      # Monitors for ingester errors which may indicate disk or storage problems.
      - alert: PyroscopeIngesterErrors
        expr: increase(pyroscope_ingester_tsdb_wal_corruptions_total[1h]) > 0
        for: 5m
        labels:
          severity: critical
          component: profiling
          service: pyroscope
        annotations:
          summary: 'Pyroscope storage corruption detected'
          description: 'Pyroscope has detected WAL corruptions. Profiling data may be lost. Check disk health and available space.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/pyroscope-storage-corruption'

      # Warning: Pyroscope disk usage high
      # Monitor Pyroscope's storage consumption to prevent disk full conditions.
      - alert: PyroscopeDiskUsageHigh
        expr: |
          pyroscope_tsdb_size_bytes / (1024 * 1024 * 1024) > 10
        for: 30m
        labels:
          severity: warning
          component: profiling
          service: pyroscope
        annotations:
          summary: 'Pyroscope storage exceeds 10GB'
          description: 'Pyroscope storage is using {{ $value | printf "%.1f" }}GB. Consider adjusting retention or increasing disk space.'
