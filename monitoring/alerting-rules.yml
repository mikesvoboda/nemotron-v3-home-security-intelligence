# Alerting Rules for Home Security Intelligence
# Defines critical, warning, and info alerts for pipeline health and anomaly detection

groups:
  # Pipeline Health Alerts
  - name: pipeline_health_alerts
    rules:
      # Critical: Pipeline is completely down
      - alert: HSIPipelineDown
        expr: probe_success{job="blackbox-http-live", service="backend"} == 0
        for: 1m
        labels:
          severity: critical
          component: pipeline
        annotations:
          summary: 'HSI Pipeline is down'
          description: 'The HSI backend service has been unreachable for more than 1 minute.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/pipeline-down'

      # Critical: Pipeline health check failing
      # Uses hsi_system_healthy_unhealthy gauge (1 when unhealthy, 0 when healthy)
      - alert: HSIPipelineUnhealthy
        expr: hsi_system_healthy_unhealthy > 0
        for: 2m
        labels:
          severity: critical
          component: pipeline
        annotations:
          summary: 'HSI Pipeline health check failing'
          description: 'The HSI pipeline health check has been failing for more than 2 minutes.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/pipeline-unhealthy'

  # Database Alerts
  - name: database_alerts
    rules:
      # Critical: Database connection failures
      # Uses hsi_database_healthy_unhealthy gauge (1 when unhealthy, 0 when healthy)
      - alert: HSIDatabaseUnhealthy
        expr: hsi_database_healthy_unhealthy > 0
        for: 2m
        labels:
          severity: critical
          component: database
        annotations:
          summary: 'HSI Database is unhealthy'
          description: 'Database health check has been failing for more than 2 minutes.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/database-unhealthy'

      # Warning: Database query latency high
      - alert: HSIDatabaseSlowQueries
        expr: |
          histogram_quantile(0.95, rate(hsi_db_query_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: 'HSI Database queries are slow'
          description: 'P95 database query latency exceeds 1 second for more than 5 minutes.'

  # Redis Alerts
  - name: redis_alerts
    rules:
      # Critical: Redis unavailable
      - alert: HSIRedisUnhealthy
        expr: up{job="redis"} == 0
        for: 2m
        labels:
          severity: critical
          component: redis
        annotations:
          summary: 'HSI Redis is unhealthy'
          description: 'Redis has been unreachable for more than 2 minutes.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/redis-unhealthy'

      # Warning: Redis memory high
      # Note: Only fires when maxmemory is configured (> 0), avoids false positives from division by zero
      - alert: HSIRedisMemoryHigh
        expr: redis_memory_max_bytes > 0 and redis_memory_used_bytes / redis_memory_max_bytes > 0.8
        for: 5m
        labels:
          severity: warning
          component: redis
        annotations:
          summary: 'HSI Redis memory usage high'
          description: 'Redis memory usage is above 80% for more than 5 minutes.'

  # GPU Alerts
  - name: gpu_alerts
    rules:
      # Critical: GPU memory critically high
      - alert: HSIGPUMemoryHigh
        expr: hsi:gpu:memory_utilization > 0.9
        for: 5m
        labels:
          severity: critical
          component: gpu
        annotations:
          summary: 'HSI GPU memory usage critically high'
          description: 'GPU memory utilization is above 90% for more than 5 minutes. Risk of OOM errors.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/gpu-memory-high'

      # Warning: GPU memory elevated
      - alert: HSIGPUMemoryElevated
        expr: hsi:gpu:memory_utilization > 0.95
        for: 10m
        labels:
          severity: warning
          component: gpu
        annotations:
          summary: 'HSI GPU memory usage elevated'
          description: 'GPU memory utilization is above 95% for more than 10 minutes.'

      # Warning: GPU utilization low (potential issue)
      - alert: HSIGPUUtilizationLow
        expr: hsi:gpu:utilization < 10 and hsi_detection_total > 0
        for: 15m
        labels:
          severity: warning
          component: gpu
        annotations:
          summary: 'HSI GPU utilization unexpectedly low'
          description: 'GPU utilization is below 10% while detections are happening. May indicate processing issues.'

  # Queue Alerts
  - name: queue_alerts
    rules:
      # Warning: Detection queue backing up
      - alert: HSIDetectionQueueHigh
        expr: hsi_detection_queue_depth > 100
        for: 5m
        labels:
          severity: warning
          component: detection
        annotations:
          summary: 'HSI Detection queue is backing up'
          description: 'More than 100 items in the detection queue for more than 5 minutes.'

      # Warning: Analysis queue backing up
      - alert: HSIAnalysisQueueHigh
        expr: hsi_analysis_queue_depth > 50
        for: 5m
        labels:
          severity: warning
          component: analysis
        annotations:
          summary: 'HSI Analysis queue is backing up'
          description: 'More than 50 items in the analysis queue for more than 5 minutes.'

      # Critical: Queues severely backed up
      - alert: HSIQueueCritical
        expr: hsi_detection_queue_depth > 500 or hsi_analysis_queue_depth > 200
        for: 2m
        labels:
          severity: critical
          component: queue
        annotations:
          summary: 'HSI Processing queues critically backed up'
          description: 'Detection or analysis queue has reached critical levels. Processing may be stalled.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/queue-critical'

  # Latency Alerts
  # NOTE: Disabled - depends on hsi:detection_latency:p95_5m and hsi:analysis_latency:p95_5m
  # recording rules which require hsi_stage_duration_seconds_bucket metrics (not implemented)
  # Re-enable when metrics are added to backend/core/metrics.py
  # - name: latency_alerts
  #   rules:
  #     - alert: HSISlowDetection
  #       expr: hsi:detection_latency:p95_5m > 2
  #       ...
  #     - alert: HSISlowAnalysis
  #       expr: hsi:analysis_latency:p95_5m > 30
  #       ...
  #     - alert: HSIExtremeLatency
  #       expr: hsi:detection_latency:p95_5m > 10 or hsi:analysis_latency:p95_5m > 120
  #       ...

  # Error Rate Alerts
  - name: error_rate_alerts
    rules:
      # Warning: Elevated error rate
      - alert: HSIHighErrorRate
        expr: |
          1 - hsi:api_requests:success_rate_5m > 0.05
        for: 5m
        labels:
          severity: warning
          component: api
        annotations:
          summary: 'HSI API error rate elevated'
          description: 'API error rate is above 5% for more than 5 minutes.'

      # Critical: High error rate
      - alert: HSICriticalErrorRate
        expr: |
          1 - hsi:api_requests:success_rate_5m > 0.1
        for: 2m
        labels:
          severity: critical
          component: api
        annotations:
          summary: 'HSI API error rate critical'
          description: 'API error rate is above 10% for more than 2 minutes.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/high-error-rate'

      # Warning: Detection failures
      - alert: HSIDetectionFailureRate
        expr: |
          1 - hsi:detection:success_rate_5m > 0.1
        for: 5m
        labels:
          severity: warning
          component: detection
        annotations:
          summary: 'HSI Detection failure rate elevated'
          description: 'More than 10% of detections are failing for more than 5 minutes.'

      # Warning: Analysis failures
      - alert: HSIAnalysisFailureRate
        expr: |
          1 - hsi:analysis:success_rate_5m > 0.1
        for: 5m
        labels:
          severity: warning
          component: analysis
        annotations:
          summary: 'HSI Analysis failure rate elevated'
          description: 'More than 10% of analyses are failing for more than 5 minutes.'

  # SLO Burn Rate Alerts (Multi-window)
  - name: slo_burn_rate_alerts
    rules:
      # Critical: Fast burn on API availability (14.4x burn rate over 1h)
      - alert: HSIAPIAvailabilityFastBurn
        expr: |
          hsi:burn_rate:api_availability_1h > 14.4
          and
          hsi:burn_rate:api_availability_6h > 6
        for: 2m
        labels:
          severity: critical
          component: slo
          slo: api_availability
        annotations:
          summary: 'HSI API Availability SLO fast burn detected'
          description: 'API availability is burning through error budget at 14.4x rate. Will exhaust 30-day budget in ~2 hours.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/slo-fast-burn'

      # Warning: Slow burn on API availability (3x burn rate over 1d)
      - alert: HSIAPIAvailabilitySlowBurn
        expr: |
          hsi:burn_rate:api_availability_1d > 3
        for: 1h
        labels:
          severity: warning
          component: slo
          slo: api_availability
        annotations:
          summary: 'HSI API Availability SLO slow burn detected'
          description: 'API availability is burning through error budget at 3x rate. Will exhaust 30-day budget in ~10 days.'

      # NOTE: Detection and analysis latency burn rate alerts disabled
      # Depends on hsi:burn_rate:detection_latency_* and hsi:burn_rate:analysis_latency_*
      # recording rules which require hsi_stage_duration_seconds_bucket metrics (not implemented)
      # - alert: HSIDetectionLatencyFastBurn
      #   expr: hsi:burn_rate:detection_latency_1h > 14.4 and hsi:burn_rate:detection_latency_6h > 6
      #   ...
      # - alert: HSIAnalysisLatencyFastBurn
      #   expr: hsi:burn_rate:analysis_latency_1h > 14.4 and hsi:burn_rate:analysis_latency_6h > 6
      #   ...

  # WebSocket Alerts
  # NOTE: Disabled - WebSocket metrics (hsi_websocket_*) not yet implemented
  # Re-enable when metrics are added to backend/core/metrics.py
  # - name: websocket_alerts
  #   rules:
  #     # Warning: WebSocket connection failures
  #     - alert: HSIWebSocketConnectionFailures
  #       expr: hsi:websocket:connection_success_rate_5m < 0.95
  #       for: 5m
  #       labels:
  #         severity: warning
  #         component: websocket
  #       annotations:
  #         summary: 'HSI WebSocket connection failures elevated'
  #         description: 'WebSocket connection success rate is below 95% for more than 5 minutes.'
  #
  #     # Critical: WebSocket completely failing
  #     - alert: HSIWebSocketDown
  #       expr: hsi:websocket:connection_success_rate_5m < 0.5
  #       for: 2m
  #       labels:
  #         severity: critical
  #         component: websocket
  #       annotations:
  #         summary: 'HSI WebSocket service degraded'
  #         description: 'More than 50% of WebSocket connections are failing.'
  #         runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/websocket-down'
  #
  #     # Info: No active WebSocket connections
  #     - alert: HSINoWebSocketConnections
  #       expr: hsi:websocket:active_connections == 0
  #       for: 30m
  #       labels:
  #         severity: info
  #         component: websocket
  #       annotations:
  #         summary: 'HSI No active WebSocket connections'
  #         description: 'No clients have been connected via WebSocket for 30 minutes. This may be expected during low-traffic periods.'

  # =============================================================================
  # Prometheus Self-Monitoring Alerts (NEM-2468)
  # =============================================================================
  # Monitor Prometheus itself to ensure observability infrastructure is healthy.
  # These alerts help detect issues with the monitoring system before they
  # impact visibility into the main application.
  - name: prometheus_self_monitoring_alerts
    rules:
      # Critical: Prometheus is not scraping itself
      - alert: PrometheusNotScrapingSelf
        expr: up{job="prometheus"} == 0
        for: 2m
        labels:
          severity: critical
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus is not scraping itself'
          description: 'Prometheus self-monitoring target has been down for more than 2 minutes. This indicates Prometheus may be unhealthy or misconfigured.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-self-scrape-down'

      # Critical: Prometheus configuration reload failed
      - alert: PrometheusConfigReloadFailed
        expr: prometheus_config_last_reload_successful == 0
        for: 5m
        labels:
          severity: critical
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus configuration reload failed'
          description: 'Prometheus configuration reload has been failing for more than 5 minutes. New alerting rules or scrape configurations are not being applied.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-config-reload-failed'

      # Warning: Prometheus rule evaluation failures
      - alert: PrometheusRuleEvaluationFailures
        expr: increase(prometheus_rule_evaluation_failures_total[5m]) > 0
        for: 5m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus rule evaluation failures detected'
          description: 'Prometheus has encountered rule evaluation failures in the last 5 minutes. Some recording rules or alerts may not be evaluated correctly.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-rule-eval-failures'

      # Warning: Prometheus rule evaluation slow
      - alert: PrometheusRuleEvaluationSlow
        expr: |
          prometheus_rule_group_last_duration_seconds
          >
          prometheus_rule_group_interval_seconds
        for: 10m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus rule evaluation is taking too long'
          description: 'Prometheus rule group evaluation is taking longer than the group interval for more than 10 minutes. This may cause gaps in alerting and recording rules.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-rule-eval-slow'

      # Critical: Prometheus scrape failures high
      - alert: PrometheusScrapeFailuresHigh
        expr: |
          (
            sum(increase(prometheus_target_scrape_pool_sync_total{status="failed"}[5m]))
            /
            (sum(increase(prometheus_target_scrape_pool_sync_total[5m])) + 0.001)
          ) > 0.1
        for: 5m
        labels:
          severity: critical
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus scrape sync failures are high'
          description: 'More than 10% of Prometheus scrape pool sync operations are failing for more than 5 minutes. Metrics collection may be degraded.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-scrape-failures'

      # Warning: Prometheus targets down
      - alert: PrometheusTargetsUnhealthy
        expr: |
          (
            count(up == 0)
            /
            count(up)
          ) > 0.2
        for: 5m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'High percentage of Prometheus targets are down'
          description: 'More than 20% of Prometheus scrape targets are down for more than 5 minutes. Current unhealthy: {{ $value | humanizePercentage }}.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-targets-unhealthy'

      # Warning: Prometheus notification queue full
      - alert: PrometheusNotificationQueueFull
        expr: prometheus_notifications_queue_length > prometheus_notifications_queue_capacity * 0.9
        for: 5m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus notification queue is nearly full'
          description: 'Prometheus notification queue is above 90% capacity for more than 5 minutes. Alerts may be delayed or dropped.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-notification-queue-full'

      # Critical: Prometheus notification failures
      - alert: PrometheusNotificationsFailing
        expr: increase(prometheus_notifications_errors_total[5m]) > 5
        for: 5m
        labels:
          severity: critical
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus is failing to send notifications'
          description: 'Prometheus has failed to send more than 5 notifications to Alertmanager in the last 5 minutes. Alerts may not be delivered.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-notifications-failing'

      # Warning: Prometheus TSDB compaction failures
      - alert: PrometheusTSDBCompactionsFailing
        expr: increase(prometheus_tsdb_compactions_failed_total[6h]) > 0
        for: 5m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus TSDB compactions are failing'
          description: 'Prometheus TSDB has experienced compaction failures in the last 6 hours. This may lead to increased disk usage and query performance degradation.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-tsdb-compactions-failing'

      # Critical: Prometheus TSDB head truncation failures
      - alert: PrometheusTSDBHeadTruncationsFailing
        expr: increase(prometheus_tsdb_head_truncations_failed_total[1h]) > 0
        for: 5m
        labels:
          severity: critical
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus TSDB head truncations are failing'
          description: 'Prometheus TSDB head truncations have failed. This is a critical issue that may lead to memory exhaustion and data corruption.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-tsdb-head-truncations-failing'

      # Warning: Prometheus TSDB WAL corruptions
      - alert: PrometheusTSDBWALCorruptions
        expr: increase(prometheus_tsdb_wal_corruptions_total[1h]) > 0
        for: 1m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus TSDB WAL corruptions detected'
          description: 'Prometheus TSDB has detected WAL (Write-Ahead Log) corruptions. Some metrics data may be lost.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-tsdb-wal-corruptions'

      # Warning: Prometheus storage is filling up
      - alert: PrometheusStorageFillingUp
        expr: |
          (
            prometheus_tsdb_storage_blocks_bytes
            /
            (prometheus_tsdb_storage_blocks_bytes + prometheus_tsdb_storage_blocks_bytes_free)
          ) > 0.8
        for: 15m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus storage is filling up'
          description: 'Prometheus TSDB storage is more than 80% full for more than 15 minutes. Consider increasing retention or storage capacity.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-storage-filling'

      # Warning: Prometheus query load high
      - alert: PrometheusQueryLoadHigh
        expr: |
          rate(prometheus_engine_query_duration_seconds_sum[5m])
          /
          rate(prometheus_engine_query_duration_seconds_count[5m])
          > 10
        for: 10m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus query load is high'
          description: 'Average Prometheus query duration exceeds 10 seconds for more than 10 minutes. Dashboard and API queries may be slow.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-query-load-high'

      # Info: Prometheus has restarted
      - alert: PrometheusRestarted
        expr: changes(prometheus_build_info[15m]) > 0
        for: 0m
        labels:
          severity: info
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus has restarted'
          description: 'Prometheus instance has restarted within the last 15 minutes. Check for any configuration changes or issues that may have caused the restart.'

      # Warning: Alertmanager connectivity issues
      - alert: PrometheusAlertmanagerDown
        expr: prometheus_notifications_alertmanagers_discovered < 1
        for: 5m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus cannot discover Alertmanager'
          description: 'Prometheus has no discovered Alertmanager instances for more than 5 minutes. Alerts will not be delivered.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-alertmanager-down'

      # Critical: Too many samples rejected
      - alert: PrometheusSamplesRejected
        expr: |
          rate(prometheus_target_scrapes_sample_out_of_order_total[5m]) > 0
          or
          rate(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0
        for: 10m
        labels:
          severity: warning
          component: monitoring
          service: prometheus
        annotations:
          summary: 'Prometheus is rejecting samples'
          description: 'Prometheus is rejecting samples due to out-of-order timestamps or duplicates. This may indicate clock drift or incorrect metric exposition.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/prometheus-samples-rejected'

  # =============================================================================
  # Pipeline Worker Health Alerts (NEM-2459)
  # =============================================================================
  # Monitor pipeline worker restarts, failures, and health state.
  # These alerts help detect worker stability issues before they impact
  # the AI processing pipeline.
  - name: pipeline_worker_alerts
    rules:
      # NOTE: HSIWorkerRestartStorm disabled - hsi_pipeline_worker_restarts_total metric not implemented
      # Re-enable when metric is added to backend/core/metrics.py
      # - alert: HSIWorkerRestartStorm
      #   expr: increase(hsi_pipeline_worker_restarts_total[5m]) > 3
      #   ...

      # Critical: Worker in failed state for 30 seconds
      - alert: HSIWorkerFailed
        expr: |
          hsi_pipeline_worker_state == 3
        for: 30s
        labels:
          severity: critical
          component: worker
          service: pipeline
        annotations:
          summary: 'Pipeline worker failed ({{ $labels.worker_name }})'
          description: 'Worker {{ $labels.worker_name }} has been in FAILED state for more than 30 seconds. The worker has exceeded its maximum restart attempts and requires manual intervention.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/worker-failed'

      # Warning: Worker not running for 2 minutes (state != 1)
      - alert: HSIWorkerNotRunning
        expr: |
          hsi_pipeline_worker_state != 1
        for: 2m
        labels:
          severity: warning
          component: worker
          service: pipeline
        annotations:
          summary: 'Pipeline worker not running ({{ $labels.worker_name }})'
          description: 'Worker {{ $labels.worker_name }} has not been in RUNNING state for more than 2 minutes. Current state: {{ $value }} (0=stopped, 1=running, 2=restarting, 3=failed).'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/worker-not-running'

      # NOTE: HSIWorkerRestartSlow disabled - hsi_pipeline_worker_restart_duration_seconds_bucket metric not implemented
      # - alert: HSIWorkerRestartSlow
      #   expr: histogram_quantile(0.95, rate(hsi_pipeline_worker_restart_duration_seconds_bucket[5m])) > 30
      #   ...

      # Warning: High consecutive failure count
      - alert: HSIWorkerConsecutiveFailures
        expr: |
          hsi_pipeline_worker_consecutive_failures >= 3
        for: 1m
        labels:
          severity: warning
          component: worker
          service: pipeline
        annotations:
          summary: 'Pipeline worker has multiple consecutive failures ({{ $labels.worker_name }})'
          description: 'Worker {{ $labels.worker_name }} has {{ $value }} consecutive failures. Worker may be approaching the max restart limit.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/worker-consecutive-failures'

      # NOTE: The following alerts are disabled - hsi_pipeline_worker_restarts_total metric not implemented
      # Re-enable when metric is added to backend/core/metrics.py
      # - alert: HSIWorkerRestarted
      # - alert: HSIWorkerConnectionIssues
      # - alert: HSIWorkerMemoryIssues

      # Critical: All workers in the supervisor are failing
      - alert: HSIAllWorkersFailing
        expr: |
          count(hsi_pipeline_worker_state == 3) == count(hsi_pipeline_worker_state)
          and
          count(hsi_pipeline_worker_state) > 0
        for: 1m
        labels:
          severity: critical
          component: worker
          service: pipeline
        annotations:
          summary: 'All pipeline workers are in failed state'
          description: 'All registered pipeline workers have failed. The AI processing pipeline is completely stopped.'
          runbook_url: 'https://github.com/mikesvoboda/nemotron-v3-home-security-intelligence/wiki/runbooks/all-workers-failing'
